[2020-05-26T17:13:55,643][DEBUG][logstash.modules.scaffold] Found module {:module_name=>"fb_apache", :directory=>"D:/Softwares/logstash-7.7.0/modules/fb_apache/configuration"}
[2020-05-26T17:13:55,761][DEBUG][logstash.plugins.registry] Adding plugin to the registry {:name=>"fb_apache", :type=>:modules, :class=>#<LogStash::Modules::Scaffold:0x630f24a5 @directory="D:/Softwares/logstash-7.7.0/modules/fb_apache/configuration", @module_name="fb_apache", @kibana_version_parts=["6", "0", "0"]>}
[2020-05-26T17:13:55,764][DEBUG][logstash.modules.scaffold] Found module {:module_name=>"netflow", :directory=>"D:/Softwares/logstash-7.7.0/modules/netflow/configuration"}
[2020-05-26T17:13:55,767][DEBUG][logstash.plugins.registry] Adding plugin to the registry {:name=>"netflow", :type=>:modules, :class=>#<LogStash::Modules::Scaffold:0x34ceabf1 @directory="D:/Softwares/logstash-7.7.0/modules/netflow/configuration", @module_name="netflow", @kibana_version_parts=["6", "0", "0"]>}
[2020-05-26T17:13:55,808][INFO ][logstash.setting.writabledirectory] Creating directory {:setting=>"path.queue", :path=>"D:/Softwares/logstash-7.7.0/data/queue"}
[2020-05-26T17:13:55,822][INFO ][logstash.setting.writabledirectory] Creating directory {:setting=>"path.dead_letter_queue", :path=>"D:/Softwares/logstash-7.7.0/data/dead_letter_queue"}
[2020-05-26T17:13:55,873][DEBUG][logstash.runner          ] -------- Logstash Settings (* means modified) ---------
[2020-05-26T17:13:55,877][DEBUG][logstash.runner          ] node.name: "DESKTOP-IU1N73S"
[2020-05-26T17:13:55,879][DEBUG][logstash.runner          ] *path.config: "config\\logstash-sample.conf"
[2020-05-26T17:13:55,880][DEBUG][logstash.runner          ] path.data: "D:/Softwares/logstash-7.7.0/data"
[2020-05-26T17:13:55,882][DEBUG][logstash.runner          ] modules.cli: []
[2020-05-26T17:13:55,884][DEBUG][logstash.runner          ] modules: []
[2020-05-26T17:13:55,888][DEBUG][logstash.runner          ] modules_list: []
[2020-05-26T17:13:55,890][DEBUG][logstash.runner          ] modules_variable_list: []
[2020-05-26T17:13:55,891][DEBUG][logstash.runner          ] modules_setup: false
[2020-05-26T17:13:55,893][DEBUG][logstash.runner          ] config.test_and_exit: false
[2020-05-26T17:13:55,894][DEBUG][logstash.runner          ] config.reload.automatic: false
[2020-05-26T17:13:55,896][DEBUG][logstash.runner          ] config.reload.interval: 3000000000
[2020-05-26T17:13:55,897][DEBUG][logstash.runner          ] config.support_escapes: false
[2020-05-26T17:13:55,898][DEBUG][logstash.runner          ] config.field_reference.parser: "STRICT"
[2020-05-26T17:13:55,899][DEBUG][logstash.runner          ] metric.collect: true
[2020-05-26T17:13:55,901][DEBUG][logstash.runner          ] pipeline.id: "main"
[2020-05-26T17:13:55,903][DEBUG][logstash.runner          ] pipeline.system: false
[2020-05-26T17:13:55,904][DEBUG][logstash.runner          ] pipeline.workers: 8
[2020-05-26T17:13:55,906][DEBUG][logstash.runner          ] pipeline.batch.size: 125
[2020-05-26T17:13:55,907][DEBUG][logstash.runner          ] pipeline.batch.delay: 50
[2020-05-26T17:13:55,908][DEBUG][logstash.runner          ] pipeline.unsafe_shutdown: false
[2020-05-26T17:13:55,909][DEBUG][logstash.runner          ] pipeline.java_execution: true
[2020-05-26T17:13:55,911][DEBUG][logstash.runner          ] pipeline.reloadable: true
[2020-05-26T17:13:55,912][DEBUG][logstash.runner          ] pipeline.plugin_classloaders: false
[2020-05-26T17:13:55,913][DEBUG][logstash.runner          ] pipeline.separate_logs: false
[2020-05-26T17:13:55,914][DEBUG][logstash.runner          ] pipeline.ordered: "auto"
[2020-05-26T17:13:55,915][DEBUG][logstash.runner          ] path.plugins: []
[2020-05-26T17:13:55,916][DEBUG][logstash.runner          ] config.debug: false
[2020-05-26T17:13:55,918][DEBUG][logstash.runner          ] *log.level: "debug" (default: "info")
[2020-05-26T17:13:55,919][DEBUG][logstash.runner          ] version: false
[2020-05-26T17:13:55,920][DEBUG][logstash.runner          ] help: false
[2020-05-26T17:13:55,922][DEBUG][logstash.runner          ] log.format: "plain"
[2020-05-26T17:13:55,923][DEBUG][logstash.runner          ] http.host: "127.0.0.1"
[2020-05-26T17:13:55,924][DEBUG][logstash.runner          ] http.port: 9600..9700
[2020-05-26T17:13:55,925][DEBUG][logstash.runner          ] http.environment: "production"
[2020-05-26T17:13:55,927][DEBUG][logstash.runner          ] queue.type: "memory"
[2020-05-26T17:13:55,928][DEBUG][logstash.runner          ] queue.drain: false
[2020-05-26T17:13:55,930][DEBUG][logstash.runner          ] queue.page_capacity: 67108864
[2020-05-26T17:13:55,931][DEBUG][logstash.runner          ] queue.max_bytes: 1073741824
[2020-05-26T17:13:55,932][DEBUG][logstash.runner          ] queue.max_events: 0
[2020-05-26T17:13:55,934][DEBUG][logstash.runner          ] queue.checkpoint.acks: 1024
[2020-05-26T17:13:55,935][DEBUG][logstash.runner          ] queue.checkpoint.writes: 1024
[2020-05-26T17:13:55,937][DEBUG][logstash.runner          ] queue.checkpoint.interval: 1000
[2020-05-26T17:13:55,938][DEBUG][logstash.runner          ] queue.checkpoint.retry: false
[2020-05-26T17:13:55,940][DEBUG][logstash.runner          ] dead_letter_queue.enable: false
[2020-05-26T17:13:55,941][DEBUG][logstash.runner          ] dead_letter_queue.max_bytes: 1073741824
[2020-05-26T17:13:55,942][DEBUG][logstash.runner          ] slowlog.threshold.warn: -1
[2020-05-26T17:13:55,943][DEBUG][logstash.runner          ] slowlog.threshold.info: -1
[2020-05-26T17:13:55,944][DEBUG][logstash.runner          ] slowlog.threshold.debug: -1
[2020-05-26T17:13:55,945][DEBUG][logstash.runner          ] slowlog.threshold.trace: -1
[2020-05-26T17:13:55,946][DEBUG][logstash.runner          ] keystore.classname: "org.logstash.secret.store.backend.JavaKeyStore"
[2020-05-26T17:13:55,948][DEBUG][logstash.runner          ] keystore.file: "D:/Softwares/logstash-7.7.0/config/logstash.keystore"
[2020-05-26T17:13:55,949][DEBUG][logstash.runner          ] path.queue: "D:/Softwares/logstash-7.7.0/data/queue"
[2020-05-26T17:13:55,952][DEBUG][logstash.runner          ] path.dead_letter_queue: "D:/Softwares/logstash-7.7.0/data/dead_letter_queue"
[2020-05-26T17:13:55,954][DEBUG][logstash.runner          ] path.settings: "D:/Softwares/logstash-7.7.0/config"
[2020-05-26T17:13:55,955][DEBUG][logstash.runner          ] path.logs: "D:/Softwares/logstash-7.7.0/logs"
[2020-05-26T17:13:55,957][DEBUG][logstash.runner          ] xpack.management.enabled: false
[2020-05-26T17:13:55,958][DEBUG][logstash.runner          ] xpack.management.logstash.poll_interval: 5000000000
[2020-05-26T17:13:55,959][DEBUG][logstash.runner          ] xpack.management.pipeline.id: ["main"]
[2020-05-26T17:13:55,961][DEBUG][logstash.runner          ] xpack.management.elasticsearch.username: "logstash_system"
[2020-05-26T17:13:55,962][DEBUG][logstash.runner          ] xpack.management.elasticsearch.hosts: ["https://localhost:9200"]
[2020-05-26T17:13:55,963][DEBUG][logstash.runner          ] xpack.management.elasticsearch.ssl.verification_mode: "certificate"
[2020-05-26T17:13:55,964][DEBUG][logstash.runner          ] xpack.management.elasticsearch.sniffing: false
[2020-05-26T17:13:55,964][DEBUG][logstash.runner          ] xpack.monitoring.enabled: false
[2020-05-26T17:13:55,965][DEBUG][logstash.runner          ] xpack.monitoring.elasticsearch.hosts: ["http://localhost:9200"]
[2020-05-26T17:13:55,966][DEBUG][logstash.runner          ] xpack.monitoring.collection.interval: 10000000000
[2020-05-26T17:13:55,967][DEBUG][logstash.runner          ] xpack.monitoring.collection.timeout_interval: 600000000000
[2020-05-26T17:13:55,969][DEBUG][logstash.runner          ] xpack.monitoring.elasticsearch.username: "logstash_system"
[2020-05-26T17:13:55,972][DEBUG][logstash.runner          ] xpack.monitoring.elasticsearch.ssl.verification_mode: "certificate"
[2020-05-26T17:13:55,973][DEBUG][logstash.runner          ] xpack.monitoring.elasticsearch.sniffing: false
[2020-05-26T17:13:55,974][DEBUG][logstash.runner          ] xpack.monitoring.collection.pipeline.details.enabled: true
[2020-05-26T17:13:55,975][DEBUG][logstash.runner          ] xpack.monitoring.collection.config.enabled: true
[2020-05-26T17:13:55,977][DEBUG][logstash.runner          ] monitoring.enabled: false
[2020-05-26T17:13:55,978][DEBUG][logstash.runner          ] monitoring.elasticsearch.hosts: ["http://localhost:9200"]
[2020-05-26T17:13:55,979][DEBUG][logstash.runner          ] monitoring.collection.interval: 10000000000
[2020-05-26T17:13:55,981][DEBUG][logstash.runner          ] monitoring.collection.timeout_interval: 600000000000
[2020-05-26T17:13:55,982][DEBUG][logstash.runner          ] monitoring.elasticsearch.username: "logstash_system"
[2020-05-26T17:13:55,984][DEBUG][logstash.runner          ] monitoring.elasticsearch.ssl.verification_mode: "certificate"
[2020-05-26T17:13:55,985][DEBUG][logstash.runner          ] monitoring.elasticsearch.sniffing: false
[2020-05-26T17:13:55,986][DEBUG][logstash.runner          ] monitoring.collection.pipeline.details.enabled: true
[2020-05-26T17:13:55,988][DEBUG][logstash.runner          ] monitoring.collection.config.enabled: true
[2020-05-26T17:13:55,990][DEBUG][logstash.runner          ] node.uuid: ""
[2020-05-26T17:13:55,997][DEBUG][logstash.runner          ] --------------- Logstash Settings -------------------
[2020-05-26T17:13:56,047][WARN ][logstash.config.source.multilocal] Ignoring the 'pipelines.yml' file because modules or command line options are specified
[2020-05-26T17:13:56,058][INFO ][logstash.runner          ] Starting Logstash {"logstash.version"=>"7.7.0"}
[2020-05-26T17:13:56,082][INFO ][logstash.agent           ] No persistent UUID file found. Generating new UUID {:uuid=>"f0733212-7197-4ad7-9df7-85b00cc7f7fa", :path=>"D:/Softwares/logstash-7.7.0/data/uuid"}
[2020-05-26T17:13:56,105][DEBUG][logstash.agent           ] Setting up metric collection
[2020-05-26T17:13:56,175][DEBUG][logstash.instrument.periodicpoller.os] Starting {:polling_interval=>5, :polling_timeout=>120}
[2020-05-26T17:13:56,189][DEBUG][logstash.instrument.periodicpoller.cgroup] One or more required cgroup files or directories not found: /proc/self/cgroup, /sys/fs/cgroup/cpuacct, /sys/fs/cgroup/cpu
[2020-05-26T17:13:56,325][DEBUG][logstash.instrument.periodicpoller.jvm] Starting {:polling_interval=>5, :polling_timeout=>120}
[2020-05-26T17:13:56,436][DEBUG][logstash.instrument.periodicpoller.jvm] collector name {:name=>"ParNew"}
[2020-05-26T17:13:56,443][DEBUG][logstash.instrument.periodicpoller.jvm] collector name {:name=>"ConcurrentMarkSweep"}
[2020-05-26T17:13:56,461][DEBUG][logstash.instrument.periodicpoller.persistentqueue] Starting {:polling_interval=>5, :polling_timeout=>120}
[2020-05-26T17:13:56,474][DEBUG][logstash.instrument.periodicpoller.deadletterqueue] Starting {:polling_interval=>5, :polling_timeout=>120}
[2020-05-26T17:13:56,531][DEBUG][logstash.agent           ] Starting agent
[2020-05-26T17:13:56,595][DEBUG][logstash.config.source.local.configpathloader] Skipping the following files while reading config since they don't match the specified glob pattern {:files=>["D:/Softwares/logstash-7.7.0/config/jvm.options", "D:/Softwares/logstash-7.7.0/config/log4j2.properties", "D:/Softwares/logstash-7.7.0/config/logstash.yml", "D:/Softwares/logstash-7.7.0/config/pipelines.yml", "D:/Softwares/logstash-7.7.0/config/startup.options"]}
[2020-05-26T17:13:56,600][DEBUG][logstash.config.source.local.configpathloader] Reading config file {:config_file=>"D:/Softwares/logstash-7.7.0/config/logstash-sample.conf"}
[2020-05-26T17:13:56,647][DEBUG][logstash.agent           ] Converging pipelines state {:actions_count=>1}
[2020-05-26T17:13:56,669][DEBUG][logstash.agent           ] Executing action {:action=>LogStash::PipelineAction::Create/pipeline_id:main}
[2020-05-26T17:13:57,670][DEBUG][org.logstash.secret.store.SecretStoreFactory] Attempting to exists or secret store with implementation: org.logstash.secret.store.backend.JavaKeyStore
[2020-05-26T17:13:58,215][DEBUG][org.reflections.Reflections] going to scan these urls:
jar:file:/D:/Softwares/logstash-7.7.0/logstash-core/lib/jars/logstash-core.jar!/
[2020-05-26T17:13:58,299][INFO ][org.reflections.Reflections] Reflections took 79 ms to scan 1 urls, producing 21 keys and 41 values 
[2020-05-26T17:13:58,325][DEBUG][org.reflections.Reflections] expanded subtype co.elastic.logstash.api.Plugin -> co.elastic.logstash.api.Input
[2020-05-26T17:13:58,326][DEBUG][org.reflections.Reflections] expanded subtype co.elastic.logstash.api.Plugin -> co.elastic.logstash.api.Codec
[2020-05-26T17:13:58,336][DEBUG][org.reflections.Reflections] expanded subtype org.jruby.RubyBasicObject -> org.jruby.RubyObject
[2020-05-26T17:13:58,338][DEBUG][org.reflections.Reflections] expanded subtype java.lang.Cloneable -> org.jruby.RubyBasicObject
[2020-05-26T17:13:58,342][DEBUG][org.reflections.Reflections] expanded subtype org.jruby.runtime.builtin.IRubyObject -> org.jruby.RubyBasicObject
[2020-05-26T17:13:58,352][DEBUG][org.reflections.Reflections] expanded subtype java.io.Serializable -> org.jruby.RubyBasicObject
[2020-05-26T17:13:58,357][DEBUG][org.reflections.Reflections] expanded subtype java.lang.Comparable -> org.jruby.RubyBasicObject
[2020-05-26T17:13:58,369][DEBUG][org.reflections.Reflections] expanded subtype org.jruby.runtime.marshal.CoreObjectType -> org.jruby.RubyBasicObject
[2020-05-26T17:13:58,373][DEBUG][org.reflections.Reflections] expanded subtype org.jruby.runtime.builtin.InstanceVariables -> org.jruby.RubyBasicObject
[2020-05-26T17:13:58,386][DEBUG][org.reflections.Reflections] expanded subtype org.jruby.runtime.builtin.InternalVariables -> org.jruby.RubyBasicObject
[2020-05-26T17:13:58,390][DEBUG][org.reflections.Reflections] expanded subtype co.elastic.logstash.api.Plugin -> co.elastic.logstash.api.Output
[2020-05-26T17:13:58,396][DEBUG][org.reflections.Reflections] expanded subtype co.elastic.logstash.api.Metric -> co.elastic.logstash.api.NamespacedMetric
[2020-05-26T17:13:58,397][DEBUG][org.reflections.Reflections] expanded subtype java.security.SecureClassLoader -> java.net.URLClassLoader
[2020-05-26T17:13:58,402][DEBUG][org.reflections.Reflections] expanded subtype java.lang.ClassLoader -> java.security.SecureClassLoader
[2020-05-26T17:13:58,413][DEBUG][org.reflections.Reflections] expanded subtype java.io.Closeable -> java.net.URLClassLoader
[2020-05-26T17:13:58,416][DEBUG][org.reflections.Reflections] expanded subtype java.lang.AutoCloseable -> java.io.Closeable
[2020-05-26T17:13:58,423][DEBUG][org.reflections.Reflections] expanded subtype java.lang.Comparable -> java.lang.Enum
[2020-05-26T17:13:58,424][DEBUG][org.reflections.Reflections] expanded subtype java.io.Serializable -> java.lang.Enum
[2020-05-26T17:13:58,431][DEBUG][org.reflections.Reflections] expanded subtype co.elastic.logstash.api.Plugin -> co.elastic.logstash.api.Filter
[2020-05-26T17:13:59,171][DEBUG][logstash.plugins.registry] On demand adding plugin to the registry {:name=>"file", :type=>"input", :class=>LogStash::Inputs::File}
[2020-05-26T17:13:59,314][DEBUG][logstash.plugins.registry] On demand adding plugin to the registry {:name=>"plain", :type=>"codec", :class=>LogStash::Codecs::Plain}
[2020-05-26T17:13:59,333][DEBUG][logstash.codecs.plain    ] config LogStash::Codecs::Plain/@id = "plain_9b408066-1ce4-41cf-9261-3cff5d143f02"
[2020-05-26T17:13:59,335][DEBUG][logstash.codecs.plain    ] config LogStash::Codecs::Plain/@enable_metric = true
[2020-05-26T17:13:59,337][DEBUG][logstash.codecs.plain    ] config LogStash::Codecs::Plain/@charset = "UTF-8"
[2020-05-26T17:13:59,372][DEBUG][logstash.inputs.file     ] config LogStash::Inputs::File/@path = ["C:/Users/karan/Desktop/*.log"]
[2020-05-26T17:13:59,374][DEBUG][logstash.inputs.file     ] config LogStash::Inputs::File/@id = "57578b95369b2238c183943d813809ecc891db9e70c0a43cafaf3000c9c37372"
[2020-05-26T17:13:59,387][DEBUG][logstash.inputs.file     ] config LogStash::Inputs::File/@type = "applog"
[2020-05-26T17:13:59,397][DEBUG][logstash.inputs.file     ] config LogStash::Inputs::File/@tags = ["applog"]
[2020-05-26T17:13:59,406][DEBUG][logstash.inputs.file     ] config LogStash::Inputs::File/@enable_metric = true
[2020-05-26T17:13:59,422][DEBUG][logstash.inputs.file     ] config LogStash::Inputs::File/@codec = <LogStash::Codecs::Plain id=>"plain_9b408066-1ce4-41cf-9261-3cff5d143f02", enable_metric=>true, charset=>"UTF-8">
[2020-05-26T17:13:59,425][DEBUG][logstash.inputs.file     ] config LogStash::Inputs::File/@add_field = {}
[2020-05-26T17:13:59,433][DEBUG][logstash.inputs.file     ] config LogStash::Inputs::File/@stat_interval = 1.0
[2020-05-26T17:13:59,437][DEBUG][logstash.inputs.file     ] config LogStash::Inputs::File/@discover_interval = 15
[2020-05-26T17:13:59,451][DEBUG][logstash.inputs.file     ] config LogStash::Inputs::File/@sincedb_write_interval = 15.0
[2020-05-26T17:13:59,455][DEBUG][logstash.inputs.file     ] config LogStash::Inputs::File/@start_position = "end"
[2020-05-26T17:13:59,468][DEBUG][logstash.inputs.file     ] config LogStash::Inputs::File/@delimiter = "\n"
[2020-05-26T17:13:59,472][DEBUG][logstash.inputs.file     ] config LogStash::Inputs::File/@close_older = 3600.0
[2020-05-26T17:13:59,485][DEBUG][logstash.inputs.file     ] config LogStash::Inputs::File/@mode = "tail"
[2020-05-26T17:13:59,489][DEBUG][logstash.inputs.file     ] config LogStash::Inputs::File/@file_completed_action = "delete"
[2020-05-26T17:13:59,496][DEBUG][logstash.inputs.file     ] config LogStash::Inputs::File/@sincedb_clean_after = 1209600.0
[2020-05-26T17:13:59,496][DEBUG][logstash.inputs.file     ] config LogStash::Inputs::File/@file_chunk_size = 32768
[2020-05-26T17:13:59,498][DEBUG][logstash.inputs.file     ] config LogStash::Inputs::File/@file_chunk_count = 140737488355327
[2020-05-26T17:13:59,508][DEBUG][logstash.inputs.file     ] config LogStash::Inputs::File/@file_sort_by = "last_modified"
[2020-05-26T17:13:59,509][DEBUG][logstash.inputs.file     ] config LogStash::Inputs::File/@file_sort_direction = "asc"
[2020-05-26T17:13:59,511][DEBUG][logstash.inputs.file     ] config LogStash::Inputs::File/@exit_after_read = false
[2020-05-26T17:13:59,654][DEBUG][logstash.plugins.registry] On demand adding plugin to the registry {:name=>"grok", :type=>"filter", :class=>LogStash::Filters::Grok}
[2020-05-26T17:13:59,670][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@match = {"message"=>"%{TIME:log_time} %{LOGLEVEL:log_level} %{DATA:class_name} %{GREEDYDATA :threadName}"}
[2020-05-26T17:13:59,671][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@id = "e68692e25e5d100f0ff915615303269508751e97a86606ceeb4a10d6258f0594"
[2020-05-26T17:13:59,672][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@enable_metric = true
[2020-05-26T17:13:59,674][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@add_tag = []
[2020-05-26T17:13:59,675][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@remove_tag = []
[2020-05-26T17:13:59,677][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@add_field = {}
[2020-05-26T17:13:59,678][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@remove_field = []
[2020-05-26T17:13:59,689][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@periodic_flush = false
[2020-05-26T17:13:59,690][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@patterns_dir = []
[2020-05-26T17:13:59,691][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@pattern_definitions = {}
[2020-05-26T17:13:59,692][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@patterns_files_glob = "*"
[2020-05-26T17:13:59,695][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@break_on_match = true
[2020-05-26T17:13:59,705][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@named_captures_only = true
[2020-05-26T17:13:59,706][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@keep_empty_captures = false
[2020-05-26T17:13:59,707][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@tag_on_failure = ["_grokparsefailure"]
[2020-05-26T17:13:59,708][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@timeout_millis = 30000
[2020-05-26T17:13:59,710][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@timeout_scope = "pattern"
[2020-05-26T17:13:59,719][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@tag_on_timeout = "_groktimeout"
[2020-05-26T17:13:59,720][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@overwrite = []
[2020-05-26T17:13:59,756][DEBUG][logstash.plugins.registry] On demand adding plugin to the registry {:name=>"file", :type=>"output", :class=>LogStash::Outputs::File}
[2020-05-26T17:13:59,792][DEBUG][logstash.plugins.registry] On demand adding plugin to the registry {:name=>"json_lines", :type=>"codec", :class=>LogStash::Codecs::JSONLines}
[2020-05-26T17:13:59,800][DEBUG][logstash.codecs.jsonlines] config LogStash::Codecs::JSONLines/@id = "json_lines_de8d9c71-4c29-4fbc-a193-9bbc9b6abf7f"
[2020-05-26T17:13:59,801][DEBUG][logstash.codecs.jsonlines] config LogStash::Codecs::JSONLines/@enable_metric = true
[2020-05-26T17:13:59,803][DEBUG][logstash.codecs.jsonlines] config LogStash::Codecs::JSONLines/@charset = "UTF-8"
[2020-05-26T17:13:59,804][DEBUG][logstash.codecs.jsonlines] config LogStash::Codecs::JSONLines/@delimiter = "\n"
[2020-05-26T17:13:59,812][DEBUG][logstash.outputs.file    ] config LogStash::Outputs::File/@path = "C:/Users/karan/Desktop/OUTPUT_LOGS/output.txt"
[2020-05-26T17:13:59,813][DEBUG][logstash.outputs.file    ] config LogStash::Outputs::File/@id = "7f4f544900ab0f14064ae8514d3672ab18add11ed23afaba42a559c140fc5741"
[2020-05-26T17:13:59,815][DEBUG][logstash.outputs.file    ] config LogStash::Outputs::File/@enable_metric = true
[2020-05-26T17:13:59,817][DEBUG][logstash.outputs.file    ] config LogStash::Outputs::File/@codec = <LogStash::Codecs::JSONLines id=>"json_lines_de8d9c71-4c29-4fbc-a193-9bbc9b6abf7f", enable_metric=>true, charset=>"UTF-8", delimiter=>"\n">
[2020-05-26T17:13:59,818][DEBUG][logstash.outputs.file    ] config LogStash::Outputs::File/@workers = 1
[2020-05-26T17:13:59,819][DEBUG][logstash.outputs.file    ] config LogStash::Outputs::File/@flush_interval = 2
[2020-05-26T17:13:59,825][DEBUG][logstash.outputs.file    ] config LogStash::Outputs::File/@gzip = false
[2020-05-26T17:13:59,827][DEBUG][logstash.outputs.file    ] config LogStash::Outputs::File/@filename_failure = "_filepath_failures"
[2020-05-26T17:13:59,828][DEBUG][logstash.outputs.file    ] config LogStash::Outputs::File/@create_if_deleted = true
[2020-05-26T17:13:59,829][DEBUG][logstash.outputs.file    ] config LogStash::Outputs::File/@dir_mode = -1
[2020-05-26T17:13:59,830][DEBUG][logstash.outputs.file    ] config LogStash::Outputs::File/@file_mode = -1
[2020-05-26T17:13:59,832][DEBUG][logstash.outputs.file    ] config LogStash::Outputs::File/@write_behavior = "append"
[2020-05-26T17:13:59,873][DEBUG][logstash.javapipeline    ] Starting pipeline {:pipeline_id=>"main"}
[2020-05-26T17:13:59,931][DEBUG][logstash.filters.grok    ][main] Grok patterns path {:paths=>["D:/Softwares/logstash-7.7.0/vendor/bundle/jruby/2.5.0/gems/logstash-patterns-core-4.1.2/patterns", "D:/Softwares/logstash-7.7.0/patterns/*"]}
[2020-05-26T17:13:59,952][DEBUG][logstash.filters.grok    ][main] Grok patterns path {:paths=>[]}
[2020-05-26T17:13:59,957][DEBUG][logstash.filters.grok    ][main] Match data {:match=>{"message"=>"%{TIME:log_time} %{LOGLEVEL:log_level} %{DATA:class_name} %{GREEDYDATA :threadName}"}}
[2020-05-26T17:13:59,965][DEBUG][logstash.filters.grok    ][main] regexp: /message {:pattern=>"%{TIME:log_time} %{LOGLEVEL:log_level} %{DATA:class_name} %{GREEDYDATA :threadName}"}
[2020-05-26T17:14:00,027][DEBUG][logstash.filters.grok    ][main] Adding pattern {"S3_REQUEST_LINE"=>"(?:%{WORD:verb} %{NOTSPACE:request}(?: HTTP/%{NUMBER:httpversion})?|%{DATA:rawrequest})"}
[2020-05-26T17:14:00,032][DEBUG][logstash.filters.grok    ][main] Adding pattern {"S3_ACCESS_LOG"=>"%{WORD:owner} %{NOTSPACE:bucket} \\[%{HTTPDATE:timestamp}\\] %{IP:clientip} %{NOTSPACE:requester} %{NOTSPACE:request_id} %{NOTSPACE:operation} %{NOTSPACE:key} (?:\"%{S3_REQUEST_LINE}\"|-) (?:%{INT:response:int}|-) (?:-|%{NOTSPACE:error_code}) (?:%{INT:bytes:int}|-) (?:%{INT:object_size:int}|-) (?:%{INT:request_time_ms:int}|-) (?:%{INT:turnaround_time_ms:int}|-) (?:%{QS:referrer}|-) (?:\"?%{QS:agent}\"?|-) (?:-|%{NOTSPACE:version_id})"}
[2020-05-26T17:14:00,037][DEBUG][logstash.filters.grok    ][main] Adding pattern {"ELB_URIPATHPARAM"=>"%{URIPATH:path}(?:%{URIPARAM:params})?"}
[2020-05-26T17:14:00,039][DEBUG][logstash.filters.grok    ][main] Adding pattern {"ELB_URI"=>"%{URIPROTO:proto}://(?:%{USER}(?::[^@]*)?@)?(?:%{URIHOST:urihost})?(?:%{ELB_URIPATHPARAM})?"}
[2020-05-26T17:14:00,040][DEBUG][logstash.filters.grok    ][main] Adding pattern {"ELB_REQUEST_LINE"=>"(?:%{WORD:verb} %{ELB_URI:request}(?: HTTP/%{NUMBER:httpversion})?|%{DATA:rawrequest})"}
[2020-05-26T17:14:00,041][DEBUG][logstash.filters.grok    ][main] Adding pattern {"ELB_ACCESS_LOG"=>"%{TIMESTAMP_ISO8601:timestamp} %{NOTSPACE:elb} %{IP:clientip}:%{INT:clientport:int} (?:(%{IP:backendip}:?:%{INT:backendport:int})|-) %{NUMBER:request_processing_time:float} %{NUMBER:backend_processing_time:float} %{NUMBER:response_processing_time:float} %{INT:response:int} %{INT:backend_response:int} %{INT:received_bytes:int} %{INT:bytes:int} \"%{ELB_REQUEST_LINE}\""}
[2020-05-26T17:14:00,046][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CLOUDFRONT_ACCESS_LOG"=>"(?<timestamp>%{YEAR}-%{MONTHNUM}-%{MONTHDAY}\\t%{TIME})\\t%{WORD:x_edge_location}\\t(?:%{NUMBER:sc_bytes:int}|-)\\t%{IPORHOST:clientip}\\t%{WORD:cs_method}\\t%{HOSTNAME:cs_host}\\t%{NOTSPACE:cs_uri_stem}\\t%{NUMBER:sc_status:int}\\t%{GREEDYDATA:referrer}\\t%{GREEDYDATA:agent}\\t%{GREEDYDATA:cs_uri_query}\\t%{GREEDYDATA:cookies}\\t%{WORD:x_edge_result_type}\\t%{NOTSPACE:x_edge_request_id}\\t%{HOSTNAME:x_host_header}\\t%{URIPROTO:cs_protocol}\\t%{INT:cs_bytes:int}\\t%{GREEDYDATA:time_taken:float}\\t%{GREEDYDATA:x_forwarded_for}\\t%{GREEDYDATA:ssl_protocol}\\t%{GREEDYDATA:ssl_cipher}\\t%{GREEDYDATA:x_edge_response_result_type}"}
[2020-05-26T17:14:00,066][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_TIMESTAMP"=>"%{MONTHDAY}-%{MONTH} %{HOUR}:%{MINUTE}"}
[2020-05-26T17:14:00,067][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_HOST"=>"[a-zA-Z0-9-]+"}
[2020-05-26T17:14:00,071][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_VOLUME"=>"%{USER}"}
[2020-05-26T17:14:00,075][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_DEVICE"=>"%{USER}"}
[2020-05-26T17:14:00,076][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_DEVICEPATH"=>"%{UNIXPATH}"}
[2020-05-26T17:14:00,078][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_CAPACITY"=>"%{INT}{1,3}(,%{INT}{3})*"}
[2020-05-26T17:14:00,079][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_VERSION"=>"%{USER}"}
[2020-05-26T17:14:00,086][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_JOB"=>"%{USER}"}
[2020-05-26T17:14:00,090][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_MAX_CAPACITY"=>"User defined maximum volume capacity %{BACULA_CAPACITY} exceeded on device \\\"%{BACULA_DEVICE:device}\\\" \\(%{BACULA_DEVICEPATH}\\)"}
[2020-05-26T17:14:00,091][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_END_VOLUME"=>"End of medium on Volume \\\"%{BACULA_VOLUME:volume}\\\" Bytes=%{BACULA_CAPACITY} Blocks=%{BACULA_CAPACITY} at %{MONTHDAY}-%{MONTH}-%{YEAR} %{HOUR}:%{MINUTE}."}
[2020-05-26T17:14:00,096][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_NEW_VOLUME"=>"Created new Volume \\\"%{BACULA_VOLUME:volume}\\\" in catalog."}
[2020-05-26T17:14:00,101][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_NEW_LABEL"=>"Labeled new Volume \\\"%{BACULA_VOLUME:volume}\\\" on device \\\"%{BACULA_DEVICE:device}\\\" \\(%{BACULA_DEVICEPATH}\\)."}
[2020-05-26T17:14:00,105][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_WROTE_LABEL"=>"Wrote label to prelabeled Volume \\\"%{BACULA_VOLUME:volume}\\\" on device \\\"%{BACULA_DEVICE}\\\" \\(%{BACULA_DEVICEPATH}\\)"}
[2020-05-26T17:14:00,106][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_NEW_MOUNT"=>"New volume \\\"%{BACULA_VOLUME:volume}\\\" mounted on device \\\"%{BACULA_DEVICE:device}\\\" \\(%{BACULA_DEVICEPATH}\\) at %{MONTHDAY}-%{MONTH}-%{YEAR} %{HOUR}:%{MINUTE}."}
[2020-05-26T17:14:00,107][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_NOOPEN"=>"\\s+Cannot open %{DATA}: ERR=%{GREEDYDATA:berror}"}
[2020-05-26T17:14:00,108][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_NOOPENDIR"=>"\\s+Could not open directory %{DATA}: ERR=%{GREEDYDATA:berror}"}
[2020-05-26T17:14:00,111][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_NOSTAT"=>"\\s+Could not stat %{DATA}: ERR=%{GREEDYDATA:berror}"}
[2020-05-26T17:14:00,120][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_NOJOBS"=>"There are no more Jobs associated with Volume \\\"%{BACULA_VOLUME:volume}\\\". Marking it purged."}
[2020-05-26T17:14:00,122][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_ALL_RECORDS_PRUNED"=>"All records pruned from Volume \\\"%{BACULA_VOLUME:volume}\\\"; marking it \\\"Purged\\\""}
[2020-05-26T17:14:00,123][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_BEGIN_PRUNE_JOBS"=>"Begin pruning Jobs older than %{INT} month %{INT} days ."}
[2020-05-26T17:14:00,124][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_BEGIN_PRUNE_FILES"=>"Begin pruning Files."}
[2020-05-26T17:14:00,149][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_PRUNED_JOBS"=>"Pruned %{INT} Jobs* for client %{BACULA_HOST:client} from catalog."}
[2020-05-26T17:14:00,150][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_PRUNED_FILES"=>"Pruned Files from %{INT} Jobs* for client %{BACULA_HOST:client} from catalog."}
[2020-05-26T17:14:00,154][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_ENDPRUNE"=>"End auto prune."}
[2020-05-26T17:14:00,155][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_STARTJOB"=>"Start Backup JobId %{INT}, Job=%{BACULA_JOB:job}"}
[2020-05-26T17:14:00,156][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_STARTRESTORE"=>"Start Restore Job %{BACULA_JOB:job}"}
[2020-05-26T17:14:00,158][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_USEDEVICE"=>"Using Device \\\"%{BACULA_DEVICE:device}\\\""}
[2020-05-26T17:14:00,164][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_DIFF_FS"=>"\\s+%{UNIXPATH} is a different filesystem. Will not descend from %{UNIXPATH} into it."}
[2020-05-26T17:14:00,168][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_JOBEND"=>"Job write elapsed time = %{DATA:elapsed}, Transfer rate = %{NUMBER} (K|M|G)? Bytes/second"}
[2020-05-26T17:14:00,169][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_NOPRUNE_JOBS"=>"No Jobs found to prune."}
[2020-05-26T17:14:00,170][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_NOPRUNE_FILES"=>"No Files found to prune."}
[2020-05-26T17:14:00,171][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_VOLUME_PREVWRITTEN"=>"Volume \\\"%{BACULA_VOLUME:volume}\\\" previously written, moving to end of data."}
[2020-05-26T17:14:00,172][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_READYAPPEND"=>"Ready to append to end of Volume \\\"%{BACULA_VOLUME:volume}\\\" size=%{INT}"}
[2020-05-26T17:14:00,179][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_CANCELLING"=>"Cancelling duplicate JobId=%{INT}."}
[2020-05-26T17:14:00,182][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_MARKCANCEL"=>"JobId %{INT}, Job %{BACULA_JOB:job} marked to be canceled."}
[2020-05-26T17:14:00,183][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_CLIENT_RBJ"=>"shell command: run ClientRunBeforeJob \\\"%{GREEDYDATA:runjob}\\\""}
[2020-05-26T17:14:00,184][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_VSS"=>"(Generate )?VSS (Writer)?"}
[2020-05-26T17:14:00,185][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_MAXSTART"=>"Fatal error: Job canceled because max start delay time exceeded."}
[2020-05-26T17:14:00,187][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_DUPLICATE"=>"Fatal error: JobId %{INT:duplicate} already running. Duplicate job not allowed."}
[2020-05-26T17:14:00,194][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_NOJOBSTAT"=>"Fatal error: No Job status returned from FD."}
[2020-05-26T17:14:00,196][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_FATAL_CONN"=>"Fatal error: bsock.c:133 Unable to connect to (Client: %{BACULA_HOST:client}|Storage daemon) on %{HOSTNAME}:%{POSINT}. ERR=(?<berror>%{GREEDYDATA})"}
[2020-05-26T17:14:00,197][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_NO_CONNECT"=>"Warning: bsock.c:127 Could not connect to (Client: %{BACULA_HOST:client}|Storage daemon) on %{HOSTNAME}:%{POSINT}. ERR=(?<berror>%{GREEDYDATA})"}
[2020-05-26T17:14:00,199][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_NO_AUTH"=>"Fatal error: Unable to authenticate with File daemon at %{HOSTNAME}. Possible causes:"}
[2020-05-26T17:14:00,200][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_NOSUIT"=>"No prior or suitable Full backup found in catalog. Doing FULL backup."}
[2020-05-26T17:14:00,201][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_NOPRIOR"=>"No prior Full backup Job record found."}
[2020-05-26T17:14:00,203][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_JOB"=>"(Error: )?Bacula %{BACULA_HOST} %{BACULA_VERSION} \\(%{BACULA_VERSION}\\):"}
[2020-05-26T17:14:00,211][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOGLINE"=>"%{BACULA_TIMESTAMP:bts} %{BACULA_HOST:hostname} JobId %{INT:jobid}: (%{BACULA_LOG_MAX_CAPACITY}|%{BACULA_LOG_END_VOLUME}|%{BACULA_LOG_NEW_VOLUME}|%{BACULA_LOG_NEW_LABEL}|%{BACULA_LOG_WROTE_LABEL}|%{BACULA_LOG_NEW_MOUNT}|%{BACULA_LOG_NOOPEN}|%{BACULA_LOG_NOOPENDIR}|%{BACULA_LOG_NOSTAT}|%{BACULA_LOG_NOJOBS}|%{BACULA_LOG_ALL_RECORDS_PRUNED}|%{BACULA_LOG_BEGIN_PRUNE_JOBS}|%{BACULA_LOG_BEGIN_PRUNE_FILES}|%{BACULA_LOG_PRUNED_JOBS}|%{BACULA_LOG_PRUNED_FILES}|%{BACULA_LOG_ENDPRUNE}|%{BACULA_LOG_STARTJOB}|%{BACULA_LOG_STARTRESTORE}|%{BACULA_LOG_USEDEVICE}|%{BACULA_LOG_DIFF_FS}|%{BACULA_LOG_JOBEND}|%{BACULA_LOG_NOPRUNE_JOBS}|%{BACULA_LOG_NOPRUNE_FILES}|%{BACULA_LOG_VOLUME_PREVWRITTEN}|%{BACULA_LOG_READYAPPEND}|%{BACULA_LOG_CANCELLING}|%{BACULA_LOG_MARKCANCEL}|%{BACULA_LOG_CLIENT_RBJ}|%{BACULA_LOG_VSS}|%{BACULA_LOG_MAXSTART}|%{BACULA_LOG_DUPLICATE}|%{BACULA_LOG_NOJOBSTAT}|%{BACULA_LOG_FATAL_CONN}|%{BACULA_LOG_NO_CONNECT}|%{BACULA_LOG_NO_AUTH}|%{BACULA_LOG_NOSUIT}|%{BACULA_LOG_JOB}|%{BACULA_LOG_NOPRIOR})"}
[2020-05-26T17:14:00,219][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BIND9_TIMESTAMP"=>"%{MONTHDAY}[-]%{MONTH}[-]%{YEAR} %{TIME}"}
[2020-05-26T17:14:00,224][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BIND9"=>"%{BIND9_TIMESTAMP:timestamp} queries: %{LOGLEVEL:loglevel}: client %{IP:clientip}#%{POSINT:clientport} \\(%{GREEDYDATA:query}\\): query: %{GREEDYDATA:query} IN %{GREEDYDATA:querytype} \\(%{IP:dns}\\)"}
[2020-05-26T17:14:00,243][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BRO_HTTP"=>"%{NUMBER:ts}\\t%{NOTSPACE:uid}\\t%{IP:orig_h}\\t%{INT:orig_p}\\t%{IP:resp_h}\\t%{INT:resp_p}\\t%{INT:trans_depth}\\t%{GREEDYDATA:method}\\t%{GREEDYDATA:domain}\\t%{GREEDYDATA:uri}\\t%{GREEDYDATA:referrer}\\t%{GREEDYDATA:user_agent}\\t%{NUMBER:request_body_len}\\t%{NUMBER:response_body_len}\\t%{GREEDYDATA:status_code}\\t%{GREEDYDATA:status_msg}\\t%{GREEDYDATA:info_code}\\t%{GREEDYDATA:info_msg}\\t%{GREEDYDATA:filename}\\t%{GREEDYDATA:bro_tags}\\t%{GREEDYDATA:username}\\t%{GREEDYDATA:password}\\t%{GREEDYDATA:proxied}\\t%{GREEDYDATA:orig_fuids}\\t%{GREEDYDATA:orig_mime_types}\\t%{GREEDYDATA:resp_fuids}\\t%{GREEDYDATA:resp_mime_types}"}
[2020-05-26T17:14:00,244][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BRO_DNS"=>"%{NUMBER:ts}\\t%{NOTSPACE:uid}\\t%{IP:orig_h}\\t%{INT:orig_p}\\t%{IP:resp_h}\\t%{INT:resp_p}\\t%{WORD:proto}\\t%{INT:trans_id}\\t%{GREEDYDATA:query}\\t%{GREEDYDATA:qclass}\\t%{GREEDYDATA:qclass_name}\\t%{GREEDYDATA:qtype}\\t%{GREEDYDATA:qtype_name}\\t%{GREEDYDATA:rcode}\\t%{GREEDYDATA:rcode_name}\\t%{GREEDYDATA:AA}\\t%{GREEDYDATA:TC}\\t%{GREEDYDATA:RD}\\t%{GREEDYDATA:RA}\\t%{GREEDYDATA:Z}\\t%{GREEDYDATA:answers}\\t%{GREEDYDATA:TTLs}\\t%{GREEDYDATA:rejected}"}
[2020-05-26T17:14:00,246][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BRO_CONN"=>"%{NUMBER:ts}\\t%{NOTSPACE:uid}\\t%{IP:orig_h}\\t%{INT:orig_p}\\t%{IP:resp_h}\\t%{INT:resp_p}\\t%{WORD:proto}\\t%{GREEDYDATA:service}\\t%{NUMBER:duration}\\t%{NUMBER:orig_bytes}\\t%{NUMBER:resp_bytes}\\t%{GREEDYDATA:conn_state}\\t%{GREEDYDATA:local_orig}\\t%{GREEDYDATA:missed_bytes}\\t%{GREEDYDATA:history}\\t%{GREEDYDATA:orig_pkts}\\t%{GREEDYDATA:orig_ip_bytes}\\t%{GREEDYDATA:resp_pkts}\\t%{GREEDYDATA:resp_ip_bytes}\\t%{GREEDYDATA:tunnel_parents}"}
[2020-05-26T17:14:00,253][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BRO_FILES"=>"%{NUMBER:ts}\\t%{NOTSPACE:fuid}\\t%{IP:tx_hosts}\\t%{IP:rx_hosts}\\t%{NOTSPACE:conn_uids}\\t%{GREEDYDATA:source}\\t%{GREEDYDATA:depth}\\t%{GREEDYDATA:analyzers}\\t%{GREEDYDATA:mime_type}\\t%{GREEDYDATA:filename}\\t%{GREEDYDATA:duration}\\t%{GREEDYDATA:local_orig}\\t%{GREEDYDATA:is_orig}\\t%{GREEDYDATA:seen_bytes}\\t%{GREEDYDATA:total_bytes}\\t%{GREEDYDATA:missing_bytes}\\t%{GREEDYDATA:overflow_bytes}\\t%{GREEDYDATA:timedout}\\t%{GREEDYDATA:parent_fuid}\\t%{GREEDYDATA:md5}\\t%{GREEDYDATA:sha1}\\t%{GREEDYDATA:sha256}\\t%{GREEDYDATA:extracted}"}
[2020-05-26T17:14:00,275][DEBUG][logstash.filters.grok    ][main] Adding pattern {"EXIM_MSGID"=>"[0-9A-Za-z]{6}-[0-9A-Za-z]{6}-[0-9A-Za-z]{2}"}
[2020-05-26T17:14:00,276][DEBUG][logstash.filters.grok    ][main] Adding pattern {"EXIM_FLAGS"=>"(<=|[-=>*]>|[*]{2}|==)"}
[2020-05-26T17:14:00,283][DEBUG][logstash.filters.grok    ][main] Adding pattern {"EXIM_DATE"=>"%{YEAR:exim_year}-%{MONTHNUM:exim_month}-%{MONTHDAY:exim_day} %{TIME:exim_time}"}
[2020-05-26T17:14:00,284][DEBUG][logstash.filters.grok    ][main] Adding pattern {"EXIM_PID"=>"\\[%{POSINT}\\]"}
[2020-05-26T17:14:00,285][DEBUG][logstash.filters.grok    ][main] Adding pattern {"EXIM_QT"=>"((\\d+y)?(\\d+w)?(\\d+d)?(\\d+h)?(\\d+m)?(\\d+s)?)"}
[2020-05-26T17:14:00,286][DEBUG][logstash.filters.grok    ][main] Adding pattern {"EXIM_EXCLUDE_TERMS"=>"(Message is frozen|(Start|End) queue run| Warning: | retry time not reached | no (IP address|host name) found for (IP address|host) | unexpected disconnection while reading SMTP command | no immediate delivery: |another process is handling this message)"}
[2020-05-26T17:14:00,287][DEBUG][logstash.filters.grok    ][main] Adding pattern {"EXIM_REMOTE_HOST"=>"(H=(%{NOTSPACE:remote_hostname} )?(\\(%{NOTSPACE:remote_heloname}\\) )?\\[%{IP:remote_host}\\])"}
[2020-05-26T17:14:00,288][DEBUG][logstash.filters.grok    ][main] Adding pattern {"EXIM_INTERFACE"=>"(I=\\[%{IP:exim_interface}\\](:%{NUMBER:exim_interface_port}))"}
[2020-05-26T17:14:00,295][DEBUG][logstash.filters.grok    ][main] Adding pattern {"EXIM_PROTOCOL"=>"(P=%{NOTSPACE:protocol})"}
[2020-05-26T17:14:00,296][DEBUG][logstash.filters.grok    ][main] Adding pattern {"EXIM_MSG_SIZE"=>"(S=%{NUMBER:exim_msg_size})"}
[2020-05-26T17:14:00,297][DEBUG][logstash.filters.grok    ][main] Adding pattern {"EXIM_HEADER_ID"=>"(id=%{NOTSPACE:exim_header_id})"}
[2020-05-26T17:14:00,298][DEBUG][logstash.filters.grok    ][main] Adding pattern {"EXIM_SUBJECT"=>"(T=%{QS:exim_subject})"}
[2020-05-26T17:14:00,310][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NETSCREENSESSIONLOG"=>"%{SYSLOGTIMESTAMP:date} %{IPORHOST:device} %{IPORHOST}: NetScreen device_id=%{WORD:device_id}%{DATA}: start_time=%{QUOTEDSTRING:start_time} duration=%{INT:duration} policy_id=%{INT:policy_id} service=%{DATA:service} proto=%{INT:proto} src zone=%{WORD:src_zone} dst zone=%{WORD:dst_zone} action=%{WORD:action} sent=%{INT:sent} rcvd=%{INT:rcvd} src=%{IPORHOST:src_ip} dst=%{IPORHOST:dst_ip} src_port=%{INT:src_port} dst_port=%{INT:dst_port} src-xlated ip=%{IPORHOST:src_xlated_ip} port=%{INT:src_xlated_port} dst-xlated ip=%{IPORHOST:dst_xlated_ip} port=%{INT:dst_xlated_port} session_id=%{INT:session_id} reason=%{GREEDYDATA:reason}"}
[2020-05-26T17:14:00,311][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCO_TAGGED_SYSLOG"=>"^<%{POSINT:syslog_pri}>%{CISCOTIMESTAMP:timestamp}( %{SYSLOGHOST:sysloghost})? ?: %%{CISCOTAG:ciscotag}:"}
[2020-05-26T17:14:00,312][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOTIMESTAMP"=>"%{MONTH} +%{MONTHDAY}(?: %{YEAR})? %{TIME}"}
[2020-05-26T17:14:00,313][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOTAG"=>"[A-Z0-9]+-%{INT}-(?:[A-Z0-9_]+)"}
[2020-05-26T17:14:00,314][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCO_ACTION"=>"Built|Teardown|Deny|Denied|denied|requested|permitted|denied by ACL|discarded|est-allowed|Dropping|created|deleted"}
[2020-05-26T17:14:00,315][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCO_REASON"=>"Duplicate TCP SYN|Failed to locate egress interface|Invalid transport field|No matching connection|DNS Response|DNS Query|(?:%{WORD}\\s*)*"}
[2020-05-26T17:14:00,316][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCO_DIRECTION"=>"Inbound|inbound|Outbound|outbound"}
[2020-05-26T17:14:00,317][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCO_INTERVAL"=>"first hit|%{INT}-second interval"}
[2020-05-26T17:14:00,326][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCO_XLATE_TYPE"=>"static|dynamic"}
[2020-05-26T17:14:00,327][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOFW104001"=>"\\((?:Primary|Secondary)\\) Switching to ACTIVE - %{GREEDYDATA:switch_reason}"}
[2020-05-26T17:14:00,328][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOFW104002"=>"\\((?:Primary|Secondary)\\) Switching to STANDBY - %{GREEDYDATA:switch_reason}"}
[2020-05-26T17:14:00,329][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOFW104003"=>"\\((?:Primary|Secondary)\\) Switching to FAILED\\."}
[2020-05-26T17:14:00,330][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOFW104004"=>"\\((?:Primary|Secondary)\\) Switching to OK\\."}
[2020-05-26T17:14:00,331][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOFW105003"=>"\\((?:Primary|Secondary)\\) Monitoring on [Ii]nterface %{GREEDYDATA:interface_name} waiting"}
[2020-05-26T17:14:00,331][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOFW105004"=>"\\((?:Primary|Secondary)\\) Monitoring on [Ii]nterface %{GREEDYDATA:interface_name} normal"}
[2020-05-26T17:14:00,332][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOFW105005"=>"\\((?:Primary|Secondary)\\) Lost Failover communications with mate on [Ii]nterface %{GREEDYDATA:interface_name}"}
[2020-05-26T17:14:00,333][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOFW105008"=>"\\((?:Primary|Secondary)\\) Testing [Ii]nterface %{GREEDYDATA:interface_name}"}
[2020-05-26T17:14:00,334][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOFW105009"=>"\\((?:Primary|Secondary)\\) Testing on [Ii]nterface %{GREEDYDATA:interface_name} (?:Passed|Failed)"}
[2020-05-26T17:14:00,343][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOFW106001"=>"%{CISCO_DIRECTION:direction} %{WORD:protocol} connection %{CISCO_ACTION:action} from %{IP:src_ip}/%{INT:src_port} to %{IP:dst_ip}/%{INT:dst_port} flags %{GREEDYDATA:tcp_flags} on interface %{GREEDYDATA:interface}"}
[2020-05-26T17:14:00,344][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOFW106006_106007_106010"=>"%{CISCO_ACTION:action} %{CISCO_DIRECTION:direction} %{WORD:protocol} (?:from|src) %{IP:src_ip}/%{INT:src_port}(\\(%{DATA:src_fwuser}\\))? (?:to|dst) %{IP:dst_ip}/%{INT:dst_port}(\\(%{DATA:dst_fwuser}\\))? (?:on interface %{DATA:interface}|due to %{CISCO_REASON:reason})"}
[2020-05-26T17:14:00,346][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOFW106014"=>"%{CISCO_ACTION:action} %{CISCO_DIRECTION:direction} %{WORD:protocol} src %{DATA:src_interface}:%{IP:src_ip}(\\(%{DATA:src_fwuser}\\))? dst %{DATA:dst_interface}:%{IP:dst_ip}(\\(%{DATA:dst_fwuser}\\))? \\(type %{INT:icmp_type}, code %{INT:icmp_code}\\)"}
[2020-05-26T17:14:00,347][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOFW106015"=>"%{CISCO_ACTION:action} %{WORD:protocol} \\(%{DATA:policy_id}\\) from %{IP:src_ip}/%{INT:src_port} to %{IP:dst_ip}/%{INT:dst_port} flags %{DATA:tcp_flags} on interface %{GREEDYDATA:interface}"}
[2020-05-26T17:14:00,348][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOFW106021"=>"%{CISCO_ACTION:action} %{WORD:protocol} reverse path check from %{IP:src_ip} to %{IP:dst_ip} on interface %{GREEDYDATA:interface}"}
[2020-05-26T17:14:00,349][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOFW106023"=>"%{CISCO_ACTION:action}( protocol)? %{WORD:protocol} src %{DATA:src_interface}:%{DATA:src_ip}(/%{INT:src_port})?(\\(%{DATA:src_fwuser}\\))? dst %{DATA:dst_interface}:%{DATA:dst_ip}(/%{INT:dst_port})?(\\(%{DATA:dst_fwuser}\\))?( \\(type %{INT:icmp_type}, code %{INT:icmp_code}\\))? by access-group \"?%{DATA:policy_id}\"? \\[%{DATA:hashcode1}, %{DATA:hashcode2}\\]"}
[2020-05-26T17:14:00,350][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOFW106100_2_3"=>"access-list %{NOTSPACE:policy_id} %{CISCO_ACTION:action} %{WORD:protocol} for user '%{DATA:src_fwuser}' %{DATA:src_interface}/%{IP:src_ip}\\(%{INT:src_port}\\) -> %{DATA:dst_interface}/%{IP:dst_ip}\\(%{INT:dst_port}\\) hit-cnt %{INT:hit_count} %{CISCO_INTERVAL:interval} \\[%{DATA:hashcode1}, %{DATA:hashcode2}\\]"}
[2020-05-26T17:14:00,352][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOFW106100"=>"access-list %{NOTSPACE:policy_id} %{CISCO_ACTION:action} %{WORD:protocol} %{DATA:src_interface}/%{IP:src_ip}\\(%{INT:src_port}\\)(\\(%{DATA:src_fwuser}\\))? -> %{DATA:dst_interface}/%{IP:dst_ip}\\(%{INT:dst_port}\\)(\\(%{DATA:src_fwuser}\\))? hit-cnt %{INT:hit_count} %{CISCO_INTERVAL:interval} \\[%{DATA:hashcode1}, %{DATA:hashcode2}\\]"}
[2020-05-26T17:14:00,361][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOFW304001"=>"%{IP:src_ip}(\\(%{DATA:src_fwuser}\\))? Accessed URL %{IP:dst_ip}:%{GREEDYDATA:dst_url}"}
[2020-05-26T17:14:00,362][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOFW110002"=>"%{CISCO_REASON:reason} for %{WORD:protocol} from %{DATA:src_interface}:%{IP:src_ip}/%{INT:src_port} to %{IP:dst_ip}/%{INT:dst_port}"}
[2020-05-26T17:14:00,363][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOFW302010"=>"%{INT:connection_count} in use, %{INT:connection_count_max} most used"}
[2020-05-26T17:14:00,364][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOFW302013_302014_302015_302016"=>"%{CISCO_ACTION:action}(?: %{CISCO_DIRECTION:direction})? %{WORD:protocol} connection %{INT:connection_id} for %{DATA:src_interface}:%{IP:src_ip}/%{INT:src_port}( \\(%{IP:src_mapped_ip}/%{INT:src_mapped_port}\\))?(\\(%{DATA:src_fwuser}\\))? to %{DATA:dst_interface}:%{IP:dst_ip}/%{INT:dst_port}( \\(%{IP:dst_mapped_ip}/%{INT:dst_mapped_port}\\))?(\\(%{DATA:dst_fwuser}\\))?( duration %{TIME:duration} bytes %{INT:bytes})?(?: %{CISCO_REASON:reason})?( \\(%{DATA:user}\\))?"}
[2020-05-26T17:14:00,365][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOFW302020_302021"=>"%{CISCO_ACTION:action}(?: %{CISCO_DIRECTION:direction})? %{WORD:protocol} connection for faddr %{IP:dst_ip}/%{INT:icmp_seq_num}(?:\\(%{DATA:fwuser}\\))? gaddr %{IP:src_xlated_ip}/%{INT:icmp_code_xlated} laddr %{IP:src_ip}/%{INT:icmp_code}( \\(%{DATA:user}\\))?"}
[2020-05-26T17:14:00,367][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOFW305011"=>"%{CISCO_ACTION:action} %{CISCO_XLATE_TYPE:xlate_type} %{WORD:protocol} translation from %{DATA:src_interface}:%{IP:src_ip}(/%{INT:src_port})?(\\(%{DATA:src_fwuser}\\))? to %{DATA:src_xlated_interface}:%{IP:src_xlated_ip}/%{DATA:src_xlated_port}"}
[2020-05-26T17:14:00,368][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOFW313001_313004_313008"=>"%{CISCO_ACTION:action} %{WORD:protocol} type=%{INT:icmp_type}, code=%{INT:icmp_code} from %{IP:src_ip} on interface %{DATA:interface}( to %{IP:dst_ip})?"}
[2020-05-26T17:14:00,369][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOFW313005"=>"%{CISCO_REASON:reason} for %{WORD:protocol} error message: %{WORD:err_protocol} src %{DATA:err_src_interface}:%{IP:err_src_ip}(\\(%{DATA:err_src_fwuser}\\))? dst %{DATA:err_dst_interface}:%{IP:err_dst_ip}(\\(%{DATA:err_dst_fwuser}\\))? \\(type %{INT:err_icmp_type}, code %{INT:err_icmp_code}\\) on %{DATA:interface} interface\\.  Original IP payload: %{WORD:protocol} src %{IP:orig_src_ip}/%{INT:orig_src_port}(\\(%{DATA:orig_src_fwuser}\\))? dst %{IP:orig_dst_ip}/%{INT:orig_dst_port}(\\(%{DATA:orig_dst_fwuser}\\))?"}
[2020-05-26T17:14:00,378][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOFW321001"=>"Resource '%{WORD:resource_name}' limit of %{POSINT:resource_limit} reached for system"}
[2020-05-26T17:14:00,379][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOFW402117"=>"%{WORD:protocol}: Received a non-IPSec packet \\(protocol= %{WORD:orig_protocol}\\) from %{IP:src_ip} to %{IP:dst_ip}"}
[2020-05-26T17:14:00,381][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOFW402119"=>"%{WORD:protocol}: Received an %{WORD:orig_protocol} packet \\(SPI= %{DATA:spi}, sequence number= %{DATA:seq_num}\\) from %{IP:src_ip} \\(user= %{DATA:user}\\) to %{IP:dst_ip} that failed anti-replay checking"}
[2020-05-26T17:14:00,382][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOFW419001"=>"%{CISCO_ACTION:action} %{WORD:protocol} packet from %{DATA:src_interface}:%{IP:src_ip}/%{INT:src_port} to %{DATA:dst_interface}:%{IP:dst_ip}/%{INT:dst_port}, reason: %{GREEDYDATA:reason}"}
[2020-05-26T17:14:00,383][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOFW419002"=>"%{CISCO_REASON:reason} from %{DATA:src_interface}:%{IP:src_ip}/%{INT:src_port} to %{DATA:dst_interface}:%{IP:dst_ip}/%{INT:dst_port} with different initial sequence number"}
[2020-05-26T17:14:00,384][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOFW500004"=>"%{CISCO_REASON:reason} for protocol=%{WORD:protocol}, from %{IP:src_ip}/%{INT:src_port} to %{IP:dst_ip}/%{INT:dst_port}"}
[2020-05-26T17:14:00,385][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOFW602303_602304"=>"%{WORD:protocol}: An %{CISCO_DIRECTION:direction} %{GREEDYDATA:tunnel_type} SA \\(SPI= %{DATA:spi}\\) between %{IP:src_ip} and %{IP:dst_ip} \\(user= %{DATA:user}\\) has been %{CISCO_ACTION:action}"}
[2020-05-26T17:14:00,387][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOFW710001_710002_710003_710005_710006"=>"%{WORD:protocol} (?:request|access) %{CISCO_ACTION:action} from %{IP:src_ip}/%{INT:src_port} to %{DATA:dst_interface}:%{IP:dst_ip}/%{INT:dst_port}"}
[2020-05-26T17:14:00,396][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOFW713172"=>"Group = %{GREEDYDATA:group}, IP = %{IP:src_ip}, Automatic NAT Detection Status:\\s+Remote end\\s*%{DATA:is_remote_natted}\\s*behind a NAT device\\s+This\\s+end\\s*%{DATA:is_local_natted}\\s*behind a NAT device"}
[2020-05-26T17:14:00,396][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOFW733100"=>"\\[\\s*%{DATA:drop_type}\\s*\\] drop %{DATA:drop_rate_id} exceeded. Current burst rate is %{INT:drop_rate_current_burst} per second, max configured rate is %{INT:drop_rate_max_burst}; Current average rate is %{INT:drop_rate_current_avg} per second, max configured rate is %{INT:drop_rate_max_avg}; Cumulative total count is %{INT:drop_total_count}"}
[2020-05-26T17:14:00,398][DEBUG][logstash.filters.grok    ][main] Adding pattern {"SHOREWALL"=>"(%{SYSLOGTIMESTAMP:timestamp}) (%{WORD:nf_host}) kernel:.*Shorewall:(%{WORD:nf_action1})?:(%{WORD:nf_action2})?.*IN=(%{USERNAME:nf_in_interface})?.*(OUT= *MAC=(%{COMMONMAC:nf_dst_mac}):(%{COMMONMAC:nf_src_mac})?|OUT=%{USERNAME:nf_out_interface}).*SRC=(%{IPV4:nf_src_ip}).*DST=(%{IPV4:nf_dst_ip}).*LEN=(%{WORD:nf_len}).?*TOS=(%{WORD:nf_tos}).?*PREC=(%{WORD:nf_prec}).?*TTL=(%{INT:nf_ttl}).?*ID=(%{INT:nf_id}).?*PROTO=(%{WORD:nf_protocol}).?*SPT=(%{INT:nf_src_port}?.*DPT=%{INT:nf_dst_port}?.*)"}
[2020-05-26T17:14:00,399][DEBUG][logstash.filters.grok    ][main] Adding pattern {"SFW2"=>"((%{SYSLOGTIMESTAMP})|(%{TIMESTAMP_ISO8601}))\\s*%{HOSTNAME}\\s*kernel\\S+\\s*%{NAGIOSTIME}\\s*SFW2\\-INext\\-%{NOTSPACE:nf_action}\\s*IN=%{USERNAME:nf_in_interface}.*OUT=((\\s*%{USERNAME:nf_out_interface})|(\\s*))MAC=((%{COMMONMAC:nf_dst_mac}:%{COMMONMAC:nf_src_mac})|(\\s*)).*SRC=%{IP:nf_src_ip}\\s*DST=%{IP:nf_dst_ip}.*PROTO=%{WORD:nf_protocol}((.*SPT=%{INT:nf_src_port}.*DPT=%{INT:nf_dst_port}.*)|())"}
[2020-05-26T17:14:00,407][DEBUG][logstash.filters.grok    ][main] Adding pattern {"USERNAME"=>"[a-zA-Z0-9._-]+"}
[2020-05-26T17:14:00,412][DEBUG][logstash.filters.grok    ][main] Adding pattern {"USER"=>"%{USERNAME}"}
[2020-05-26T17:14:00,413][DEBUG][logstash.filters.grok    ][main] Adding pattern {"EMAILLOCALPART"=>"[a-zA-Z][a-zA-Z0-9_.+-=:]+"}
[2020-05-26T17:14:00,414][DEBUG][logstash.filters.grok    ][main] Adding pattern {"EMAILADDRESS"=>"%{EMAILLOCALPART}@%{HOSTNAME}"}
[2020-05-26T17:14:00,415][DEBUG][logstash.filters.grok    ][main] Adding pattern {"INT"=>"(?:[+-]?(?:[0-9]+))"}
[2020-05-26T17:14:00,417][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BASE10NUM"=>"(?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\\.[0-9]+)?)|(?:\\.[0-9]+)))"}
[2020-05-26T17:14:00,418][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NUMBER"=>"(?:%{BASE10NUM})"}
[2020-05-26T17:14:00,419][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BASE16NUM"=>"(?<![0-9A-Fa-f])(?:[+-]?(?:0x)?(?:[0-9A-Fa-f]+))"}
[2020-05-26T17:14:00,420][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BASE16FLOAT"=>"\\b(?<![0-9A-Fa-f.])(?:[+-]?(?:0x)?(?:(?:[0-9A-Fa-f]+(?:\\.[0-9A-Fa-f]*)?)|(?:\\.[0-9A-Fa-f]+)))\\b"}
[2020-05-26T17:14:00,429][DEBUG][logstash.filters.grok    ][main] Adding pattern {"POSINT"=>"\\b(?:[1-9][0-9]*)\\b"}
[2020-05-26T17:14:00,431][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NONNEGINT"=>"\\b(?:[0-9]+)\\b"}
[2020-05-26T17:14:00,431][DEBUG][logstash.filters.grok    ][main] Adding pattern {"WORD"=>"\\b\\w+\\b"}
[2020-05-26T17:14:00,434][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NOTSPACE"=>"\\S+"}
[2020-05-26T17:14:00,435][DEBUG][logstash.filters.grok    ][main] Adding pattern {"SPACE"=>"\\s*"}
[2020-05-26T17:14:00,436][DEBUG][logstash.filters.grok    ][main] Adding pattern {"DATA"=>".*?"}
[2020-05-26T17:14:00,437][DEBUG][logstash.filters.grok    ][main] Adding pattern {"GREEDYDATA"=>".*"}
[2020-05-26T17:14:00,445][DEBUG][logstash.filters.grok    ][main] Adding pattern {"QUOTEDSTRING"=>"(?>(?<!\\\\)(?>\"(?>\\\\.|[^\\\\\"]+)+\"|\"\"|(?>'(?>\\\\.|[^\\\\']+)+')|''|(?>`(?>\\\\.|[^\\\\`]+)+`)|``))"}
[2020-05-26T17:14:00,446][DEBUG][logstash.filters.grok    ][main] Adding pattern {"UUID"=>"[A-Fa-f0-9]{8}-(?:[A-Fa-f0-9]{4}-){3}[A-Fa-f0-9]{12}"}
[2020-05-26T17:14:00,448][DEBUG][logstash.filters.grok    ][main] Adding pattern {"URN"=>"urn:[0-9A-Za-z][0-9A-Za-z-]{0,31}:(?:%[0-9a-fA-F]{2}|[0-9A-Za-z()+,.:=@;$_!*'/?#-])+"}
[2020-05-26T17:14:00,449][DEBUG][logstash.filters.grok    ][main] Adding pattern {"MAC"=>"(?:%{CISCOMAC}|%{WINDOWSMAC}|%{COMMONMAC})"}
[2020-05-26T17:14:00,450][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOMAC"=>"(?:(?:[A-Fa-f0-9]{4}\\.){2}[A-Fa-f0-9]{4})"}
[2020-05-26T17:14:00,451][DEBUG][logstash.filters.grok    ][main] Adding pattern {"WINDOWSMAC"=>"(?:(?:[A-Fa-f0-9]{2}-){5}[A-Fa-f0-9]{2})"}
[2020-05-26T17:14:00,452][DEBUG][logstash.filters.grok    ][main] Adding pattern {"COMMONMAC"=>"(?:(?:[A-Fa-f0-9]{2}:){5}[A-Fa-f0-9]{2})"}
[2020-05-26T17:14:00,454][DEBUG][logstash.filters.grok    ][main] Adding pattern {"IPV6"=>"((([0-9A-Fa-f]{1,4}:){7}([0-9A-Fa-f]{1,4}|:))|(([0-9A-Fa-f]{1,4}:){6}(:[0-9A-Fa-f]{1,4}|((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3})|:))|(([0-9A-Fa-f]{1,4}:){5}(((:[0-9A-Fa-f]{1,4}){1,2})|:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3})|:))|(([0-9A-Fa-f]{1,4}:){4}(((:[0-9A-Fa-f]{1,4}){1,3})|((:[0-9A-Fa-f]{1,4})?:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:))|(([0-9A-Fa-f]{1,4}:){3}(((:[0-9A-Fa-f]{1,4}){1,4})|((:[0-9A-Fa-f]{1,4}){0,2}:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:))|(([0-9A-Fa-f]{1,4}:){2}(((:[0-9A-Fa-f]{1,4}){1,5})|((:[0-9A-Fa-f]{1,4}){0,3}:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:))|(([0-9A-Fa-f]{1,4}:){1}(((:[0-9A-Fa-f]{1,4}){1,6})|((:[0-9A-Fa-f]{1,4}){0,4}:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:))|(:(((:[0-9A-Fa-f]{1,4}){1,7})|((:[0-9A-Fa-f]{1,4}){0,5}:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:)))(%.+)?"}
[2020-05-26T17:14:00,463][DEBUG][logstash.filters.grok    ][main] Adding pattern {"IPV4"=>"(?<![0-9])(?:(?:[0-1]?[0-9]{1,2}|2[0-4][0-9]|25[0-5])[.](?:[0-1]?[0-9]{1,2}|2[0-4][0-9]|25[0-5])[.](?:[0-1]?[0-9]{1,2}|2[0-4][0-9]|25[0-5])[.](?:[0-1]?[0-9]{1,2}|2[0-4][0-9]|25[0-5]))(?![0-9])"}
[2020-05-26T17:14:00,464][DEBUG][logstash.filters.grok    ][main] Adding pattern {"IP"=>"(?:%{IPV6}|%{IPV4})"}
[2020-05-26T17:14:00,465][DEBUG][logstash.filters.grok    ][main] Adding pattern {"HOSTNAME"=>"\\b(?:[0-9A-Za-z][0-9A-Za-z-]{0,62})(?:\\.(?:[0-9A-Za-z][0-9A-Za-z-]{0,62}))*(\\.?|\\b)"}
[2020-05-26T17:14:00,466][DEBUG][logstash.filters.grok    ][main] Adding pattern {"IPORHOST"=>"(?:%{IP}|%{HOSTNAME})"}
[2020-05-26T17:14:00,467][DEBUG][logstash.filters.grok    ][main] Adding pattern {"HOSTPORT"=>"%{IPORHOST}:%{POSINT}"}
[2020-05-26T17:14:00,468][DEBUG][logstash.filters.grok    ][main] Adding pattern {"PATH"=>"(?:%{UNIXPATH}|%{WINPATH})"}
[2020-05-26T17:14:00,469][DEBUG][logstash.filters.grok    ][main] Adding pattern {"UNIXPATH"=>"(/([\\w_%!$@:.,+~-]+|\\\\.)*)+"}
[2020-05-26T17:14:00,470][DEBUG][logstash.filters.grok    ][main] Adding pattern {"TTY"=>"(?:/dev/(pts|tty([pq])?)(\\w+)?/?(?:[0-9]+))"}
[2020-05-26T17:14:00,478][DEBUG][logstash.filters.grok    ][main] Adding pattern {"WINPATH"=>"(?>[A-Za-z]+:|\\\\)(?:\\\\[^\\\\?*]*)+"}
[2020-05-26T17:14:00,480][DEBUG][logstash.filters.grok    ][main] Adding pattern {"URIPROTO"=>"[A-Za-z]([A-Za-z0-9+\\-.]+)+"}
[2020-05-26T17:14:00,481][DEBUG][logstash.filters.grok    ][main] Adding pattern {"URIHOST"=>"%{IPORHOST}(?::%{POSINT:port})?"}
[2020-05-26T17:14:00,482][DEBUG][logstash.filters.grok    ][main] Adding pattern {"URIPATH"=>"(?:/[A-Za-z0-9$.+!*'(){},~:;=@#%&_\\-]*)+"}
[2020-05-26T17:14:00,483][DEBUG][logstash.filters.grok    ][main] Adding pattern {"URIPARAM"=>"\\?[A-Za-z0-9$.+!*'|(){},~@#%&/=:;_?\\-\\[\\]<>]*"}
[2020-05-26T17:14:00,484][DEBUG][logstash.filters.grok    ][main] Adding pattern {"URIPATHPARAM"=>"%{URIPATH}(?:%{URIPARAM})?"}
[2020-05-26T17:14:00,486][DEBUG][logstash.filters.grok    ][main] Adding pattern {"URI"=>"%{URIPROTO}://(?:%{USER}(?::[^@]*)?@)?(?:%{URIHOST})?(?:%{URIPATHPARAM})?"}
[2020-05-26T17:14:00,523][DEBUG][logstash.filters.grok    ][main] Adding pattern {"MONTH"=>"\\b(?:[Jj]an(?:uary|uar)?|[Ff]eb(?:ruary|ruar)?|[Mm](?:a|)?r(?:ch|z)?|[Aa]pr(?:il)?|[Mm]a(?:y|i)?|[Jj]un(?:e|i)?|[Jj]ul(?:y)?|[Aa]ug(?:ust)?|[Ss]ep(?:tember)?|[Oo](?:c|k)?t(?:ober)?|[Nn]ov(?:ember)?|[Dd]e(?:c|z)(?:ember)?)\\b"}
[2020-05-26T17:14:00,525][DEBUG][logstash.filters.grok    ][main] Adding pattern {"MONTHNUM"=>"(?:0?[1-9]|1[0-2])"}
[2020-05-26T17:14:00,534][DEBUG][logstash.filters.grok    ][main] Adding pattern {"MONTHNUM2"=>"(?:0[1-9]|1[0-2])"}
[2020-05-26T17:14:00,535][DEBUG][logstash.filters.grok    ][main] Adding pattern {"MONTHDAY"=>"(?:(?:0[1-9])|(?:[12][0-9])|(?:3[01])|[1-9])"}
[2020-05-26T17:14:00,537][DEBUG][logstash.filters.grok    ][main] Adding pattern {"DAY"=>"(?:Mon(?:day)?|Tue(?:sday)?|Wed(?:nesday)?|Thu(?:rsday)?|Fri(?:day)?|Sat(?:urday)?|Sun(?:day)?)"}
[2020-05-26T17:14:00,539][DEBUG][logstash.filters.grok    ][main] Adding pattern {"YEAR"=>"(?>\\d\\d){1,2}"}
[2020-05-26T17:14:00,546][DEBUG][logstash.filters.grok    ][main] Adding pattern {"HOUR"=>"(?:2[0123]|[01]?[0-9])"}
[2020-05-26T17:14:00,548][DEBUG][logstash.filters.grok    ][main] Adding pattern {"MINUTE"=>"(?:[0-5][0-9])"}
[2020-05-26T17:14:00,549][DEBUG][logstash.filters.grok    ][main] Adding pattern {"SECOND"=>"(?:(?:[0-5]?[0-9]|60)(?:[:.,][0-9]+)?)"}
[2020-05-26T17:14:00,550][DEBUG][logstash.filters.grok    ][main] Adding pattern {"TIME"=>"(?!<[0-9])%{HOUR}:%{MINUTE}(?::%{SECOND})(?![0-9])"}
[2020-05-26T17:14:00,552][DEBUG][logstash.filters.grok    ][main] Adding pattern {"DATE_US"=>"%{MONTHNUM}[/-]%{MONTHDAY}[/-]%{YEAR}"}
[2020-05-26T17:14:00,553][DEBUG][logstash.filters.grok    ][main] Adding pattern {"DATE_EU"=>"%{MONTHDAY}[./-]%{MONTHNUM}[./-]%{YEAR}"}
[2020-05-26T17:14:00,562][DEBUG][logstash.filters.grok    ][main] Adding pattern {"ISO8601_TIMEZONE"=>"(?:Z|[+-]%{HOUR}(?::?%{MINUTE}))"}
[2020-05-26T17:14:00,564][DEBUG][logstash.filters.grok    ][main] Adding pattern {"ISO8601_SECOND"=>"(?:%{SECOND}|60)"}
[2020-05-26T17:14:00,565][DEBUG][logstash.filters.grok    ][main] Adding pattern {"TIMESTAMP_ISO8601"=>"%{YEAR}-%{MONTHNUM}-%{MONTHDAY}[T ]%{HOUR}:?%{MINUTE}(?::?%{SECOND})?%{ISO8601_TIMEZONE}?"}
[2020-05-26T17:14:00,566][DEBUG][logstash.filters.grok    ][main] Adding pattern {"DATE"=>"%{DATE_US}|%{DATE_EU}"}
[2020-05-26T17:14:00,567][DEBUG][logstash.filters.grok    ][main] Adding pattern {"DATESTAMP"=>"%{DATE}[- ]%{TIME}"}
[2020-05-26T17:14:00,569][DEBUG][logstash.filters.grok    ][main] Adding pattern {"TZ"=>"(?:[APMCE][SD]T|UTC)"}
[2020-05-26T17:14:00,570][DEBUG][logstash.filters.grok    ][main] Adding pattern {"DATESTAMP_RFC822"=>"%{DAY} %{MONTH} %{MONTHDAY} %{YEAR} %{TIME} %{TZ}"}
[2020-05-26T17:14:00,580][DEBUG][logstash.filters.grok    ][main] Adding pattern {"DATESTAMP_RFC2822"=>"%{DAY}, %{MONTHDAY} %{MONTH} %{YEAR} %{TIME} %{ISO8601_TIMEZONE}"}
[2020-05-26T17:14:00,582][DEBUG][logstash.filters.grok    ][main] Adding pattern {"DATESTAMP_OTHER"=>"%{DAY} %{MONTH} %{MONTHDAY} %{TIME} %{TZ} %{YEAR}"}
[2020-05-26T17:14:00,583][DEBUG][logstash.filters.grok    ][main] Adding pattern {"DATESTAMP_EVENTLOG"=>"%{YEAR}%{MONTHNUM2}%{MONTHDAY}%{HOUR}%{MINUTE}%{SECOND}"}
[2020-05-26T17:14:00,585][DEBUG][logstash.filters.grok    ][main] Adding pattern {"SYSLOGTIMESTAMP"=>"%{MONTH} +%{MONTHDAY} %{TIME}"}
[2020-05-26T17:14:00,587][DEBUG][logstash.filters.grok    ][main] Adding pattern {"PROG"=>"[\\x21-\\x5a\\x5c\\x5e-\\x7e]+"}
[2020-05-26T17:14:00,588][DEBUG][logstash.filters.grok    ][main] Adding pattern {"SYSLOGPROG"=>"%{PROG:program}(?:\\[%{POSINT:pid}\\])?"}
[2020-05-26T17:14:00,596][DEBUG][logstash.filters.grok    ][main] Adding pattern {"SYSLOGHOST"=>"%{IPORHOST}"}
[2020-05-26T17:14:00,597][DEBUG][logstash.filters.grok    ][main] Adding pattern {"SYSLOGFACILITY"=>"<%{NONNEGINT:facility}.%{NONNEGINT:priority}>"}
[2020-05-26T17:14:00,598][DEBUG][logstash.filters.grok    ][main] Adding pattern {"HTTPDATE"=>"%{MONTHDAY}/%{MONTH}/%{YEAR}:%{TIME} %{INT}"}
[2020-05-26T17:14:00,600][DEBUG][logstash.filters.grok    ][main] Adding pattern {"QS"=>"%{QUOTEDSTRING}"}
[2020-05-26T17:14:00,601][DEBUG][logstash.filters.grok    ][main] Adding pattern {"SYSLOGBASE"=>"%{SYSLOGTIMESTAMP:timestamp} (?:%{SYSLOGFACILITY} )?%{SYSLOGHOST:logsource} %{SYSLOGPROG}:"}
[2020-05-26T17:14:00,602][DEBUG][logstash.filters.grok    ][main] Adding pattern {"LOGLEVEL"=>"([Aa]lert|ALERT|[Tt]race|TRACE|[Dd]ebug|DEBUG|[Nn]otice|NOTICE|[Ii]nfo|INFO|[Ww]arn?(?:ing)?|WARN?(?:ING)?|[Ee]rr?(?:or)?|ERR?(?:OR)?|[Cc]rit?(?:ical)?|CRIT?(?:ICAL)?|[Ff]atal|FATAL|[Ss]evere|SEVERE|EMERG(?:ENCY)?|[Ee]merg(?:ency)?)"}
[2020-05-26T17:14:00,616][DEBUG][logstash.filters.grok    ][main] Adding pattern {"HAPROXYTIME"=>"(?!<[0-9])%{HOUR:haproxy_hour}:%{MINUTE:haproxy_minute}(?::%{SECOND:haproxy_second})(?![0-9])"}
[2020-05-26T17:14:00,618][DEBUG][logstash.filters.grok    ][main] Adding pattern {"HAPROXYDATE"=>"%{MONTHDAY:haproxy_monthday}/%{MONTH:haproxy_month}/%{YEAR:haproxy_year}:%{HAPROXYTIME:haproxy_time}.%{INT:haproxy_milliseconds}"}
[2020-05-26T17:14:00,619][DEBUG][logstash.filters.grok    ][main] Adding pattern {"HAPROXYCAPTUREDREQUESTHEADERS"=>"%{DATA:captured_request_headers}"}
[2020-05-26T17:14:00,621][DEBUG][logstash.filters.grok    ][main] Adding pattern {"HAPROXYCAPTUREDRESPONSEHEADERS"=>"%{DATA:captured_response_headers}"}
[2020-05-26T17:14:00,625][DEBUG][logstash.filters.grok    ][main] Adding pattern {"HAPROXYHTTPBASE"=>"%{IP:client_ip}:%{INT:client_port} \\[%{HAPROXYDATE:accept_date}\\] %{NOTSPACE:frontend_name} %{NOTSPACE:backend_name}/%{NOTSPACE:server_name} %{INT:time_request}/%{INT:time_queue}/%{INT:time_backend_connect}/%{INT:time_backend_response}/%{NOTSPACE:time_duration} %{INT:http_status_code} %{NOTSPACE:bytes_read} %{DATA:captured_request_cookie} %{DATA:captured_response_cookie} %{NOTSPACE:termination_state} %{INT:actconn}/%{INT:feconn}/%{INT:beconn}/%{INT:srvconn}/%{NOTSPACE:retries} %{INT:srv_queue}/%{INT:backend_queue} (\\{%{HAPROXYCAPTUREDREQUESTHEADERS}\\})?( )?(\\{%{HAPROXYCAPTUREDRESPONSEHEADERS}\\})?( )?\"(<BADREQ>|(%{WORD:http_verb} (%{URIPROTO:http_proto}://)?(?:%{USER:http_user}(?::[^@]*)?@)?(?:%{URIHOST:http_host})?(?:%{URIPATHPARAM:http_request})?( HTTP/%{NUMBER:http_version})?))?\""}
[2020-05-26T17:14:00,627][DEBUG][logstash.filters.grok    ][main] Adding pattern {"HAPROXYHTTP"=>"(?:%{SYSLOGTIMESTAMP:syslog_timestamp}|%{TIMESTAMP_ISO8601:timestamp8601}) %{IPORHOST:syslog_server} %{SYSLOGPROG}: %{HAPROXYHTTPBASE}"}
[2020-05-26T17:14:00,628][DEBUG][logstash.filters.grok    ][main] Adding pattern {"HAPROXYTCP"=>"(?:%{SYSLOGTIMESTAMP:syslog_timestamp}|%{TIMESTAMP_ISO8601:timestamp8601}) %{IPORHOST:syslog_server} %{SYSLOGPROG}: %{IP:client_ip}:%{INT:client_port} \\[%{HAPROXYDATE:accept_date}\\] %{NOTSPACE:frontend_name} %{NOTSPACE:backend_name}/%{NOTSPACE:server_name} %{INT:time_queue}/%{INT:time_backend_connect}/%{NOTSPACE:time_duration} %{NOTSPACE:bytes_read} %{NOTSPACE:termination_state} %{INT:actconn}/%{INT:feconn}/%{INT:beconn}/%{INT:srvconn}/%{NOTSPACE:retries} %{INT:srv_queue}/%{INT:backend_queue}"}
[2020-05-26T17:14:00,638][DEBUG][logstash.filters.grok    ][main] Adding pattern {"HTTPDUSER"=>"%{EMAILADDRESS}|%{USER}"}
[2020-05-26T17:14:00,643][DEBUG][logstash.filters.grok    ][main] Adding pattern {"HTTPDERROR_DATE"=>"%{DAY} %{MONTH} %{MONTHDAY} %{TIME} %{YEAR}"}
[2020-05-26T17:14:00,644][DEBUG][logstash.filters.grok    ][main] Adding pattern {"HTTPD_COMMONLOG"=>"%{IPORHOST:clientip} %{HTTPDUSER:ident} %{HTTPDUSER:auth} \\[%{HTTPDATE:timestamp}\\] \"(?:%{WORD:verb} %{NOTSPACE:request}(?: HTTP/%{NUMBER:httpversion})?|%{DATA:rawrequest})\" %{NUMBER:response} (?:%{NUMBER:bytes}|-)"}
[2020-05-26T17:14:00,646][DEBUG][logstash.filters.grok    ][main] Adding pattern {"HTTPD_COMBINEDLOG"=>"%{HTTPD_COMMONLOG} %{QS:referrer} %{QS:agent}"}
[2020-05-26T17:14:00,647][DEBUG][logstash.filters.grok    ][main] Adding pattern {"HTTPD20_ERRORLOG"=>"\\[%{HTTPDERROR_DATE:timestamp}\\] \\[%{LOGLEVEL:loglevel}\\] (?:\\[client %{IPORHOST:clientip}\\] ){0,1}%{GREEDYDATA:message}"}
[2020-05-26T17:14:00,649][DEBUG][logstash.filters.grok    ][main] Adding pattern {"HTTPD24_ERRORLOG"=>"\\[%{HTTPDERROR_DATE:timestamp}\\] \\[%{WORD:module}:%{LOGLEVEL:loglevel}\\] \\[pid %{POSINT:pid}(:tid %{NUMBER:tid})?\\]( \\(%{POSINT:proxy_errorcode}\\)%{DATA:proxy_message}:)?( \\[client %{IPORHOST:clientip}:%{POSINT:clientport}\\])?( %{DATA:errorcode}:)? %{GREEDYDATA:message}"}
[2020-05-26T17:14:00,650][DEBUG][logstash.filters.grok    ][main] Adding pattern {"HTTPD_ERRORLOG"=>"%{HTTPD20_ERRORLOG}|%{HTTPD24_ERRORLOG}"}
[2020-05-26T17:14:00,652][DEBUG][logstash.filters.grok    ][main] Adding pattern {"COMMONAPACHELOG"=>"%{HTTPD_COMMONLOG}"}
[2020-05-26T17:14:00,662][DEBUG][logstash.filters.grok    ][main] Adding pattern {"COMBINEDAPACHELOG"=>"%{HTTPD_COMBINEDLOG}"}
[2020-05-26T17:14:00,669][DEBUG][logstash.filters.grok    ][main] Adding pattern {"JAVACLASS"=>"(?:[a-zA-Z$_][a-zA-Z$_0-9]*\\.)*[a-zA-Z$_][a-zA-Z$_0-9]*"}
[2020-05-26T17:14:00,675][DEBUG][logstash.filters.grok    ][main] Adding pattern {"JAVAFILE"=>"(?:[A-Za-z0-9_. -]+)"}
[2020-05-26T17:14:00,679][DEBUG][logstash.filters.grok    ][main] Adding pattern {"JAVAMETHOD"=>"(?:(<(?:cl)?init>)|[a-zA-Z$_][a-zA-Z$_0-9]*)"}
[2020-05-26T17:14:00,681][DEBUG][logstash.filters.grok    ][main] Adding pattern {"JAVASTACKTRACEPART"=>"%{SPACE}at %{JAVACLASS:class}\\.%{JAVAMETHOD:method}\\(%{JAVAFILE:file}(?::%{NUMBER:line})?\\)"}
[2020-05-26T17:14:00,682][DEBUG][logstash.filters.grok    ][main] Adding pattern {"JAVATHREAD"=>"(?:[A-Z]{2}-Processor[\\d]+)"}
[2020-05-26T17:14:00,683][DEBUG][logstash.filters.grok    ][main] Adding pattern {"JAVACLASS"=>"(?:[a-zA-Z0-9-]+\\.)+[A-Za-z0-9$]+"}
[2020-05-26T17:14:00,689][DEBUG][logstash.filters.grok    ][main] Adding pattern {"JAVAFILE"=>"(?:[A-Za-z0-9_.-]+)"}
[2020-05-26T17:14:00,695][DEBUG][logstash.filters.grok    ][main] Adding pattern {"JAVALOGMESSAGE"=>"(.*)"}
[2020-05-26T17:14:00,696][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CATALINA_DATESTAMP"=>"%{MONTH} %{MONTHDAY}, 20%{YEAR} %{HOUR}:?%{MINUTE}(?::?%{SECOND}) (?:AM|PM)"}
[2020-05-26T17:14:00,698][DEBUG][logstash.filters.grok    ][main] Adding pattern {"TOMCAT_DATESTAMP"=>"20%{YEAR}-%{MONTHNUM}-%{MONTHDAY} %{HOUR}:?%{MINUTE}(?::?%{SECOND}) %{ISO8601_TIMEZONE}"}
[2020-05-26T17:14:00,705][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CATALINALOG"=>"%{CATALINA_DATESTAMP:timestamp} %{JAVACLASS:class} %{JAVALOGMESSAGE:logmessage}"}
[2020-05-26T17:14:00,708][DEBUG][logstash.filters.grok    ][main] Adding pattern {"TOMCATLOG"=>"%{TOMCAT_DATESTAMP:timestamp} \\| %{LOGLEVEL:level} \\| %{JAVACLASS:class} - %{JAVALOGMESSAGE:logmessage}"}
[2020-05-26T17:14:00,727][DEBUG][logstash.filters.grok    ][main] Adding pattern {"RT_FLOW_EVENT"=>"(RT_FLOW_SESSION_CREATE|RT_FLOW_SESSION_CLOSE|RT_FLOW_SESSION_DENY)"}
[2020-05-26T17:14:00,729][DEBUG][logstash.filters.grok    ][main] Adding pattern {"RT_FLOW1"=>"%{RT_FLOW_EVENT:event}: %{GREEDYDATA:close-reason}: %{IP:src-ip}/%{INT:src-port}->%{IP:dst-ip}/%{INT:dst-port} %{DATA:service} %{IP:nat-src-ip}/%{INT:nat-src-port}->%{IP:nat-dst-ip}/%{INT:nat-dst-port} %{DATA:src-nat-rule-name} %{DATA:dst-nat-rule-name} %{INT:protocol-id} %{DATA:policy-name} %{DATA:from-zone} %{DATA:to-zone} %{INT:session-id} \\d+\\(%{DATA:sent}\\) \\d+\\(%{DATA:received}\\) %{INT:elapsed-time} .*"}
[2020-05-26T17:14:00,737][DEBUG][logstash.filters.grok    ][main] Adding pattern {"RT_FLOW2"=>"%{RT_FLOW_EVENT:event}: session created %{IP:src-ip}/%{INT:src-port}->%{IP:dst-ip}/%{INT:dst-port} %{DATA:service} %{IP:nat-src-ip}/%{INT:nat-src-port}->%{IP:nat-dst-ip}/%{INT:nat-dst-port} %{DATA:src-nat-rule-name} %{DATA:dst-nat-rule-name} %{INT:protocol-id} %{DATA:policy-name} %{DATA:from-zone} %{DATA:to-zone} %{INT:session-id} .*"}
[2020-05-26T17:14:00,738][DEBUG][logstash.filters.grok    ][main] Adding pattern {"RT_FLOW3"=>"%{RT_FLOW_EVENT:event}: session denied %{IP:src-ip}/%{INT:src-port}->%{IP:dst-ip}/%{INT:dst-port} %{DATA:service} %{INT:protocol-id}\\(\\d\\) %{DATA:policy-name} %{DATA:from-zone} %{DATA:to-zone} .*"}
[2020-05-26T17:14:00,747][DEBUG][logstash.filters.grok    ][main] Adding pattern {"SYSLOG5424PRINTASCII"=>"[!-~]+"}
[2020-05-26T17:14:00,748][DEBUG][logstash.filters.grok    ][main] Adding pattern {"SYSLOGBASE2"=>"(?:%{SYSLOGTIMESTAMP:timestamp}|%{TIMESTAMP_ISO8601:timestamp8601}) (?:%{SYSLOGFACILITY} )?%{SYSLOGHOST:logsource}+(?: %{SYSLOGPROG}:|)"}
[2020-05-26T17:14:00,751][DEBUG][logstash.filters.grok    ][main] Adding pattern {"SYSLOGPAMSESSION"=>"%{SYSLOGBASE} (?=%{GREEDYDATA:message})%{WORD:pam_module}\\(%{DATA:pam_caller}\\): session %{WORD:pam_session_state} for user %{USERNAME:username}(?: by %{GREEDYDATA:pam_by})?"}
[2020-05-26T17:14:00,752][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CRON_ACTION"=>"[A-Z ]+"}
[2020-05-26T17:14:00,753][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CRONLOG"=>"%{SYSLOGBASE} \\(%{USER:user}\\) %{CRON_ACTION:action} \\(%{DATA:message}\\)"}
[2020-05-26T17:14:00,755][DEBUG][logstash.filters.grok    ][main] Adding pattern {"SYSLOGLINE"=>"%{SYSLOGBASE2} %{GREEDYDATA:message}"}
[2020-05-26T17:14:00,755][DEBUG][logstash.filters.grok    ][main] Adding pattern {"SYSLOG5424PRI"=>"<%{NONNEGINT:syslog5424_pri}>"}
[2020-05-26T17:14:00,763][DEBUG][logstash.filters.grok    ][main] Adding pattern {"SYSLOG5424SD"=>"\\[%{DATA}\\]+"}
[2020-05-26T17:14:00,769][DEBUG][logstash.filters.grok    ][main] Adding pattern {"SYSLOG5424BASE"=>"%{SYSLOG5424PRI}%{NONNEGINT:syslog5424_ver} +(?:%{TIMESTAMP_ISO8601:syslog5424_ts}|-) +(?:%{IPORHOST:syslog5424_host}|-) +(-|%{SYSLOG5424PRINTASCII:syslog5424_app}) +(-|%{SYSLOG5424PRINTASCII:syslog5424_proc}) +(-|%{SYSLOG5424PRINTASCII:syslog5424_msgid}) +(?:%{SYSLOG5424SD:syslog5424_sd}|-|)"}
[2020-05-26T17:14:00,770][DEBUG][logstash.filters.grok    ][main] Adding pattern {"SYSLOG5424LINE"=>"%{SYSLOG5424BASE} +%{GREEDYDATA:syslog5424_msg}"}
[2020-05-26T17:14:00,780][DEBUG][logstash.filters.grok    ][main] Adding pattern {"MAVEN_VERSION"=>"(?:(\\d+)\\.)?(?:(\\d+)\\.)?(\\*|\\d+)(?:[.-](RELEASE|SNAPSHOT))?"}
[2020-05-26T17:14:00,786][DEBUG][logstash.filters.grok    ][main] Adding pattern {"MCOLLECTIVEAUDIT"=>"%{TIMESTAMP_ISO8601:timestamp}:"}
[2020-05-26T17:14:00,794][DEBUG][logstash.filters.grok    ][main] Adding pattern {"MCOLLECTIVE"=>"., \\[%{TIMESTAMP_ISO8601:timestamp} #%{POSINT:pid}\\]%{SPACE}%{LOGLEVEL:event_level}"}
[2020-05-26T17:14:00,795][DEBUG][logstash.filters.grok    ][main] Adding pattern {"MCOLLECTIVEAUDIT"=>"%{TIMESTAMP_ISO8601:timestamp}:"}
[2020-05-26T17:14:00,801][DEBUG][logstash.filters.grok    ][main] Adding pattern {"MONGO_LOG"=>"%{SYSLOGTIMESTAMP:timestamp} \\[%{WORD:component}\\] %{GREEDYDATA:message}"}
[2020-05-26T17:14:00,802][DEBUG][logstash.filters.grok    ][main] Adding pattern {"MONGO_QUERY"=>"\\{ (?<={ ).*(?= } ntoreturn:) \\}"}
[2020-05-26T17:14:00,805][DEBUG][logstash.filters.grok    ][main] Adding pattern {"MONGO_SLOWQUERY"=>"%{WORD} %{MONGO_WORDDASH:database}\\.%{MONGO_WORDDASH:collection} %{WORD}: %{MONGO_QUERY:query} %{WORD}:%{NONNEGINT:ntoreturn} %{WORD}:%{NONNEGINT:ntoskip} %{WORD}:%{NONNEGINT:nscanned}.*nreturned:%{NONNEGINT:nreturned}..+ (?<duration>[0-9]+)ms"}
[2020-05-26T17:14:00,807][DEBUG][logstash.filters.grok    ][main] Adding pattern {"MONGO_WORDDASH"=>"\\b[\\w-]+\\b"}
[2020-05-26T17:14:00,808][DEBUG][logstash.filters.grok    ][main] Adding pattern {"MONGO3_SEVERITY"=>"\\w"}
[2020-05-26T17:14:00,811][DEBUG][logstash.filters.grok    ][main] Adding pattern {"MONGO3_COMPONENT"=>"%{WORD}|-"}
[2020-05-26T17:14:00,820][DEBUG][logstash.filters.grok    ][main] Adding pattern {"MONGO3_LOG"=>"%{TIMESTAMP_ISO8601:timestamp} %{MONGO3_SEVERITY:severity} %{MONGO3_COMPONENT:component}%{SPACE}(?:\\[%{DATA:context}\\])? %{GREEDYDATA:message}"}
[2020-05-26T17:14:00,838][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOSTIME"=>"\\[%{NUMBER:nagios_epoch}\\]"}
[2020-05-26T17:14:00,839][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_TYPE_CURRENT_SERVICE_STATE"=>"CURRENT SERVICE STATE"}
[2020-05-26T17:14:00,844][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_TYPE_CURRENT_HOST_STATE"=>"CURRENT HOST STATE"}
[2020-05-26T17:14:00,846][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_TYPE_SERVICE_NOTIFICATION"=>"SERVICE NOTIFICATION"}
[2020-05-26T17:14:00,847][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_TYPE_HOST_NOTIFICATION"=>"HOST NOTIFICATION"}
[2020-05-26T17:14:00,849][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_TYPE_SERVICE_ALERT"=>"SERVICE ALERT"}
[2020-05-26T17:14:00,850][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_TYPE_HOST_ALERT"=>"HOST ALERT"}
[2020-05-26T17:14:00,851][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_TYPE_SERVICE_FLAPPING_ALERT"=>"SERVICE FLAPPING ALERT"}
[2020-05-26T17:14:00,859][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_TYPE_HOST_FLAPPING_ALERT"=>"HOST FLAPPING ALERT"}
[2020-05-26T17:14:00,860][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_TYPE_SERVICE_DOWNTIME_ALERT"=>"SERVICE DOWNTIME ALERT"}
[2020-05-26T17:14:00,862][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_TYPE_HOST_DOWNTIME_ALERT"=>"HOST DOWNTIME ALERT"}
[2020-05-26T17:14:00,863][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_TYPE_PASSIVE_SERVICE_CHECK"=>"PASSIVE SERVICE CHECK"}
[2020-05-26T17:14:00,864][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_TYPE_PASSIVE_HOST_CHECK"=>"PASSIVE HOST CHECK"}
[2020-05-26T17:14:00,865][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_TYPE_SERVICE_EVENT_HANDLER"=>"SERVICE EVENT HANDLER"}
[2020-05-26T17:14:00,866][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_TYPE_HOST_EVENT_HANDLER"=>"HOST EVENT HANDLER"}
[2020-05-26T17:14:00,868][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_TYPE_EXTERNAL_COMMAND"=>"EXTERNAL COMMAND"}
[2020-05-26T17:14:00,876][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_TYPE_TIMEPERIOD_TRANSITION"=>"TIMEPERIOD TRANSITION"}
[2020-05-26T17:14:00,877][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_EC_DISABLE_SVC_CHECK"=>"DISABLE_SVC_CHECK"}
[2020-05-26T17:14:00,879][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_EC_ENABLE_SVC_CHECK"=>"ENABLE_SVC_CHECK"}
[2020-05-26T17:14:00,880][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_EC_DISABLE_HOST_CHECK"=>"DISABLE_HOST_CHECK"}
[2020-05-26T17:14:00,881][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_EC_ENABLE_HOST_CHECK"=>"ENABLE_HOST_CHECK"}
[2020-05-26T17:14:00,882][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_EC_PROCESS_SERVICE_CHECK_RESULT"=>"PROCESS_SERVICE_CHECK_RESULT"}
[2020-05-26T17:14:00,883][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_EC_PROCESS_HOST_CHECK_RESULT"=>"PROCESS_HOST_CHECK_RESULT"}
[2020-05-26T17:14:00,884][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_EC_SCHEDULE_SERVICE_DOWNTIME"=>"SCHEDULE_SERVICE_DOWNTIME"}
[2020-05-26T17:14:00,893][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_EC_SCHEDULE_HOST_DOWNTIME"=>"SCHEDULE_HOST_DOWNTIME"}
[2020-05-26T17:14:00,893][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_EC_DISABLE_HOST_SVC_NOTIFICATIONS"=>"DISABLE_HOST_SVC_NOTIFICATIONS"}
[2020-05-26T17:14:00,896][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_EC_ENABLE_HOST_SVC_NOTIFICATIONS"=>"ENABLE_HOST_SVC_NOTIFICATIONS"}
[2020-05-26T17:14:00,897][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_EC_DISABLE_HOST_NOTIFICATIONS"=>"DISABLE_HOST_NOTIFICATIONS"}
[2020-05-26T17:14:00,898][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_EC_ENABLE_HOST_NOTIFICATIONS"=>"ENABLE_HOST_NOTIFICATIONS"}
[2020-05-26T17:14:00,900][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_EC_DISABLE_SVC_NOTIFICATIONS"=>"DISABLE_SVC_NOTIFICATIONS"}
[2020-05-26T17:14:00,910][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_EC_ENABLE_SVC_NOTIFICATIONS"=>"ENABLE_SVC_NOTIFICATIONS"}
[2020-05-26T17:14:00,911][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_WARNING"=>"Warning:%{SPACE}%{GREEDYDATA:nagios_message}"}
[2020-05-26T17:14:00,914][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_CURRENT_SERVICE_STATE"=>"%{NAGIOS_TYPE_CURRENT_SERVICE_STATE:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_service};%{DATA:nagios_state};%{DATA:nagios_statetype};%{DATA:nagios_statecode};%{GREEDYDATA:nagios_message}"}
[2020-05-26T17:14:00,915][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_CURRENT_HOST_STATE"=>"%{NAGIOS_TYPE_CURRENT_HOST_STATE:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_state};%{DATA:nagios_statetype};%{DATA:nagios_statecode};%{GREEDYDATA:nagios_message}"}
[2020-05-26T17:14:00,916][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_SERVICE_NOTIFICATION"=>"%{NAGIOS_TYPE_SERVICE_NOTIFICATION:nagios_type}: %{DATA:nagios_notifyname};%{DATA:nagios_hostname};%{DATA:nagios_service};%{DATA:nagios_state};%{DATA:nagios_contact};%{GREEDYDATA:nagios_message}"}
[2020-05-26T17:14:00,918][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_HOST_NOTIFICATION"=>"%{NAGIOS_TYPE_HOST_NOTIFICATION:nagios_type}: %{DATA:nagios_notifyname};%{DATA:nagios_hostname};%{DATA:nagios_state};%{DATA:nagios_contact};%{GREEDYDATA:nagios_message}"}
[2020-05-26T17:14:00,926][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_SERVICE_ALERT"=>"%{NAGIOS_TYPE_SERVICE_ALERT:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_service};%{DATA:nagios_state};%{DATA:nagios_statelevel};%{NUMBER:nagios_attempt};%{GREEDYDATA:nagios_message}"}
[2020-05-26T17:14:00,927][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_HOST_ALERT"=>"%{NAGIOS_TYPE_HOST_ALERT:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_state};%{DATA:nagios_statelevel};%{NUMBER:nagios_attempt};%{GREEDYDATA:nagios_message}"}
[2020-05-26T17:14:00,929][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_SERVICE_FLAPPING_ALERT"=>"%{NAGIOS_TYPE_SERVICE_FLAPPING_ALERT:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_service};%{DATA:nagios_state};%{GREEDYDATA:nagios_message}"}
[2020-05-26T17:14:00,930][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_HOST_FLAPPING_ALERT"=>"%{NAGIOS_TYPE_HOST_FLAPPING_ALERT:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_state};%{GREEDYDATA:nagios_message}"}
[2020-05-26T17:14:00,931][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_SERVICE_DOWNTIME_ALERT"=>"%{NAGIOS_TYPE_SERVICE_DOWNTIME_ALERT:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_service};%{DATA:nagios_state};%{GREEDYDATA:nagios_comment}"}
[2020-05-26T17:14:00,932][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_HOST_DOWNTIME_ALERT"=>"%{NAGIOS_TYPE_HOST_DOWNTIME_ALERT:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_state};%{GREEDYDATA:nagios_comment}"}
[2020-05-26T17:14:00,933][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_PASSIVE_SERVICE_CHECK"=>"%{NAGIOS_TYPE_PASSIVE_SERVICE_CHECK:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_service};%{DATA:nagios_state};%{GREEDYDATA:nagios_comment}"}
[2020-05-26T17:14:00,942][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_PASSIVE_HOST_CHECK"=>"%{NAGIOS_TYPE_PASSIVE_HOST_CHECK:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_state};%{GREEDYDATA:nagios_comment}"}
[2020-05-26T17:14:00,943][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_SERVICE_EVENT_HANDLER"=>"%{NAGIOS_TYPE_SERVICE_EVENT_HANDLER:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_service};%{DATA:nagios_state};%{DATA:nagios_statelevel};%{DATA:nagios_event_handler_name}"}
[2020-05-26T17:14:00,944][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_HOST_EVENT_HANDLER"=>"%{NAGIOS_TYPE_HOST_EVENT_HANDLER:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_state};%{DATA:nagios_statelevel};%{DATA:nagios_event_handler_name}"}
[2020-05-26T17:14:00,945][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_TIMEPERIOD_TRANSITION"=>"%{NAGIOS_TYPE_TIMEPERIOD_TRANSITION:nagios_type}: %{DATA:nagios_service};%{DATA:nagios_unknown1};%{DATA:nagios_unknown2}"}
[2020-05-26T17:14:00,946][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_EC_LINE_DISABLE_SVC_CHECK"=>"%{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_DISABLE_SVC_CHECK:nagios_command};%{DATA:nagios_hostname};%{DATA:nagios_service}"}
[2020-05-26T17:14:00,948][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_EC_LINE_DISABLE_HOST_CHECK"=>"%{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_DISABLE_HOST_CHECK:nagios_command};%{DATA:nagios_hostname}"}
[2020-05-26T17:14:00,949][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_EC_LINE_ENABLE_SVC_CHECK"=>"%{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_ENABLE_SVC_CHECK:nagios_command};%{DATA:nagios_hostname};%{DATA:nagios_service}"}
[2020-05-26T17:14:00,950][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_EC_LINE_ENABLE_HOST_CHECK"=>"%{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_ENABLE_HOST_CHECK:nagios_command};%{DATA:nagios_hostname}"}
[2020-05-26T17:14:00,959][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_EC_LINE_PROCESS_SERVICE_CHECK_RESULT"=>"%{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_PROCESS_SERVICE_CHECK_RESULT:nagios_command};%{DATA:nagios_hostname};%{DATA:nagios_service};%{DATA:nagios_state};%{GREEDYDATA:nagios_check_result}"}
[2020-05-26T17:14:00,960][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_EC_LINE_PROCESS_HOST_CHECK_RESULT"=>"%{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_PROCESS_HOST_CHECK_RESULT:nagios_command};%{DATA:nagios_hostname};%{DATA:nagios_state};%{GREEDYDATA:nagios_check_result}"}
[2020-05-26T17:14:00,961][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_EC_LINE_DISABLE_HOST_SVC_NOTIFICATIONS"=>"%{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_DISABLE_HOST_SVC_NOTIFICATIONS:nagios_command};%{GREEDYDATA:nagios_hostname}"}
[2020-05-26T17:14:00,963][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_EC_LINE_DISABLE_HOST_NOTIFICATIONS"=>"%{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_DISABLE_HOST_NOTIFICATIONS:nagios_command};%{GREEDYDATA:nagios_hostname}"}
[2020-05-26T17:14:00,964][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_EC_LINE_DISABLE_SVC_NOTIFICATIONS"=>"%{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_DISABLE_SVC_NOTIFICATIONS:nagios_command};%{DATA:nagios_hostname};%{GREEDYDATA:nagios_service}"}
[2020-05-26T17:14:00,965][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_EC_LINE_ENABLE_HOST_SVC_NOTIFICATIONS"=>"%{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_ENABLE_HOST_SVC_NOTIFICATIONS:nagios_command};%{GREEDYDATA:nagios_hostname}"}
[2020-05-26T17:14:00,967][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_EC_LINE_ENABLE_HOST_NOTIFICATIONS"=>"%{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_ENABLE_HOST_NOTIFICATIONS:nagios_command};%{GREEDYDATA:nagios_hostname}"}
[2020-05-26T17:14:00,976][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_EC_LINE_ENABLE_SVC_NOTIFICATIONS"=>"%{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_ENABLE_SVC_NOTIFICATIONS:nagios_command};%{DATA:nagios_hostname};%{GREEDYDATA:nagios_service}"}
[2020-05-26T17:14:00,977][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_EC_LINE_SCHEDULE_HOST_DOWNTIME"=>"%{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_SCHEDULE_HOST_DOWNTIME:nagios_command};%{DATA:nagios_hostname};%{NUMBER:nagios_start_time};%{NUMBER:nagios_end_time};%{NUMBER:nagios_fixed};%{NUMBER:nagios_trigger_id};%{NUMBER:nagios_duration};%{DATA:author};%{DATA:comment}"}
[2020-05-26T17:14:00,978][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOSLOGLINE"=>"%{NAGIOSTIME} (?:%{NAGIOS_WARNING}|%{NAGIOS_CURRENT_SERVICE_STATE}|%{NAGIOS_CURRENT_HOST_STATE}|%{NAGIOS_SERVICE_NOTIFICATION}|%{NAGIOS_HOST_NOTIFICATION}|%{NAGIOS_SERVICE_ALERT}|%{NAGIOS_HOST_ALERT}|%{NAGIOS_SERVICE_FLAPPING_ALERT}|%{NAGIOS_HOST_FLAPPING_ALERT}|%{NAGIOS_SERVICE_DOWNTIME_ALERT}|%{NAGIOS_HOST_DOWNTIME_ALERT}|%{NAGIOS_PASSIVE_SERVICE_CHECK}|%{NAGIOS_PASSIVE_HOST_CHECK}|%{NAGIOS_SERVICE_EVENT_HANDLER}|%{NAGIOS_HOST_EVENT_HANDLER}|%{NAGIOS_TIMEPERIOD_TRANSITION}|%{NAGIOS_EC_LINE_DISABLE_SVC_CHECK}|%{NAGIOS_EC_LINE_ENABLE_SVC_CHECK}|%{NAGIOS_EC_LINE_DISABLE_HOST_CHECK}|%{NAGIOS_EC_LINE_ENABLE_HOST_CHECK}|%{NAGIOS_EC_LINE_PROCESS_HOST_CHECK_RESULT}|%{NAGIOS_EC_LINE_PROCESS_SERVICE_CHECK_RESULT}|%{NAGIOS_EC_LINE_SCHEDULE_HOST_DOWNTIME}|%{NAGIOS_EC_LINE_DISABLE_HOST_SVC_NOTIFICATIONS}|%{NAGIOS_EC_LINE_ENABLE_HOST_SVC_NOTIFICATIONS}|%{NAGIOS_EC_LINE_DISABLE_HOST_NOTIFICATIONS}|%{NAGIOS_EC_LINE_ENABLE_HOST_NOTIFICATIONS}|%{NAGIOS_EC_LINE_DISABLE_SVC_NOTIFICATIONS}|%{NAGIOS_EC_LINE_ENABLE_SVC_NOTIFICATIONS})"}
[2020-05-26T17:14:00,986][DEBUG][logstash.filters.grok    ][main] Adding pattern {"POSTGRESQL"=>"%{DATESTAMP:timestamp} %{TZ} %{DATA:user_id} %{GREEDYDATA:connection_id} %{POSINT:pid}"}
[2020-05-26T17:14:00,998][DEBUG][logstash.filters.grok    ][main] Adding pattern {"RUUID"=>"\\h{32}"}
[2020-05-26T17:14:00,999][DEBUG][logstash.filters.grok    ][main] Adding pattern {"RCONTROLLER"=>"(?<controller>[^#]+)#(?<action>\\w+)"}
[2020-05-26T17:14:01,000][DEBUG][logstash.filters.grok    ][main] Adding pattern {"RAILS3HEAD"=>"(?m)Started %{WORD:verb} \"%{URIPATHPARAM:request}\" for %{IPORHOST:clientip} at (?<timestamp>%{YEAR}-%{MONTHNUM}-%{MONTHDAY} %{HOUR}:%{MINUTE}:%{SECOND} %{ISO8601_TIMEZONE})"}
[2020-05-26T17:14:01,001][DEBUG][logstash.filters.grok    ][main] Adding pattern {"RPROCESSING"=>"\\W*Processing by %{RCONTROLLER} as (?<format>\\S+)(?:\\W*Parameters: {%{DATA:params}}\\W*)?"}
[2020-05-26T17:14:01,007][DEBUG][logstash.filters.grok    ][main] Adding pattern {"RAILS3FOOT"=>"Completed %{NUMBER:response}%{DATA} in %{NUMBER:totalms}ms %{RAILS3PROFILE}%{GREEDYDATA}"}
[2020-05-26T17:14:01,012][DEBUG][logstash.filters.grok    ][main] Adding pattern {"RAILS3PROFILE"=>"(?:\\(Views: %{NUMBER:viewms}ms \\| ActiveRecord: %{NUMBER:activerecordms}ms|\\(ActiveRecord: %{NUMBER:activerecordms}ms)?"}
[2020-05-26T17:14:01,014][DEBUG][logstash.filters.grok    ][main] Adding pattern {"RAILS3"=>"%{RAILS3HEAD}(?:%{RPROCESSING})?(?<context>(?:%{DATA}\\n)*)(?:%{RAILS3FOOT})?"}
[2020-05-26T17:14:01,022][DEBUG][logstash.filters.grok    ][main] Adding pattern {"REDISTIMESTAMP"=>"%{MONTHDAY} %{MONTH} %{TIME}"}
[2020-05-26T17:14:01,023][DEBUG][logstash.filters.grok    ][main] Adding pattern {"REDISLOG"=>"\\[%{POSINT:pid}\\] %{REDISTIMESTAMP:timestamp} \\* "}
[2020-05-26T17:14:01,025][DEBUG][logstash.filters.grok    ][main] Adding pattern {"REDISMONLOG"=>"%{NUMBER:timestamp} \\[%{INT:database} %{IP:client}:%{NUMBER:port}\\] \"%{WORD:command}\"\\s?%{GREEDYDATA:params}"}
[2020-05-26T17:14:01,030][DEBUG][logstash.filters.grok    ][main] Adding pattern {"RUBY_LOGLEVEL"=>"(?:DEBUG|FATAL|ERROR|WARN|INFO)"}
[2020-05-26T17:14:01,035][DEBUG][logstash.filters.grok    ][main] Adding pattern {"RUBY_LOGGER"=>"[DFEWI], \\[%{TIMESTAMP_ISO8601:timestamp} #%{POSINT:pid}\\] *%{RUBY_LOGLEVEL:loglevel} -- +%{DATA:progname}: %{GREEDYDATA:message}"}
[2020-05-26T17:14:01,044][DEBUG][logstash.filters.grok    ][main] Adding pattern {"SQUID3"=>"%{NUMBER:timestamp}\\s+%{NUMBER:duration}\\s%{IP:client_address}\\s%{WORD:cache_result}/%{POSINT:status_code}\\s%{NUMBER:bytes}\\s%{WORD:request_method}\\s%{NOTSPACE:url}\\s(%{NOTSPACE:user}|-)\\s%{WORD:hierarchy_code}/%{IPORHOST:server}\\s%{NOTSPACE:content_type}"}
[2020-05-26T17:14:01,063][DEBUG][logstash.filters.grok    ][main] replacement_pattern => (?<TIME:log_time>(?!<[0-9])%{HOUR}:%{MINUTE}(?::%{SECOND})(?![0-9]))
[2020-05-26T17:14:01,065][DEBUG][logstash.filters.grok    ][main] replacement_pattern => (?:(?:2[0123]|[01]?[0-9]))
[2020-05-26T17:14:01,066][DEBUG][logstash.filters.grok    ][main] replacement_pattern => (?:(?:[0-5][0-9]))
[2020-05-26T17:14:01,067][DEBUG][logstash.filters.grok    ][main] replacement_pattern => (?:(?:(?:[0-5]?[0-9]|60)(?:[:.,][0-9]+)?))
[2020-05-26T17:14:01,068][DEBUG][logstash.filters.grok    ][main] replacement_pattern => (?<LOGLEVEL:log_level>([Aa]lert|ALERT|[Tt]race|TRACE|[Dd]ebug|DEBUG|[Nn]otice|NOTICE|[Ii]nfo|INFO|[Ww]arn?(?:ing)?|WARN?(?:ING)?|[Ee]rr?(?:or)?|ERR?(?:OR)?|[Cc]rit?(?:ical)?|CRIT?(?:ICAL)?|[Ff]atal|FATAL|[Ss]evere|SEVERE|EMERG(?:ENCY)?|[Ee]merg(?:ency)?))
[2020-05-26T17:14:01,069][DEBUG][logstash.filters.grok    ][main] replacement_pattern => (?<DATA:class_name>.*?)
[2020-05-26T17:14:01,075][DEBUG][logstash.filters.grok    ][main] replacement_pattern => (?:.*)
[2020-05-26T17:14:01,080][DEBUG][logstash.filters.grok    ][main] Grok compiled OK {:pattern=>"%{TIME:log_time} %{LOGLEVEL:log_level} %{DATA:class_name} %{GREEDYDATA :threadName}", :expanded_pattern=>"(?<TIME:log_time>(?!<[0-9])(?:(?:2[0123]|[01]?[0-9])):(?:(?:[0-5][0-9]))(?::(?:(?:(?:[0-5]?[0-9]|60)(?:[:.,][0-9]+)?)))(?![0-9])) (?<LOGLEVEL:log_level>([Aa]lert|ALERT|[Tt]race|TRACE|[Dd]ebug|DEBUG|[Nn]otice|NOTICE|[Ii]nfo|INFO|[Ww]arn?(?:ing)?|WARN?(?:ING)?|[Ee]rr?(?:or)?|ERR?(?:OR)?|[Cc]rit?(?:ical)?|CRIT?(?:ICAL)?|[Ff]atal|FATAL|[Ss]evere|SEVERE|EMERG(?:ENCY)?|[Ee]merg(?:ency)?)) (?<DATA:class_name>.*?) (?:.*)"}
[2020-05-26T17:14:01,205][WARN ][org.logstash.instrument.metrics.gauge.LazyDelegatingGauge][main] A gauge metric of an unknown type (org.jruby.RubyArray) has been created for key: cluster_uuids. This may result in invalid serialization.  It is recommended to log an issue to the responsible developer/development team.
[2020-05-26T17:14:01,213][INFO ][logstash.javapipeline    ][main] Starting pipeline {:pipeline_id=>"main", "pipeline.workers"=>8, "pipeline.batch.size"=>125, "pipeline.batch.delay"=>50, "pipeline.max_inflight"=>1000, "pipeline.sources"=>["D:/Softwares/logstash-7.7.0/config/logstash-sample.conf"], :thread=>"#<Thread:0x3347df6a run>"}
[2020-05-26T17:14:01,335][DEBUG][logstash.instrument.periodicpoller.cgroup] One or more required cgroup files or directories not found: /proc/self/cgroup, /sys/fs/cgroup/cpuacct, /sys/fs/cgroup/cpu
[2020-05-26T17:14:01,497][DEBUG][logstash.instrument.periodicpoller.jvm] collector name {:name=>"ParNew"}
[2020-05-26T17:14:01,505][DEBUG][logstash.instrument.periodicpoller.jvm] collector name {:name=>"ConcurrentMarkSweep"}
[2020-05-26T17:14:01,936][DEBUG][logstash.outputs.file    ][main] Starting flush cycle
[2020-05-26T17:14:02,507][DEBUG][org.logstash.config.ir.CompiledPipeline][main] Compiled conditional
 [if (event.getField('[type]')=='applog')] 
 into 
 org.logstash.config.ir.compiler.ComputeStepSyntaxElement@48bce41f
[2020-05-26T17:14:02,507][DEBUG][org.logstash.config.ir.CompiledPipeline][main] Compiled conditional
 [if (event.getField('[type]')=='applog')] 
 into 
 org.logstash.config.ir.compiler.ComputeStepSyntaxElement@48bce41f
[2020-05-26T17:14:02,507][DEBUG][org.logstash.config.ir.CompiledPipeline][main] Compiled conditional
 [if (event.getField('[type]')=='applog')] 
 into 
 org.logstash.config.ir.compiler.ComputeStepSyntaxElement@48bce41f
[2020-05-26T17:14:02,507][DEBUG][org.logstash.config.ir.CompiledPipeline][main] Compiled conditional
 [if (event.getField('[type]')=='applog')] 
 into 
 org.logstash.config.ir.compiler.ComputeStepSyntaxElement@48bce41f
[2020-05-26T17:14:02,508][DEBUG][org.logstash.config.ir.CompiledPipeline][main] Compiled conditional
 [if (event.getField('[type]')=='applog')] 
 into 
 org.logstash.config.ir.compiler.ComputeStepSyntaxElement@48bce41f
[2020-05-26T17:14:02,507][DEBUG][org.logstash.config.ir.CompiledPipeline][main] Compiled conditional
 [if (event.getField('[type]')=='applog')] 
 into 
 org.logstash.config.ir.compiler.ComputeStepSyntaxElement@48bce41f
[2020-05-26T17:14:02,507][DEBUG][org.logstash.config.ir.CompiledPipeline][main] Compiled conditional
 [if (event.getField('[type]')=='applog')] 
 into 
 org.logstash.config.ir.compiler.ComputeStepSyntaxElement@48bce41f
[2020-05-26T17:14:02,507][DEBUG][org.logstash.config.ir.CompiledPipeline][main] Compiled conditional
 [if (event.getField('[type]')=='applog')] 
 into 
 org.logstash.config.ir.compiler.ComputeStepSyntaxElement@48bce41f
[2020-05-26T17:14:02,717][DEBUG][org.logstash.config.ir.CompiledPipeline][main] Compiled filter
 P[filter-grok{"match"=>{"message"=>"%{TIME:log_time} %{LOGLEVEL:log_level} %{DATA:class_name} %{GREEDYDATA :threadName}"}}|[file]D:/Softwares/logstash-7.7.0/config/logstash-sample.conf:11:3:```
grok {
        	match => { "message" => "%{TIME:log_time} %{LOGLEVEL:log_level} %{DATA:class_name} %{GREEDYDATA :threadName}"}
    	}
```] 
 into 
 org.logstash.config.ir.compiler.ComputeStepSyntaxElement@75d53c5c
[2020-05-26T17:14:02,724][DEBUG][org.logstash.config.ir.CompiledPipeline][main] Compiled filter
 P[filter-grok{"match"=>{"message"=>"%{TIME:log_time} %{LOGLEVEL:log_level} %{DATA:class_name} %{GREEDYDATA :threadName}"}}|[file]D:/Softwares/logstash-7.7.0/config/logstash-sample.conf:11:3:```
grok {
        	match => { "message" => "%{TIME:log_time} %{LOGLEVEL:log_level} %{DATA:class_name} %{GREEDYDATA :threadName}"}
    	}
```] 
 into 
 org.logstash.config.ir.compiler.ComputeStepSyntaxElement@75d53c5c
[2020-05-26T17:14:02,723][DEBUG][org.logstash.config.ir.CompiledPipeline][main] Compiled filter
 P[filter-grok{"match"=>{"message"=>"%{TIME:log_time} %{LOGLEVEL:log_level} %{DATA:class_name} %{GREEDYDATA :threadName}"}}|[file]D:/Softwares/logstash-7.7.0/config/logstash-sample.conf:11:3:```
grok {
        	match => { "message" => "%{TIME:log_time} %{LOGLEVEL:log_level} %{DATA:class_name} %{GREEDYDATA :threadName}"}
    	}
```] 
 into 
 org.logstash.config.ir.compiler.ComputeStepSyntaxElement@75d53c5c
[2020-05-26T17:14:02,717][DEBUG][org.logstash.config.ir.CompiledPipeline][main] Compiled filter
 P[filter-grok{"match"=>{"message"=>"%{TIME:log_time} %{LOGLEVEL:log_level} %{DATA:class_name} %{GREEDYDATA :threadName}"}}|[file]D:/Softwares/logstash-7.7.0/config/logstash-sample.conf:11:3:```
grok {
        	match => { "message" => "%{TIME:log_time} %{LOGLEVEL:log_level} %{DATA:class_name} %{GREEDYDATA :threadName}"}
    	}
```] 
 into 
 org.logstash.config.ir.compiler.ComputeStepSyntaxElement@75d53c5c
[2020-05-26T17:14:02,717][DEBUG][org.logstash.config.ir.CompiledPipeline][main] Compiled filter
 P[filter-grok{"match"=>{"message"=>"%{TIME:log_time} %{LOGLEVEL:log_level} %{DATA:class_name} %{GREEDYDATA :threadName}"}}|[file]D:/Softwares/logstash-7.7.0/config/logstash-sample.conf:11:3:```
grok {
        	match => { "message" => "%{TIME:log_time} %{LOGLEVEL:log_level} %{DATA:class_name} %{GREEDYDATA :threadName}"}
    	}
```] 
 into 
 org.logstash.config.ir.compiler.ComputeStepSyntaxElement@75d53c5c
[2020-05-26T17:14:02,717][DEBUG][org.logstash.config.ir.CompiledPipeline][main] Compiled filter
 P[filter-grok{"match"=>{"message"=>"%{TIME:log_time} %{LOGLEVEL:log_level} %{DATA:class_name} %{GREEDYDATA :threadName}"}}|[file]D:/Softwares/logstash-7.7.0/config/logstash-sample.conf:11:3:```
grok {
        	match => { "message" => "%{TIME:log_time} %{LOGLEVEL:log_level} %{DATA:class_name} %{GREEDYDATA :threadName}"}
    	}
```] 
 into 
 org.logstash.config.ir.compiler.ComputeStepSyntaxElement@75d53c5c
[2020-05-26T17:14:02,720][DEBUG][org.logstash.config.ir.CompiledPipeline][main] Compiled filter
 P[filter-grok{"match"=>{"message"=>"%{TIME:log_time} %{LOGLEVEL:log_level} %{DATA:class_name} %{GREEDYDATA :threadName}"}}|[file]D:/Softwares/logstash-7.7.0/config/logstash-sample.conf:11:3:```
grok {
        	match => { "message" => "%{TIME:log_time} %{LOGLEVEL:log_level} %{DATA:class_name} %{GREEDYDATA :threadName}"}
    	}
```] 
 into 
 org.logstash.config.ir.compiler.ComputeStepSyntaxElement@75d53c5c
[2020-05-26T17:14:02,725][DEBUG][org.logstash.config.ir.CompiledPipeline][main] Compiled filter
 P[filter-grok{"match"=>{"message"=>"%{TIME:log_time} %{LOGLEVEL:log_level} %{DATA:class_name} %{GREEDYDATA :threadName}"}}|[file]D:/Softwares/logstash-7.7.0/config/logstash-sample.conf:11:3:```
grok {
        	match => { "message" => "%{TIME:log_time} %{LOGLEVEL:log_level} %{DATA:class_name} %{GREEDYDATA :threadName}"}
    	}
```] 
 into 
 org.logstash.config.ir.compiler.ComputeStepSyntaxElement@75d53c5c
[2020-05-26T17:14:02,785][DEBUG][org.logstash.config.ir.CompiledPipeline][main] Compiled output
 P[output-file{"path"=>"C:/Users/karan/Desktop/OUTPUT_LOGS/output.txt"}|[file]D:/Softwares/logstash-7.7.0/config/logstash-sample.conf:21:5:```
file {
        path => "C:/Users/karan/Desktop/OUTPUT_LOGS/output.txt"
    }
```] 
 into 
 org.logstash.config.ir.compiler.ComputeStepSyntaxElement@280a0790
[2020-05-26T17:14:02,794][DEBUG][org.logstash.config.ir.CompiledPipeline][main] Compiled output
 P[output-file{"path"=>"C:/Users/karan/Desktop/OUTPUT_LOGS/output.txt"}|[file]D:/Softwares/logstash-7.7.0/config/logstash-sample.conf:21:5:```
file {
        path => "C:/Users/karan/Desktop/OUTPUT_LOGS/output.txt"
    }
```] 
 into 
 org.logstash.config.ir.compiler.ComputeStepSyntaxElement@280a0790
[2020-05-26T17:14:02,794][DEBUG][org.logstash.config.ir.CompiledPipeline][main] Compiled output
 P[output-file{"path"=>"C:/Users/karan/Desktop/OUTPUT_LOGS/output.txt"}|[file]D:/Softwares/logstash-7.7.0/config/logstash-sample.conf:21:5:```
file {
        path => "C:/Users/karan/Desktop/OUTPUT_LOGS/output.txt"
    }
```] 
 into 
 org.logstash.config.ir.compiler.ComputeStepSyntaxElement@280a0790
[2020-05-26T17:14:02,792][DEBUG][org.logstash.config.ir.CompiledPipeline][main] Compiled output
 P[output-file{"path"=>"C:/Users/karan/Desktop/OUTPUT_LOGS/output.txt"}|[file]D:/Softwares/logstash-7.7.0/config/logstash-sample.conf:21:5:```
file {
        path => "C:/Users/karan/Desktop/OUTPUT_LOGS/output.txt"
    }
```] 
 into 
 org.logstash.config.ir.compiler.ComputeStepSyntaxElement@280a0790
[2020-05-26T17:14:02,791][DEBUG][org.logstash.config.ir.CompiledPipeline][main] Compiled output
 P[output-file{"path"=>"C:/Users/karan/Desktop/OUTPUT_LOGS/output.txt"}|[file]D:/Softwares/logstash-7.7.0/config/logstash-sample.conf:21:5:```
file {
        path => "C:/Users/karan/Desktop/OUTPUT_LOGS/output.txt"
    }
```] 
 into 
 org.logstash.config.ir.compiler.ComputeStepSyntaxElement@280a0790
[2020-05-26T17:14:02,803][DEBUG][org.logstash.config.ir.CompiledPipeline][main] Compiled output
 P[output-file{"path"=>"C:/Users/karan/Desktop/OUTPUT_LOGS/output.txt"}|[file]D:/Softwares/logstash-7.7.0/config/logstash-sample.conf:21:5:```
file {
        path => "C:/Users/karan/Desktop/OUTPUT_LOGS/output.txt"
    }
```] 
 into 
 org.logstash.config.ir.compiler.ComputeStepSyntaxElement@280a0790
[2020-05-26T17:14:02,800][DEBUG][org.logstash.config.ir.CompiledPipeline][main] Compiled output
 P[output-file{"path"=>"C:/Users/karan/Desktop/OUTPUT_LOGS/output.txt"}|[file]D:/Softwares/logstash-7.7.0/config/logstash-sample.conf:21:5:```
file {
        path => "C:/Users/karan/Desktop/OUTPUT_LOGS/output.txt"
    }
```] 
 into 
 org.logstash.config.ir.compiler.ComputeStepSyntaxElement@280a0790
[2020-05-26T17:14:02,799][DEBUG][org.logstash.config.ir.CompiledPipeline][main] Compiled output
 P[output-file{"path"=>"C:/Users/karan/Desktop/OUTPUT_LOGS/output.txt"}|[file]D:/Softwares/logstash-7.7.0/config/logstash-sample.conf:21:5:```
file {
        path => "C:/Users/karan/Desktop/OUTPUT_LOGS/output.txt"
    }
```] 
 into 
 org.logstash.config.ir.compiler.ComputeStepSyntaxElement@280a0790
[2020-05-26T17:14:03,497][INFO ][logstash.inputs.file     ][main] No sincedb_path set, generating one based on the "path" setting {:sincedb_path=>"D:/Softwares/logstash-7.7.0/data/plugins/inputs/file/.sincedb_ee1c3288ae39e6bd7cb89b6934459318", :path=>["C:/Users/karan/Desktop/*.log"]}
[2020-05-26T17:14:03,520][INFO ][logstash.javapipeline    ][main] Pipeline started {"pipeline.id"=>"main"}
[2020-05-26T17:14:03,531][DEBUG][logstash.javapipeline    ] Pipeline started successfully {:pipeline_id=>"main", :thread=>"#<Thread:0x3347df6a run>"}
[2020-05-26T17:14:03,532][DEBUG][org.logstash.execution.PeriodicFlush][main] Pushing flush onto pipeline.
[2020-05-26T17:14:03,607][INFO ][filewatch.observingtail  ][main][57578b95369b2238c183943d813809ecc891db9e70c0a43cafaf3000c9c37372] START, creating Discoverer, Watch with file and sincedb collections
[2020-05-26T17:14:03,642][INFO ][logstash.agent           ] Pipelines running {:count=>1, :running_pipelines=>[:main], :non_running_pipelines=>[]}
[2020-05-26T17:14:03,670][DEBUG][logstash.agent           ] Starting puma
[2020-05-26T17:14:03,688][DEBUG][logstash.agent           ] Trying to start WebServer {:port=>9600}
[2020-05-26T17:14:03,780][DEBUG][logstash.api.service     ] [api-service] start
[2020-05-26T17:14:03,949][DEBUG][logstash.outputs.file    ][main] Starting flush cycle
[2020-05-26T17:14:04,137][INFO ][logstash.agent           ] Successfully started Logstash API endpoint {:port=>9600}
[2020-05-26T17:14:05,951][DEBUG][logstash.outputs.file    ][main] Starting flush cycle
[2020-05-26T17:14:06,357][DEBUG][logstash.instrument.periodicpoller.cgroup] One or more required cgroup files or directories not found: /proc/self/cgroup, /sys/fs/cgroup/cpuacct, /sys/fs/cgroup/cpu
[2020-05-26T17:14:06,516][DEBUG][logstash.instrument.periodicpoller.jvm] collector name {:name=>"ParNew"}
[2020-05-26T17:14:06,517][DEBUG][logstash.instrument.periodicpoller.jvm] collector name {:name=>"ConcurrentMarkSweep"}
[2020-05-26T17:14:07,952][DEBUG][logstash.outputs.file    ][main] Starting flush cycle
[2020-05-26T17:14:08,533][DEBUG][org.logstash.execution.PeriodicFlush][main] Pushing flush onto pipeline.
[2020-05-26T17:14:09,925][DEBUG][logstash.outputs.file    ][main][7f4f544900ab0f14064ae8514d3672ab18add11ed23afaba42a559c140fc5741] Starting stale files cleanup cycle {:files=>{}}
[2020-05-26T17:14:09,937][DEBUG][logstash.outputs.file    ][main][7f4f544900ab0f14064ae8514d3672ab18add11ed23afaba42a559c140fc5741] 0 stale files found {:inactive_files=>{}}
[2020-05-26T17:14:09,954][DEBUG][logstash.outputs.file    ][main] Starting flush cycle
[2020-05-26T17:14:10,329][WARN ][logstash.runner          ] SIGINT received. Shutting down.
[2020-05-26T17:14:10,348][DEBUG][logstash.instrument.periodicpoller.os] Stopping
[2020-05-26T17:14:10,359][DEBUG][logstash.instrument.periodicpoller.jvm] Stopping
[2020-05-26T17:14:10,361][DEBUG][logstash.instrument.periodicpoller.persistentqueue] Stopping
[2020-05-26T17:14:10,362][DEBUG][logstash.instrument.periodicpoller.deadletterqueue] Stopping
[2020-05-26T17:14:10,368][DEBUG][logstash.agent           ] Shutting down all pipelines {:pipelines_count=>1}
[2020-05-26T17:14:10,373][DEBUG][logstash.agent           ] Converging pipelines state {:actions_count=>1}
[2020-05-26T17:14:10,380][DEBUG][logstash.agent           ] Executing action {:action=>LogStash::PipelineAction::Stop/pipeline_id:main}
[2020-05-26T17:14:10,397][DEBUG][logstash.javapipeline    ] Closing inputs {:pipeline_id=>"main", :thread=>"#<Thread:0x3347df6a sleep>"}
[2020-05-26T17:14:10,400][DEBUG][logstash.inputs.file     ] Stopping {:plugin=>"LogStash::Inputs::File"}
[2020-05-26T17:14:10,414][INFO ][filewatch.observingtail  ] QUIT - closing all files and shutting down.
[2020-05-26T17:14:10,417][DEBUG][logstash.javapipeline    ] Closed inputs {:pipeline_id=>"main", :thread=>"#<Thread:0x3347df6a sleep>"}
[2020-05-26T17:14:10,424][DEBUG][logstash.javapipeline    ] Closing inputs {:pipeline_id=>"main", :thread=>"#<Thread:0x3347df6a sleep>"}
[2020-05-26T17:14:11,026][DEBUG][logstash.javapipeline    ][main] Input plugins stopped! Will shutdown filter/output workers. {:pipeline_id=>"main", :thread=>"#<Thread:0x3347df6a run>"}
[2020-05-26T17:14:11,037][DEBUG][logstash.javapipeline    ][main] Shutdown waiting for worker thread {:pipeline_id=>"main", :thread=>"#<Thread:0x7a9f0f11 run>"}
[2020-05-26T17:14:11,096][DEBUG][logstash.javapipeline    ][main] Shutdown waiting for worker thread {:pipeline_id=>"main", :thread=>"#<Thread:0x34423b94 dead>"}
[2020-05-26T17:14:11,098][DEBUG][logstash.javapipeline    ][main] Shutdown waiting for worker thread {:pipeline_id=>"main", :thread=>"#<Thread:0x331049b2 run>"}
[2020-05-26T17:14:11,115][DEBUG][logstash.javapipeline    ][main] Shutdown waiting for worker thread {:pipeline_id=>"main", :thread=>"#<Thread:0x5a688180 run>"}
[2020-05-26T17:14:11,119][DEBUG][logstash.javapipeline    ][main] Shutdown waiting for worker thread {:pipeline_id=>"main", :thread=>"#<Thread:0x7391648c dead>"}
[2020-05-26T17:14:11,121][DEBUG][logstash.javapipeline    ] Worker closed {:pipeline_id=>"main", :thread=>"#<Thread:0x3347df6a run>"}
[2020-05-26T17:14:11,122][DEBUG][logstash.javapipeline    ][main] Shutdown waiting for worker thread {:pipeline_id=>"main", :thread=>"#<Thread:0xe8036b6 dead>"}
[2020-05-26T17:14:11,124][DEBUG][logstash.javapipeline    ][main] Shutdown waiting for worker thread {:pipeline_id=>"main", :thread=>"#<Thread:0x5b58821c dead>"}
[2020-05-26T17:14:11,138][DEBUG][logstash.javapipeline    ][main] Shutdown waiting for worker thread {:pipeline_id=>"main", :thread=>"#<Thread:0x12d3cfc3 dead>"}
[2020-05-26T17:14:11,147][INFO ][logstash.javapipeline    ] Pipeline terminated {"pipeline.id"=>"main"}
[2020-05-26T17:14:11,155][DEBUG][logstash.filters.grok    ][main] Closing {:plugin=>"LogStash::Filters::Grok"}
[2020-05-26T17:14:11,190][DEBUG][logstash.pluginmetadata  ][main] Removing metadata for plugin e68692e25e5d100f0ff915615303269508751e97a86606ceeb4a10d6258f0594
[2020-05-26T17:14:11,194][DEBUG][logstash.outputs.file    ][main] Closing {:plugin=>"LogStash::Outputs::File"}
[2020-05-26T17:14:11,961][DEBUG][logstash.outputs.file    ][main] Close: closing files
[2020-05-26T17:14:11,966][DEBUG][logstash.pluginmetadata  ][main] Removing metadata for plugin 7f4f544900ab0f14064ae8514d3672ab18add11ed23afaba42a559c140fc5741
[2020-05-26T17:14:11,976][DEBUG][logstash.javapipeline    ][main] Pipeline has been shutdown {:pipeline_id=>"main", :thread=>"#<Thread:0x3347df6a run>"}
[2020-05-26T17:14:12,029][INFO ][logstash.runner          ] Logstash shut down.
[2020-05-26T17:15:13,204][DEBUG][logstash.modules.scaffold] Found module {:module_name=>"fb_apache", :directory=>"D:/Softwares/logstash-7.7.0/modules/fb_apache/configuration"}
[2020-05-26T17:15:13,302][DEBUG][logstash.plugins.registry] Adding plugin to the registry {:name=>"fb_apache", :type=>:modules, :class=>#<LogStash::Modules::Scaffold:0x630f24a5 @directory="D:/Softwares/logstash-7.7.0/modules/fb_apache/configuration", @module_name="fb_apache", @kibana_version_parts=["6", "0", "0"]>}
[2020-05-26T17:15:13,307][DEBUG][logstash.modules.scaffold] Found module {:module_name=>"netflow", :directory=>"D:/Softwares/logstash-7.7.0/modules/netflow/configuration"}
[2020-05-26T17:15:13,309][DEBUG][logstash.plugins.registry] Adding plugin to the registry {:name=>"netflow", :type=>:modules, :class=>#<LogStash::Modules::Scaffold:0x34ceabf1 @directory="D:/Softwares/logstash-7.7.0/modules/netflow/configuration", @module_name="netflow", @kibana_version_parts=["6", "0", "0"]>}
[2020-05-26T17:15:13,397][DEBUG][logstash.runner          ] -------- Logstash Settings (* means modified) ---------
[2020-05-26T17:15:13,400][DEBUG][logstash.runner          ] node.name: "DESKTOP-IU1N73S"
[2020-05-26T17:15:13,401][DEBUG][logstash.runner          ] *path.config: "config\\logstash-sample.conf"
[2020-05-26T17:15:13,403][DEBUG][logstash.runner          ] path.data: "D:/Softwares/logstash-7.7.0/data"
[2020-05-26T17:15:13,405][DEBUG][logstash.runner          ] modules.cli: []
[2020-05-26T17:15:13,407][DEBUG][logstash.runner          ] modules: []
[2020-05-26T17:15:13,408][DEBUG][logstash.runner          ] modules_list: []
[2020-05-26T17:15:13,410][DEBUG][logstash.runner          ] modules_variable_list: []
[2020-05-26T17:15:13,413][DEBUG][logstash.runner          ] modules_setup: false
[2020-05-26T17:15:13,415][DEBUG][logstash.runner          ] config.test_and_exit: false
[2020-05-26T17:15:13,417][DEBUG][logstash.runner          ] config.reload.automatic: false
[2020-05-26T17:15:13,418][DEBUG][logstash.runner          ] config.reload.interval: 3000000000
[2020-05-26T17:15:13,420][DEBUG][logstash.runner          ] config.support_escapes: false
[2020-05-26T17:15:13,421][DEBUG][logstash.runner          ] config.field_reference.parser: "STRICT"
[2020-05-26T17:15:13,423][DEBUG][logstash.runner          ] metric.collect: true
[2020-05-26T17:15:13,424][DEBUG][logstash.runner          ] pipeline.id: "main"
[2020-05-26T17:15:13,426][DEBUG][logstash.runner          ] pipeline.system: false
[2020-05-26T17:15:13,427][DEBUG][logstash.runner          ] pipeline.workers: 8
[2020-05-26T17:15:13,428][DEBUG][logstash.runner          ] pipeline.batch.size: 125
[2020-05-26T17:15:13,429][DEBUG][logstash.runner          ] pipeline.batch.delay: 50
[2020-05-26T17:15:13,430][DEBUG][logstash.runner          ] pipeline.unsafe_shutdown: false
[2020-05-26T17:15:13,431][DEBUG][logstash.runner          ] pipeline.java_execution: true
[2020-05-26T17:15:13,433][DEBUG][logstash.runner          ] pipeline.reloadable: true
[2020-05-26T17:15:13,434][DEBUG][logstash.runner          ] pipeline.plugin_classloaders: false
[2020-05-26T17:15:13,436][DEBUG][logstash.runner          ] pipeline.separate_logs: false
[2020-05-26T17:15:13,437][DEBUG][logstash.runner          ] pipeline.ordered: "auto"
[2020-05-26T17:15:13,438][DEBUG][logstash.runner          ] path.plugins: []
[2020-05-26T17:15:13,440][DEBUG][logstash.runner          ] config.debug: false
[2020-05-26T17:15:13,441][DEBUG][logstash.runner          ] *log.level: "debug" (default: "info")
[2020-05-26T17:15:13,443][DEBUG][logstash.runner          ] version: false
[2020-05-26T17:15:13,444][DEBUG][logstash.runner          ] help: false
[2020-05-26T17:15:13,445][DEBUG][logstash.runner          ] log.format: "plain"
[2020-05-26T17:15:13,446][DEBUG][logstash.runner          ] http.host: "127.0.0.1"
[2020-05-26T17:15:13,447][DEBUG][logstash.runner          ] http.port: 9600..9700
[2020-05-26T17:15:13,448][DEBUG][logstash.runner          ] http.environment: "production"
[2020-05-26T17:15:13,449][DEBUG][logstash.runner          ] queue.type: "memory"
[2020-05-26T17:15:13,451][DEBUG][logstash.runner          ] queue.drain: false
[2020-05-26T17:15:13,452][DEBUG][logstash.runner          ] queue.page_capacity: 67108864
[2020-05-26T17:15:13,453][DEBUG][logstash.runner          ] queue.max_bytes: 1073741824
[2020-05-26T17:15:13,454][DEBUG][logstash.runner          ] queue.max_events: 0
[2020-05-26T17:15:13,455][DEBUG][logstash.runner          ] queue.checkpoint.acks: 1024
[2020-05-26T17:15:13,457][DEBUG][logstash.runner          ] queue.checkpoint.writes: 1024
[2020-05-26T17:15:13,458][DEBUG][logstash.runner          ] queue.checkpoint.interval: 1000
[2020-05-26T17:15:13,459][DEBUG][logstash.runner          ] queue.checkpoint.retry: false
[2020-05-26T17:15:13,460][DEBUG][logstash.runner          ] dead_letter_queue.enable: false
[2020-05-26T17:15:13,461][DEBUG][logstash.runner          ] dead_letter_queue.max_bytes: 1073741824
[2020-05-26T17:15:13,462][DEBUG][logstash.runner          ] slowlog.threshold.warn: -1
[2020-05-26T17:15:13,464][DEBUG][logstash.runner          ] slowlog.threshold.info: -1
[2020-05-26T17:15:13,465][DEBUG][logstash.runner          ] slowlog.threshold.debug: -1
[2020-05-26T17:15:13,467][DEBUG][logstash.runner          ] slowlog.threshold.trace: -1
[2020-05-26T17:15:13,469][DEBUG][logstash.runner          ] keystore.classname: "org.logstash.secret.store.backend.JavaKeyStore"
[2020-05-26T17:15:13,470][DEBUG][logstash.runner          ] keystore.file: "D:/Softwares/logstash-7.7.0/config/logstash.keystore"
[2020-05-26T17:15:13,472][DEBUG][logstash.runner          ] path.queue: "D:/Softwares/logstash-7.7.0/data/queue"
[2020-05-26T17:15:13,473][DEBUG][logstash.runner          ] path.dead_letter_queue: "D:/Softwares/logstash-7.7.0/data/dead_letter_queue"
[2020-05-26T17:15:13,475][DEBUG][logstash.runner          ] path.settings: "D:/Softwares/logstash-7.7.0/config"
[2020-05-26T17:15:13,477][DEBUG][logstash.runner          ] path.logs: "D:/Softwares/logstash-7.7.0/logs"
[2020-05-26T17:15:13,479][DEBUG][logstash.runner          ] xpack.management.enabled: false
[2020-05-26T17:15:13,481][DEBUG][logstash.runner          ] xpack.management.logstash.poll_interval: 5000000000
[2020-05-26T17:15:13,483][DEBUG][logstash.runner          ] xpack.management.pipeline.id: ["main"]
[2020-05-26T17:15:13,484][DEBUG][logstash.runner          ] xpack.management.elasticsearch.username: "logstash_system"
[2020-05-26T17:15:13,486][DEBUG][logstash.runner          ] xpack.management.elasticsearch.hosts: ["https://localhost:9200"]
[2020-05-26T17:15:13,487][DEBUG][logstash.runner          ] xpack.management.elasticsearch.ssl.verification_mode: "certificate"
[2020-05-26T17:15:13,489][DEBUG][logstash.runner          ] xpack.management.elasticsearch.sniffing: false
[2020-05-26T17:15:13,490][DEBUG][logstash.runner          ] xpack.monitoring.enabled: false
[2020-05-26T17:15:13,491][DEBUG][logstash.runner          ] xpack.monitoring.elasticsearch.hosts: ["http://localhost:9200"]
[2020-05-26T17:15:13,492][DEBUG][logstash.runner          ] xpack.monitoring.collection.interval: 10000000000
[2020-05-26T17:15:13,494][DEBUG][logstash.runner          ] xpack.monitoring.collection.timeout_interval: 600000000000
[2020-05-26T17:15:13,495][DEBUG][logstash.runner          ] xpack.monitoring.elasticsearch.username: "logstash_system"
[2020-05-26T17:15:13,496][DEBUG][logstash.runner          ] xpack.monitoring.elasticsearch.ssl.verification_mode: "certificate"
[2020-05-26T17:15:13,498][DEBUG][logstash.runner          ] xpack.monitoring.elasticsearch.sniffing: false
[2020-05-26T17:15:13,499][DEBUG][logstash.runner          ] xpack.monitoring.collection.pipeline.details.enabled: true
[2020-05-26T17:15:13,501][DEBUG][logstash.runner          ] xpack.monitoring.collection.config.enabled: true
[2020-05-26T17:15:13,503][DEBUG][logstash.runner          ] monitoring.enabled: false
[2020-05-26T17:15:13,504][DEBUG][logstash.runner          ] monitoring.elasticsearch.hosts: ["http://localhost:9200"]
[2020-05-26T17:15:13,505][DEBUG][logstash.runner          ] monitoring.collection.interval: 10000000000
[2020-05-26T17:15:13,506][DEBUG][logstash.runner          ] monitoring.collection.timeout_interval: 600000000000
[2020-05-26T17:15:13,507][DEBUG][logstash.runner          ] monitoring.elasticsearch.username: "logstash_system"
[2020-05-26T17:15:13,508][DEBUG][logstash.runner          ] monitoring.elasticsearch.ssl.verification_mode: "certificate"
[2020-05-26T17:15:13,511][DEBUG][logstash.runner          ] monitoring.elasticsearch.sniffing: false
[2020-05-26T17:15:13,513][DEBUG][logstash.runner          ] monitoring.collection.pipeline.details.enabled: true
[2020-05-26T17:15:13,515][DEBUG][logstash.runner          ] monitoring.collection.config.enabled: true
[2020-05-26T17:15:13,516][DEBUG][logstash.runner          ] node.uuid: ""
[2020-05-26T17:15:13,518][DEBUG][logstash.runner          ] --------------- Logstash Settings -------------------
[2020-05-26T17:15:13,561][WARN ][logstash.config.source.multilocal] Ignoring the 'pipelines.yml' file because modules or command line options are specified
[2020-05-26T17:15:13,569][INFO ][logstash.runner          ] Starting Logstash {"logstash.version"=>"7.7.0"}
[2020-05-26T17:15:13,620][DEBUG][logstash.agent           ] Setting up metric collection
[2020-05-26T17:15:13,684][DEBUG][logstash.instrument.periodicpoller.os] Starting {:polling_interval=>5, :polling_timeout=>120}
[2020-05-26T17:15:13,699][DEBUG][logstash.instrument.periodicpoller.cgroup] One or more required cgroup files or directories not found: /proc/self/cgroup, /sys/fs/cgroup/cpuacct, /sys/fs/cgroup/cpu
[2020-05-26T17:15:13,838][DEBUG][logstash.instrument.periodicpoller.jvm] Starting {:polling_interval=>5, :polling_timeout=>120}
[2020-05-26T17:15:13,947][DEBUG][logstash.instrument.periodicpoller.jvm] collector name {:name=>"ParNew"}
[2020-05-26T17:15:13,955][DEBUG][logstash.instrument.periodicpoller.jvm] collector name {:name=>"ConcurrentMarkSweep"}
[2020-05-26T17:15:13,974][DEBUG][logstash.instrument.periodicpoller.persistentqueue] Starting {:polling_interval=>5, :polling_timeout=>120}
[2020-05-26T17:15:13,987][DEBUG][logstash.instrument.periodicpoller.deadletterqueue] Starting {:polling_interval=>5, :polling_timeout=>120}
[2020-05-26T17:15:14,039][DEBUG][logstash.agent           ] Starting agent
[2020-05-26T17:15:14,100][DEBUG][logstash.config.source.local.configpathloader] Skipping the following files while reading config since they don't match the specified glob pattern {:files=>["D:/Softwares/logstash-7.7.0/config/jvm.options", "D:/Softwares/logstash-7.7.0/config/log4j2.properties", "D:/Softwares/logstash-7.7.0/config/logstash.yml", "D:/Softwares/logstash-7.7.0/config/pipelines.yml", "D:/Softwares/logstash-7.7.0/config/startup.options"]}
[2020-05-26T17:15:14,105][DEBUG][logstash.config.source.local.configpathloader] Reading config file {:config_file=>"D:/Softwares/logstash-7.7.0/config/logstash-sample.conf"}
[2020-05-26T17:15:14,155][DEBUG][logstash.agent           ] Converging pipelines state {:actions_count=>1}
[2020-05-26T17:15:14,173][DEBUG][logstash.agent           ] Executing action {:action=>LogStash::PipelineAction::Create/pipeline_id:main}
[2020-05-26T17:15:15,159][DEBUG][org.logstash.secret.store.SecretStoreFactory] Attempting to exists or secret store with implementation: org.logstash.secret.store.backend.JavaKeyStore
[2020-05-26T17:15:15,535][DEBUG][org.reflections.Reflections] going to scan these urls:
jar:file:/D:/Softwares/logstash-7.7.0/logstash-core/lib/jars/logstash-core.jar!/
[2020-05-26T17:15:15,583][INFO ][org.reflections.Reflections] Reflections took 44 ms to scan 1 urls, producing 21 keys and 41 values 
[2020-05-26T17:15:15,594][DEBUG][org.reflections.Reflections] expanded subtype co.elastic.logstash.api.Plugin -> co.elastic.logstash.api.Input
[2020-05-26T17:15:15,597][DEBUG][org.reflections.Reflections] expanded subtype co.elastic.logstash.api.Plugin -> co.elastic.logstash.api.Codec
[2020-05-26T17:15:15,601][DEBUG][org.reflections.Reflections] expanded subtype org.jruby.RubyBasicObject -> org.jruby.RubyObject
[2020-05-26T17:15:15,607][DEBUG][org.reflections.Reflections] expanded subtype java.lang.Cloneable -> org.jruby.RubyBasicObject
[2020-05-26T17:15:15,608][DEBUG][org.reflections.Reflections] expanded subtype org.jruby.runtime.builtin.IRubyObject -> org.jruby.RubyBasicObject
[2020-05-26T17:15:15,609][DEBUG][org.reflections.Reflections] expanded subtype java.io.Serializable -> org.jruby.RubyBasicObject
[2020-05-26T17:15:15,617][DEBUG][org.reflections.Reflections] expanded subtype java.lang.Comparable -> org.jruby.RubyBasicObject
[2020-05-26T17:15:15,624][DEBUG][org.reflections.Reflections] expanded subtype org.jruby.runtime.marshal.CoreObjectType -> org.jruby.RubyBasicObject
[2020-05-26T17:15:15,625][DEBUG][org.reflections.Reflections] expanded subtype org.jruby.runtime.builtin.InstanceVariables -> org.jruby.RubyBasicObject
[2020-05-26T17:15:15,631][DEBUG][org.reflections.Reflections] expanded subtype org.jruby.runtime.builtin.InternalVariables -> org.jruby.RubyBasicObject
[2020-05-26T17:15:15,638][DEBUG][org.reflections.Reflections] expanded subtype co.elastic.logstash.api.Plugin -> co.elastic.logstash.api.Output
[2020-05-26T17:15:15,639][DEBUG][org.reflections.Reflections] expanded subtype co.elastic.logstash.api.Metric -> co.elastic.logstash.api.NamespacedMetric
[2020-05-26T17:15:15,645][DEBUG][org.reflections.Reflections] expanded subtype java.security.SecureClassLoader -> java.net.URLClassLoader
[2020-05-26T17:15:15,652][DEBUG][org.reflections.Reflections] expanded subtype java.lang.ClassLoader -> java.security.SecureClassLoader
[2020-05-26T17:15:15,654][DEBUG][org.reflections.Reflections] expanded subtype java.io.Closeable -> java.net.URLClassLoader
[2020-05-26T17:15:15,659][DEBUG][org.reflections.Reflections] expanded subtype java.lang.AutoCloseable -> java.io.Closeable
[2020-05-26T17:15:15,665][DEBUG][org.reflections.Reflections] expanded subtype java.lang.Comparable -> java.lang.Enum
[2020-05-26T17:15:15,671][DEBUG][org.reflections.Reflections] expanded subtype java.io.Serializable -> java.lang.Enum
[2020-05-26T17:15:15,674][DEBUG][org.reflections.Reflections] expanded subtype co.elastic.logstash.api.Plugin -> co.elastic.logstash.api.Filter
[2020-05-26T17:15:15,901][DEBUG][logstash.plugins.registry] On demand adding plugin to the registry {:name=>"file", :type=>"input", :class=>LogStash::Inputs::File}
[2020-05-26T17:15:16,030][DEBUG][logstash.plugins.registry] On demand adding plugin to the registry {:name=>"plain", :type=>"codec", :class=>LogStash::Codecs::Plain}
[2020-05-26T17:15:16,051][DEBUG][logstash.codecs.plain    ] config LogStash::Codecs::Plain/@id = "plain_0a473f63-0851-4420-9a80-f28c879c8f23"
[2020-05-26T17:15:16,054][DEBUG][logstash.codecs.plain    ] config LogStash::Codecs::Plain/@enable_metric = true
[2020-05-26T17:15:16,058][DEBUG][logstash.codecs.plain    ] config LogStash::Codecs::Plain/@charset = "UTF-8"
[2020-05-26T17:15:16,095][DEBUG][logstash.inputs.file     ] config LogStash::Inputs::File/@path = ["C:/Users/karan/Desktop/app.log"]
[2020-05-26T17:15:16,097][DEBUG][logstash.inputs.file     ] config LogStash::Inputs::File/@id = "e7a181030b83e1a5f6683c0258731fbbd14e4985f3cd81c1d9a872faec8ade6d"
[2020-05-26T17:15:16,107][DEBUG][logstash.inputs.file     ] config LogStash::Inputs::File/@type = "applog"
[2020-05-26T17:15:16,108][DEBUG][logstash.inputs.file     ] config LogStash::Inputs::File/@tags = ["applog"]
[2020-05-26T17:15:16,110][DEBUG][logstash.inputs.file     ] config LogStash::Inputs::File/@enable_metric = true
[2020-05-26T17:15:16,122][DEBUG][logstash.inputs.file     ] config LogStash::Inputs::File/@codec = <LogStash::Codecs::Plain id=>"plain_0a473f63-0851-4420-9a80-f28c879c8f23", enable_metric=>true, charset=>"UTF-8">
[2020-05-26T17:15:16,125][DEBUG][logstash.inputs.file     ] config LogStash::Inputs::File/@add_field = {}
[2020-05-26T17:15:16,129][DEBUG][logstash.inputs.file     ] config LogStash::Inputs::File/@stat_interval = 1.0
[2020-05-26T17:15:16,135][DEBUG][logstash.inputs.file     ] config LogStash::Inputs::File/@discover_interval = 15
[2020-05-26T17:15:16,137][DEBUG][logstash.inputs.file     ] config LogStash::Inputs::File/@sincedb_write_interval = 15.0
[2020-05-26T17:15:16,142][DEBUG][logstash.inputs.file     ] config LogStash::Inputs::File/@start_position = "end"
[2020-05-26T17:15:16,147][DEBUG][logstash.inputs.file     ] config LogStash::Inputs::File/@delimiter = "\n"
[2020-05-26T17:15:16,148][DEBUG][logstash.inputs.file     ] config LogStash::Inputs::File/@close_older = 3600.0
[2020-05-26T17:15:16,149][DEBUG][logstash.inputs.file     ] config LogStash::Inputs::File/@mode = "tail"
[2020-05-26T17:15:16,150][DEBUG][logstash.inputs.file     ] config LogStash::Inputs::File/@file_completed_action = "delete"
[2020-05-26T17:15:16,156][DEBUG][logstash.inputs.file     ] config LogStash::Inputs::File/@sincedb_clean_after = 1209600.0
[2020-05-26T17:15:16,164][DEBUG][logstash.inputs.file     ] config LogStash::Inputs::File/@file_chunk_size = 32768
[2020-05-26T17:15:16,168][DEBUG][logstash.inputs.file     ] config LogStash::Inputs::File/@file_chunk_count = 140737488355327
[2020-05-26T17:15:16,176][DEBUG][logstash.inputs.file     ] config LogStash::Inputs::File/@file_sort_by = "last_modified"
[2020-05-26T17:15:16,181][DEBUG][logstash.inputs.file     ] config LogStash::Inputs::File/@file_sort_direction = "asc"
[2020-05-26T17:15:16,186][DEBUG][logstash.inputs.file     ] config LogStash::Inputs::File/@exit_after_read = false
[2020-05-26T17:15:16,256][DEBUG][logstash.plugins.registry] On demand adding plugin to the registry {:name=>"grok", :type=>"filter", :class=>LogStash::Filters::Grok}
[2020-05-26T17:15:16,272][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@match = {"message"=>"%{TIME:log_time} %{LOGLEVEL:log_level} %{DATA:class_name} %{GREEDYDATA :threadName}"}
[2020-05-26T17:15:16,273][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@id = "449d117e0db14733ffa7c115ab6a07f5f311aa4367a21e085c9d69fc6ae85cc9"
[2020-05-26T17:15:16,275][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@enable_metric = true
[2020-05-26T17:15:16,276][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@add_tag = []
[2020-05-26T17:15:16,277][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@remove_tag = []
[2020-05-26T17:15:16,279][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@add_field = {}
[2020-05-26T17:15:16,286][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@remove_field = []
[2020-05-26T17:15:16,294][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@periodic_flush = false
[2020-05-26T17:15:16,300][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@patterns_dir = []
[2020-05-26T17:15:16,311][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@pattern_definitions = {}
[2020-05-26T17:15:16,312][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@patterns_files_glob = "*"
[2020-05-26T17:15:16,318][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@break_on_match = true
[2020-05-26T17:15:16,319][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@named_captures_only = true
[2020-05-26T17:15:16,320][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@keep_empty_captures = false
[2020-05-26T17:15:16,328][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@tag_on_failure = ["_grokparsefailure"]
[2020-05-26T17:15:16,331][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@timeout_millis = 30000
[2020-05-26T17:15:16,332][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@timeout_scope = "pattern"
[2020-05-26T17:15:16,663][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@tag_on_timeout = "_groktimeout"
[2020-05-26T17:15:16,665][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@overwrite = []
[2020-05-26T17:15:16,684][DEBUG][logstash.plugins.registry] On demand adding plugin to the registry {:name=>"file", :type=>"output", :class=>LogStash::Outputs::File}
[2020-05-26T17:15:16,704][DEBUG][logstash.plugins.registry] On demand adding plugin to the registry {:name=>"json_lines", :type=>"codec", :class=>LogStash::Codecs::JSONLines}
[2020-05-26T17:15:16,711][DEBUG][logstash.codecs.jsonlines] config LogStash::Codecs::JSONLines/@id = "json_lines_b54bdaf9-9d1d-45d2-aea4-a034cf54a483"
[2020-05-26T17:15:16,715][DEBUG][logstash.codecs.jsonlines] config LogStash::Codecs::JSONLines/@enable_metric = true
[2020-05-26T17:15:16,716][DEBUG][logstash.codecs.jsonlines] config LogStash::Codecs::JSONLines/@charset = "UTF-8"
[2020-05-26T17:15:16,718][DEBUG][logstash.codecs.jsonlines] config LogStash::Codecs::JSONLines/@delimiter = "\n"
[2020-05-26T17:15:16,728][DEBUG][logstash.outputs.file    ] config LogStash::Outputs::File/@path = "C:/Users/karan/Desktop/output.txt"
[2020-05-26T17:15:16,730][DEBUG][logstash.outputs.file    ] config LogStash::Outputs::File/@id = "005d7a6a7c2eabd470f6c2a0dda909b7078cf3af678917e60d710782155df230"
[2020-05-26T17:15:16,732][DEBUG][logstash.outputs.file    ] config LogStash::Outputs::File/@enable_metric = true
[2020-05-26T17:15:16,737][DEBUG][logstash.outputs.file    ] config LogStash::Outputs::File/@codec = <LogStash::Codecs::JSONLines id=>"json_lines_b54bdaf9-9d1d-45d2-aea4-a034cf54a483", enable_metric=>true, charset=>"UTF-8", delimiter=>"\n">
[2020-05-26T17:15:16,745][DEBUG][logstash.outputs.file    ] config LogStash::Outputs::File/@workers = 1
[2020-05-26T17:15:16,756][DEBUG][logstash.outputs.file    ] config LogStash::Outputs::File/@flush_interval = 2
[2020-05-26T17:15:16,757][DEBUG][logstash.outputs.file    ] config LogStash::Outputs::File/@gzip = false
[2020-05-26T17:15:16,759][DEBUG][logstash.outputs.file    ] config LogStash::Outputs::File/@filename_failure = "_filepath_failures"
[2020-05-26T17:15:16,764][DEBUG][logstash.outputs.file    ] config LogStash::Outputs::File/@create_if_deleted = true
[2020-05-26T17:15:16,770][DEBUG][logstash.outputs.file    ] config LogStash::Outputs::File/@dir_mode = -1
[2020-05-26T17:15:16,771][DEBUG][logstash.outputs.file    ] config LogStash::Outputs::File/@file_mode = -1
[2020-05-26T17:15:16,772][DEBUG][logstash.outputs.file    ] config LogStash::Outputs::File/@write_behavior = "append"
[2020-05-26T17:15:16,814][DEBUG][logstash.javapipeline    ] Starting pipeline {:pipeline_id=>"main"}
[2020-05-26T17:15:16,868][DEBUG][logstash.filters.grok    ][main] Grok patterns path {:paths=>["D:/Softwares/logstash-7.7.0/vendor/bundle/jruby/2.5.0/gems/logstash-patterns-core-4.1.2/patterns", "D:/Softwares/logstash-7.7.0/patterns/*"]}
[2020-05-26T17:15:16,880][DEBUG][logstash.filters.grok    ][main] Grok patterns path {:paths=>[]}
[2020-05-26T17:15:16,884][DEBUG][logstash.filters.grok    ][main] Match data {:match=>{"message"=>"%{TIME:log_time} %{LOGLEVEL:log_level} %{DATA:class_name} %{GREEDYDATA :threadName}"}}
[2020-05-26T17:15:16,895][DEBUG][logstash.filters.grok    ][main] regexp: /message {:pattern=>"%{TIME:log_time} %{LOGLEVEL:log_level} %{DATA:class_name} %{GREEDYDATA :threadName}"}
[2020-05-26T17:15:16,940][DEBUG][logstash.filters.grok    ][main] Adding pattern {"S3_REQUEST_LINE"=>"(?:%{WORD:verb} %{NOTSPACE:request}(?: HTTP/%{NUMBER:httpversion})?|%{DATA:rawrequest})"}
[2020-05-26T17:15:16,944][DEBUG][logstash.filters.grok    ][main] Adding pattern {"S3_ACCESS_LOG"=>"%{WORD:owner} %{NOTSPACE:bucket} \\[%{HTTPDATE:timestamp}\\] %{IP:clientip} %{NOTSPACE:requester} %{NOTSPACE:request_id} %{NOTSPACE:operation} %{NOTSPACE:key} (?:\"%{S3_REQUEST_LINE}\"|-) (?:%{INT:response:int}|-) (?:-|%{NOTSPACE:error_code}) (?:%{INT:bytes:int}|-) (?:%{INT:object_size:int}|-) (?:%{INT:request_time_ms:int}|-) (?:%{INT:turnaround_time_ms:int}|-) (?:%{QS:referrer}|-) (?:\"?%{QS:agent}\"?|-) (?:-|%{NOTSPACE:version_id})"}
[2020-05-26T17:15:16,953][DEBUG][logstash.filters.grok    ][main] Adding pattern {"ELB_URIPATHPARAM"=>"%{URIPATH:path}(?:%{URIPARAM:params})?"}
[2020-05-26T17:15:16,958][DEBUG][logstash.filters.grok    ][main] Adding pattern {"ELB_URI"=>"%{URIPROTO:proto}://(?:%{USER}(?::[^@]*)?@)?(?:%{URIHOST:urihost})?(?:%{ELB_URIPATHPARAM})?"}
[2020-05-26T17:15:16,964][DEBUG][logstash.filters.grok    ][main] Adding pattern {"ELB_REQUEST_LINE"=>"(?:%{WORD:verb} %{ELB_URI:request}(?: HTTP/%{NUMBER:httpversion})?|%{DATA:rawrequest})"}
[2020-05-26T17:15:16,966][DEBUG][logstash.filters.grok    ][main] Adding pattern {"ELB_ACCESS_LOG"=>"%{TIMESTAMP_ISO8601:timestamp} %{NOTSPACE:elb} %{IP:clientip}:%{INT:clientport:int} (?:(%{IP:backendip}:?:%{INT:backendport:int})|-) %{NUMBER:request_processing_time:float} %{NUMBER:backend_processing_time:float} %{NUMBER:response_processing_time:float} %{INT:response:int} %{INT:backend_response:int} %{INT:received_bytes:int} %{INT:bytes:int} \"%{ELB_REQUEST_LINE}\""}
[2020-05-26T17:15:16,971][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CLOUDFRONT_ACCESS_LOG"=>"(?<timestamp>%{YEAR}-%{MONTHNUM}-%{MONTHDAY}\\t%{TIME})\\t%{WORD:x_edge_location}\\t(?:%{NUMBER:sc_bytes:int}|-)\\t%{IPORHOST:clientip}\\t%{WORD:cs_method}\\t%{HOSTNAME:cs_host}\\t%{NOTSPACE:cs_uri_stem}\\t%{NUMBER:sc_status:int}\\t%{GREEDYDATA:referrer}\\t%{GREEDYDATA:agent}\\t%{GREEDYDATA:cs_uri_query}\\t%{GREEDYDATA:cookies}\\t%{WORD:x_edge_result_type}\\t%{NOTSPACE:x_edge_request_id}\\t%{HOSTNAME:x_host_header}\\t%{URIPROTO:cs_protocol}\\t%{INT:cs_bytes:int}\\t%{GREEDYDATA:time_taken:float}\\t%{GREEDYDATA:x_forwarded_for}\\t%{GREEDYDATA:ssl_protocol}\\t%{GREEDYDATA:ssl_cipher}\\t%{GREEDYDATA:x_edge_response_result_type}"}
[2020-05-26T17:15:16,982][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_TIMESTAMP"=>"%{MONTHDAY}-%{MONTH} %{HOUR}:%{MINUTE}"}
[2020-05-26T17:15:16,998][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_HOST"=>"[a-zA-Z0-9-]+"}
[2020-05-26T17:15:17,007][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_VOLUME"=>"%{USER}"}
[2020-05-26T17:15:17,016][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_DEVICE"=>"%{USER}"}
[2020-05-26T17:15:17,029][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_DEVICEPATH"=>"%{UNIXPATH}"}
[2020-05-26T17:15:17,036][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_CAPACITY"=>"%{INT}{1,3}(,%{INT}{3})*"}
[2020-05-26T17:15:17,043][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_VERSION"=>"%{USER}"}
[2020-05-26T17:15:17,047][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_JOB"=>"%{USER}"}
[2020-05-26T17:15:17,054][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_MAX_CAPACITY"=>"User defined maximum volume capacity %{BACULA_CAPACITY} exceeded on device \\\"%{BACULA_DEVICE:device}\\\" \\(%{BACULA_DEVICEPATH}\\)"}
[2020-05-26T17:15:17,055][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_END_VOLUME"=>"End of medium on Volume \\\"%{BACULA_VOLUME:volume}\\\" Bytes=%{BACULA_CAPACITY} Blocks=%{BACULA_CAPACITY} at %{MONTHDAY}-%{MONTH}-%{YEAR} %{HOUR}:%{MINUTE}."}
[2020-05-26T17:15:17,060][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_NEW_VOLUME"=>"Created new Volume \\\"%{BACULA_VOLUME:volume}\\\" in catalog."}
[2020-05-26T17:15:17,065][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_NEW_LABEL"=>"Labeled new Volume \\\"%{BACULA_VOLUME:volume}\\\" on device \\\"%{BACULA_DEVICE:device}\\\" \\(%{BACULA_DEVICEPATH}\\)."}
[2020-05-26T17:15:17,067][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_WROTE_LABEL"=>"Wrote label to prelabeled Volume \\\"%{BACULA_VOLUME:volume}\\\" on device \\\"%{BACULA_DEVICE}\\\" \\(%{BACULA_DEVICEPATH}\\)"}
[2020-05-26T17:15:17,068][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_NEW_MOUNT"=>"New volume \\\"%{BACULA_VOLUME:volume}\\\" mounted on device \\\"%{BACULA_DEVICE:device}\\\" \\(%{BACULA_DEVICEPATH}\\) at %{MONTHDAY}-%{MONTH}-%{YEAR} %{HOUR}:%{MINUTE}."}
[2020-05-26T17:15:17,073][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_NOOPEN"=>"\\s+Cannot open %{DATA}: ERR=%{GREEDYDATA:berror}"}
[2020-05-26T17:15:17,082][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_NOOPENDIR"=>"\\s+Could not open directory %{DATA}: ERR=%{GREEDYDATA:berror}"}
[2020-05-26T17:15:17,087][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_NOSTAT"=>"\\s+Could not stat %{DATA}: ERR=%{GREEDYDATA:berror}"}
[2020-05-26T17:15:17,097][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_NOJOBS"=>"There are no more Jobs associated with Volume \\\"%{BACULA_VOLUME:volume}\\\". Marking it purged."}
[2020-05-26T17:15:17,106][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_ALL_RECORDS_PRUNED"=>"All records pruned from Volume \\\"%{BACULA_VOLUME:volume}\\\"; marking it \\\"Purged\\\""}
[2020-05-26T17:15:17,107][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_BEGIN_PRUNE_JOBS"=>"Begin pruning Jobs older than %{INT} month %{INT} days ."}
[2020-05-26T17:15:17,108][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_BEGIN_PRUNE_FILES"=>"Begin pruning Files."}
[2020-05-26T17:15:17,113][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_PRUNED_JOBS"=>"Pruned %{INT} Jobs* for client %{BACULA_HOST:client} from catalog."}
[2020-05-26T17:15:17,119][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_PRUNED_FILES"=>"Pruned Files from %{INT} Jobs* for client %{BACULA_HOST:client} from catalog."}
[2020-05-26T17:15:17,120][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_ENDPRUNE"=>"End auto prune."}
[2020-05-26T17:15:17,120][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_STARTJOB"=>"Start Backup JobId %{INT}, Job=%{BACULA_JOB:job}"}
[2020-05-26T17:15:17,121][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_STARTRESTORE"=>"Start Restore Job %{BACULA_JOB:job}"}
[2020-05-26T17:15:17,135][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_USEDEVICE"=>"Using Device \\\"%{BACULA_DEVICE:device}\\\""}
[2020-05-26T17:15:17,145][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_DIFF_FS"=>"\\s+%{UNIXPATH} is a different filesystem. Will not descend from %{UNIXPATH} into it."}
[2020-05-26T17:15:17,147][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_JOBEND"=>"Job write elapsed time = %{DATA:elapsed}, Transfer rate = %{NUMBER} (K|M|G)? Bytes/second"}
[2020-05-26T17:15:17,149][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_NOPRUNE_JOBS"=>"No Jobs found to prune."}
[2020-05-26T17:15:17,150][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_NOPRUNE_FILES"=>"No Files found to prune."}
[2020-05-26T17:15:17,151][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_VOLUME_PREVWRITTEN"=>"Volume \\\"%{BACULA_VOLUME:volume}\\\" previously written, moving to end of data."}
[2020-05-26T17:15:17,152][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_READYAPPEND"=>"Ready to append to end of Volume \\\"%{BACULA_VOLUME:volume}\\\" size=%{INT}"}
[2020-05-26T17:15:17,162][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_CANCELLING"=>"Cancelling duplicate JobId=%{INT}."}
[2020-05-26T17:15:17,163][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_MARKCANCEL"=>"JobId %{INT}, Job %{BACULA_JOB:job} marked to be canceled."}
[2020-05-26T17:15:17,163][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_CLIENT_RBJ"=>"shell command: run ClientRunBeforeJob \\\"%{GREEDYDATA:runjob}\\\""}
[2020-05-26T17:15:17,164][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_VSS"=>"(Generate )?VSS (Writer)?"}
[2020-05-26T17:15:17,165][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_MAXSTART"=>"Fatal error: Job canceled because max start delay time exceeded."}
[2020-05-26T17:15:17,166][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_DUPLICATE"=>"Fatal error: JobId %{INT:duplicate} already running. Duplicate job not allowed."}
[2020-05-26T17:15:17,167][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_NOJOBSTAT"=>"Fatal error: No Job status returned from FD."}
[2020-05-26T17:15:17,168][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_FATAL_CONN"=>"Fatal error: bsock.c:133 Unable to connect to (Client: %{BACULA_HOST:client}|Storage daemon) on %{HOSTNAME}:%{POSINT}. ERR=(?<berror>%{GREEDYDATA})"}
[2020-05-26T17:15:17,169][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_NO_CONNECT"=>"Warning: bsock.c:127 Could not connect to (Client: %{BACULA_HOST:client}|Storage daemon) on %{HOSTNAME}:%{POSINT}. ERR=(?<berror>%{GREEDYDATA})"}
[2020-05-26T17:15:17,170][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_NO_AUTH"=>"Fatal error: Unable to authenticate with File daemon at %{HOSTNAME}. Possible causes:"}
[2020-05-26T17:15:17,179][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_NOSUIT"=>"No prior or suitable Full backup found in catalog. Doing FULL backup."}
[2020-05-26T17:15:17,181][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_NOPRIOR"=>"No prior Full backup Job record found."}
[2020-05-26T17:15:17,183][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_JOB"=>"(Error: )?Bacula %{BACULA_HOST} %{BACULA_VERSION} \\(%{BACULA_VERSION}\\):"}
[2020-05-26T17:15:17,184][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOGLINE"=>"%{BACULA_TIMESTAMP:bts} %{BACULA_HOST:hostname} JobId %{INT:jobid}: (%{BACULA_LOG_MAX_CAPACITY}|%{BACULA_LOG_END_VOLUME}|%{BACULA_LOG_NEW_VOLUME}|%{BACULA_LOG_NEW_LABEL}|%{BACULA_LOG_WROTE_LABEL}|%{BACULA_LOG_NEW_MOUNT}|%{BACULA_LOG_NOOPEN}|%{BACULA_LOG_NOOPENDIR}|%{BACULA_LOG_NOSTAT}|%{BACULA_LOG_NOJOBS}|%{BACULA_LOG_ALL_RECORDS_PRUNED}|%{BACULA_LOG_BEGIN_PRUNE_JOBS}|%{BACULA_LOG_BEGIN_PRUNE_FILES}|%{BACULA_LOG_PRUNED_JOBS}|%{BACULA_LOG_PRUNED_FILES}|%{BACULA_LOG_ENDPRUNE}|%{BACULA_LOG_STARTJOB}|%{BACULA_LOG_STARTRESTORE}|%{BACULA_LOG_USEDEVICE}|%{BACULA_LOG_DIFF_FS}|%{BACULA_LOG_JOBEND}|%{BACULA_LOG_NOPRUNE_JOBS}|%{BACULA_LOG_NOPRUNE_FILES}|%{BACULA_LOG_VOLUME_PREVWRITTEN}|%{BACULA_LOG_READYAPPEND}|%{BACULA_LOG_CANCELLING}|%{BACULA_LOG_MARKCANCEL}|%{BACULA_LOG_CLIENT_RBJ}|%{BACULA_LOG_VSS}|%{BACULA_LOG_MAXSTART}|%{BACULA_LOG_DUPLICATE}|%{BACULA_LOG_NOJOBSTAT}|%{BACULA_LOG_FATAL_CONN}|%{BACULA_LOG_NO_CONNECT}|%{BACULA_LOG_NO_AUTH}|%{BACULA_LOG_NOSUIT}|%{BACULA_LOG_JOB}|%{BACULA_LOG_NOPRIOR})"}
[2020-05-26T17:15:17,186][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BIND9_TIMESTAMP"=>"%{MONTHDAY}[-]%{MONTH}[-]%{YEAR} %{TIME}"}
[2020-05-26T17:15:17,188][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BIND9"=>"%{BIND9_TIMESTAMP:timestamp} queries: %{LOGLEVEL:loglevel}: client %{IP:clientip}#%{POSINT:clientport} \\(%{GREEDYDATA:query}\\): query: %{GREEDYDATA:query} IN %{GREEDYDATA:querytype} \\(%{IP:dns}\\)"}
[2020-05-26T17:15:17,197][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BRO_HTTP"=>"%{NUMBER:ts}\\t%{NOTSPACE:uid}\\t%{IP:orig_h}\\t%{INT:orig_p}\\t%{IP:resp_h}\\t%{INT:resp_p}\\t%{INT:trans_depth}\\t%{GREEDYDATA:method}\\t%{GREEDYDATA:domain}\\t%{GREEDYDATA:uri}\\t%{GREEDYDATA:referrer}\\t%{GREEDYDATA:user_agent}\\t%{NUMBER:request_body_len}\\t%{NUMBER:response_body_len}\\t%{GREEDYDATA:status_code}\\t%{GREEDYDATA:status_msg}\\t%{GREEDYDATA:info_code}\\t%{GREEDYDATA:info_msg}\\t%{GREEDYDATA:filename}\\t%{GREEDYDATA:bro_tags}\\t%{GREEDYDATA:username}\\t%{GREEDYDATA:password}\\t%{GREEDYDATA:proxied}\\t%{GREEDYDATA:orig_fuids}\\t%{GREEDYDATA:orig_mime_types}\\t%{GREEDYDATA:resp_fuids}\\t%{GREEDYDATA:resp_mime_types}"}
[2020-05-26T17:15:17,199][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BRO_DNS"=>"%{NUMBER:ts}\\t%{NOTSPACE:uid}\\t%{IP:orig_h}\\t%{INT:orig_p}\\t%{IP:resp_h}\\t%{INT:resp_p}\\t%{WORD:proto}\\t%{INT:trans_id}\\t%{GREEDYDATA:query}\\t%{GREEDYDATA:qclass}\\t%{GREEDYDATA:qclass_name}\\t%{GREEDYDATA:qtype}\\t%{GREEDYDATA:qtype_name}\\t%{GREEDYDATA:rcode}\\t%{GREEDYDATA:rcode_name}\\t%{GREEDYDATA:AA}\\t%{GREEDYDATA:TC}\\t%{GREEDYDATA:RD}\\t%{GREEDYDATA:RA}\\t%{GREEDYDATA:Z}\\t%{GREEDYDATA:answers}\\t%{GREEDYDATA:TTLs}\\t%{GREEDYDATA:rejected}"}
[2020-05-26T17:15:17,201][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BRO_CONN"=>"%{NUMBER:ts}\\t%{NOTSPACE:uid}\\t%{IP:orig_h}\\t%{INT:orig_p}\\t%{IP:resp_h}\\t%{INT:resp_p}\\t%{WORD:proto}\\t%{GREEDYDATA:service}\\t%{NUMBER:duration}\\t%{NUMBER:orig_bytes}\\t%{NUMBER:resp_bytes}\\t%{GREEDYDATA:conn_state}\\t%{GREEDYDATA:local_orig}\\t%{GREEDYDATA:missed_bytes}\\t%{GREEDYDATA:history}\\t%{GREEDYDATA:orig_pkts}\\t%{GREEDYDATA:orig_ip_bytes}\\t%{GREEDYDATA:resp_pkts}\\t%{GREEDYDATA:resp_ip_bytes}\\t%{GREEDYDATA:tunnel_parents}"}
[2020-05-26T17:15:17,202][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BRO_FILES"=>"%{NUMBER:ts}\\t%{NOTSPACE:fuid}\\t%{IP:tx_hosts}\\t%{IP:rx_hosts}\\t%{NOTSPACE:conn_uids}\\t%{GREEDYDATA:source}\\t%{GREEDYDATA:depth}\\t%{GREEDYDATA:analyzers}\\t%{GREEDYDATA:mime_type}\\t%{GREEDYDATA:filename}\\t%{GREEDYDATA:duration}\\t%{GREEDYDATA:local_orig}\\t%{GREEDYDATA:is_orig}\\t%{GREEDYDATA:seen_bytes}\\t%{GREEDYDATA:total_bytes}\\t%{GREEDYDATA:missing_bytes}\\t%{GREEDYDATA:overflow_bytes}\\t%{GREEDYDATA:timedout}\\t%{GREEDYDATA:parent_fuid}\\t%{GREEDYDATA:md5}\\t%{GREEDYDATA:sha1}\\t%{GREEDYDATA:sha256}\\t%{GREEDYDATA:extracted}"}
[2020-05-26T17:15:17,204][DEBUG][logstash.filters.grok    ][main] Adding pattern {"EXIM_MSGID"=>"[0-9A-Za-z]{6}-[0-9A-Za-z]{6}-[0-9A-Za-z]{2}"}
[2020-05-26T17:15:17,212][DEBUG][logstash.filters.grok    ][main] Adding pattern {"EXIM_FLAGS"=>"(<=|[-=>*]>|[*]{2}|==)"}
[2020-05-26T17:15:17,214][DEBUG][logstash.filters.grok    ][main] Adding pattern {"EXIM_DATE"=>"%{YEAR:exim_year}-%{MONTHNUM:exim_month}-%{MONTHDAY:exim_day} %{TIME:exim_time}"}
[2020-05-26T17:15:17,215][DEBUG][logstash.filters.grok    ][main] Adding pattern {"EXIM_PID"=>"\\[%{POSINT}\\]"}
[2020-05-26T17:15:17,217][DEBUG][logstash.filters.grok    ][main] Adding pattern {"EXIM_QT"=>"((\\d+y)?(\\d+w)?(\\d+d)?(\\d+h)?(\\d+m)?(\\d+s)?)"}
[2020-05-26T17:15:17,218][DEBUG][logstash.filters.grok    ][main] Adding pattern {"EXIM_EXCLUDE_TERMS"=>"(Message is frozen|(Start|End) queue run| Warning: | retry time not reached | no (IP address|host name) found for (IP address|host) | unexpected disconnection while reading SMTP command | no immediate delivery: |another process is handling this message)"}
[2020-05-26T17:15:17,220][DEBUG][logstash.filters.grok    ][main] Adding pattern {"EXIM_REMOTE_HOST"=>"(H=(%{NOTSPACE:remote_hostname} )?(\\(%{NOTSPACE:remote_heloname}\\) )?\\[%{IP:remote_host}\\])"}
[2020-05-26T17:15:17,229][DEBUG][logstash.filters.grok    ][main] Adding pattern {"EXIM_INTERFACE"=>"(I=\\[%{IP:exim_interface}\\](:%{NUMBER:exim_interface_port}))"}
[2020-05-26T17:15:17,230][DEBUG][logstash.filters.grok    ][main] Adding pattern {"EXIM_PROTOCOL"=>"(P=%{NOTSPACE:protocol})"}
[2020-05-26T17:15:17,231][DEBUG][logstash.filters.grok    ][main] Adding pattern {"EXIM_MSG_SIZE"=>"(S=%{NUMBER:exim_msg_size})"}
[2020-05-26T17:15:17,232][DEBUG][logstash.filters.grok    ][main] Adding pattern {"EXIM_HEADER_ID"=>"(id=%{NOTSPACE:exim_header_id})"}
[2020-05-26T17:15:17,234][DEBUG][logstash.filters.grok    ][main] Adding pattern {"EXIM_SUBJECT"=>"(T=%{QS:exim_subject})"}
[2020-05-26T17:15:17,235][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NETSCREENSESSIONLOG"=>"%{SYSLOGTIMESTAMP:date} %{IPORHOST:device} %{IPORHOST}: NetScreen device_id=%{WORD:device_id}%{DATA}: start_time=%{QUOTEDSTRING:start_time} duration=%{INT:duration} policy_id=%{INT:policy_id} service=%{DATA:service} proto=%{INT:proto} src zone=%{WORD:src_zone} dst zone=%{WORD:dst_zone} action=%{WORD:action} sent=%{INT:sent} rcvd=%{INT:rcvd} src=%{IPORHOST:src_ip} dst=%{IPORHOST:dst_ip} src_port=%{INT:src_port} dst_port=%{INT:dst_port} src-xlated ip=%{IPORHOST:src_xlated_ip} port=%{INT:src_xlated_port} dst-xlated ip=%{IPORHOST:dst_xlated_ip} port=%{INT:dst_xlated_port} session_id=%{INT:session_id} reason=%{GREEDYDATA:reason}"}
[2020-05-26T17:15:17,237][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCO_TAGGED_SYSLOG"=>"^<%{POSINT:syslog_pri}>%{CISCOTIMESTAMP:timestamp}( %{SYSLOGHOST:sysloghost})? ?: %%{CISCOTAG:ciscotag}:"}
[2020-05-26T17:15:17,244][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOTIMESTAMP"=>"%{MONTH} +%{MONTHDAY}(?: %{YEAR})? %{TIME}"}
[2020-05-26T17:15:17,245][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOTAG"=>"[A-Z0-9]+-%{INT}-(?:[A-Z0-9_]+)"}
[2020-05-26T17:15:17,247][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCO_ACTION"=>"Built|Teardown|Deny|Denied|denied|requested|permitted|denied by ACL|discarded|est-allowed|Dropping|created|deleted"}
[2020-05-26T17:15:17,248][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCO_REASON"=>"Duplicate TCP SYN|Failed to locate egress interface|Invalid transport field|No matching connection|DNS Response|DNS Query|(?:%{WORD}\\s*)*"}
[2020-05-26T17:15:17,249][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCO_DIRECTION"=>"Inbound|inbound|Outbound|outbound"}
[2020-05-26T17:15:17,250][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCO_INTERVAL"=>"first hit|%{INT}-second interval"}
[2020-05-26T17:15:17,252][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCO_XLATE_TYPE"=>"static|dynamic"}
[2020-05-26T17:15:17,259][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOFW104001"=>"\\((?:Primary|Secondary)\\) Switching to ACTIVE - %{GREEDYDATA:switch_reason}"}
[2020-05-26T17:15:17,261][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOFW104002"=>"\\((?:Primary|Secondary)\\) Switching to STANDBY - %{GREEDYDATA:switch_reason}"}
[2020-05-26T17:15:17,263][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOFW104003"=>"\\((?:Primary|Secondary)\\) Switching to FAILED\\."}
[2020-05-26T17:15:17,265][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOFW104004"=>"\\((?:Primary|Secondary)\\) Switching to OK\\."}
[2020-05-26T17:15:17,266][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOFW105003"=>"\\((?:Primary|Secondary)\\) Monitoring on [Ii]nterface %{GREEDYDATA:interface_name} waiting"}
[2020-05-26T17:15:17,267][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOFW105004"=>"\\((?:Primary|Secondary)\\) Monitoring on [Ii]nterface %{GREEDYDATA:interface_name} normal"}
[2020-05-26T17:15:17,276][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOFW105005"=>"\\((?:Primary|Secondary)\\) Lost Failover communications with mate on [Ii]nterface %{GREEDYDATA:interface_name}"}
[2020-05-26T17:15:17,278][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOFW105008"=>"\\((?:Primary|Secondary)\\) Testing [Ii]nterface %{GREEDYDATA:interface_name}"}
[2020-05-26T17:15:17,279][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOFW105009"=>"\\((?:Primary|Secondary)\\) Testing on [Ii]nterface %{GREEDYDATA:interface_name} (?:Passed|Failed)"}
[2020-05-26T17:15:17,280][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOFW106001"=>"%{CISCO_DIRECTION:direction} %{WORD:protocol} connection %{CISCO_ACTION:action} from %{IP:src_ip}/%{INT:src_port} to %{IP:dst_ip}/%{INT:dst_port} flags %{GREEDYDATA:tcp_flags} on interface %{GREEDYDATA:interface}"}
[2020-05-26T17:15:17,282][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOFW106006_106007_106010"=>"%{CISCO_ACTION:action} %{CISCO_DIRECTION:direction} %{WORD:protocol} (?:from|src) %{IP:src_ip}/%{INT:src_port}(\\(%{DATA:src_fwuser}\\))? (?:to|dst) %{IP:dst_ip}/%{INT:dst_port}(\\(%{DATA:dst_fwuser}\\))? (?:on interface %{DATA:interface}|due to %{CISCO_REASON:reason})"}
[2020-05-26T17:15:17,284][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOFW106014"=>"%{CISCO_ACTION:action} %{CISCO_DIRECTION:direction} %{WORD:protocol} src %{DATA:src_interface}:%{IP:src_ip}(\\(%{DATA:src_fwuser}\\))? dst %{DATA:dst_interface}:%{IP:dst_ip}(\\(%{DATA:dst_fwuser}\\))? \\(type %{INT:icmp_type}, code %{INT:icmp_code}\\)"}
[2020-05-26T17:15:17,293][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOFW106015"=>"%{CISCO_ACTION:action} %{WORD:protocol} \\(%{DATA:policy_id}\\) from %{IP:src_ip}/%{INT:src_port} to %{IP:dst_ip}/%{INT:dst_port} flags %{DATA:tcp_flags} on interface %{GREEDYDATA:interface}"}
[2020-05-26T17:15:17,294][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOFW106021"=>"%{CISCO_ACTION:action} %{WORD:protocol} reverse path check from %{IP:src_ip} to %{IP:dst_ip} on interface %{GREEDYDATA:interface}"}
[2020-05-26T17:15:17,296][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOFW106023"=>"%{CISCO_ACTION:action}( protocol)? %{WORD:protocol} src %{DATA:src_interface}:%{DATA:src_ip}(/%{INT:src_port})?(\\(%{DATA:src_fwuser}\\))? dst %{DATA:dst_interface}:%{DATA:dst_ip}(/%{INT:dst_port})?(\\(%{DATA:dst_fwuser}\\))?( \\(type %{INT:icmp_type}, code %{INT:icmp_code}\\))? by access-group \"?%{DATA:policy_id}\"? \\[%{DATA:hashcode1}, %{DATA:hashcode2}\\]"}
[2020-05-26T17:15:17,297][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOFW106100_2_3"=>"access-list %{NOTSPACE:policy_id} %{CISCO_ACTION:action} %{WORD:protocol} for user '%{DATA:src_fwuser}' %{DATA:src_interface}/%{IP:src_ip}\\(%{INT:src_port}\\) -> %{DATA:dst_interface}/%{IP:dst_ip}\\(%{INT:dst_port}\\) hit-cnt %{INT:hit_count} %{CISCO_INTERVAL:interval} \\[%{DATA:hashcode1}, %{DATA:hashcode2}\\]"}
[2020-05-26T17:15:17,298][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOFW106100"=>"access-list %{NOTSPACE:policy_id} %{CISCO_ACTION:action} %{WORD:protocol} %{DATA:src_interface}/%{IP:src_ip}\\(%{INT:src_port}\\)(\\(%{DATA:src_fwuser}\\))? -> %{DATA:dst_interface}/%{IP:dst_ip}\\(%{INT:dst_port}\\)(\\(%{DATA:src_fwuser}\\))? hit-cnt %{INT:hit_count} %{CISCO_INTERVAL:interval} \\[%{DATA:hashcode1}, %{DATA:hashcode2}\\]"}
[2020-05-26T17:15:17,299][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOFW304001"=>"%{IP:src_ip}(\\(%{DATA:src_fwuser}\\))? Accessed URL %{IP:dst_ip}:%{GREEDYDATA:dst_url}"}
[2020-05-26T17:15:17,301][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOFW110002"=>"%{CISCO_REASON:reason} for %{WORD:protocol} from %{DATA:src_interface}:%{IP:src_ip}/%{INT:src_port} to %{IP:dst_ip}/%{INT:dst_port}"}
[2020-05-26T17:15:17,311][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOFW302010"=>"%{INT:connection_count} in use, %{INT:connection_count_max} most used"}
[2020-05-26T17:15:17,312][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOFW302013_302014_302015_302016"=>"%{CISCO_ACTION:action}(?: %{CISCO_DIRECTION:direction})? %{WORD:protocol} connection %{INT:connection_id} for %{DATA:src_interface}:%{IP:src_ip}/%{INT:src_port}( \\(%{IP:src_mapped_ip}/%{INT:src_mapped_port}\\))?(\\(%{DATA:src_fwuser}\\))? to %{DATA:dst_interface}:%{IP:dst_ip}/%{INT:dst_port}( \\(%{IP:dst_mapped_ip}/%{INT:dst_mapped_port}\\))?(\\(%{DATA:dst_fwuser}\\))?( duration %{TIME:duration} bytes %{INT:bytes})?(?: %{CISCO_REASON:reason})?( \\(%{DATA:user}\\))?"}
[2020-05-26T17:15:17,314][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOFW302020_302021"=>"%{CISCO_ACTION:action}(?: %{CISCO_DIRECTION:direction})? %{WORD:protocol} connection for faddr %{IP:dst_ip}/%{INT:icmp_seq_num}(?:\\(%{DATA:fwuser}\\))? gaddr %{IP:src_xlated_ip}/%{INT:icmp_code_xlated} laddr %{IP:src_ip}/%{INT:icmp_code}( \\(%{DATA:user}\\))?"}
[2020-05-26T17:15:17,315][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOFW305011"=>"%{CISCO_ACTION:action} %{CISCO_XLATE_TYPE:xlate_type} %{WORD:protocol} translation from %{DATA:src_interface}:%{IP:src_ip}(/%{INT:src_port})?(\\(%{DATA:src_fwuser}\\))? to %{DATA:src_xlated_interface}:%{IP:src_xlated_ip}/%{DATA:src_xlated_port}"}
[2020-05-26T17:15:17,317][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOFW313001_313004_313008"=>"%{CISCO_ACTION:action} %{WORD:protocol} type=%{INT:icmp_type}, code=%{INT:icmp_code} from %{IP:src_ip} on interface %{DATA:interface}( to %{IP:dst_ip})?"}
[2020-05-26T17:15:17,318][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOFW313005"=>"%{CISCO_REASON:reason} for %{WORD:protocol} error message: %{WORD:err_protocol} src %{DATA:err_src_interface}:%{IP:err_src_ip}(\\(%{DATA:err_src_fwuser}\\))? dst %{DATA:err_dst_interface}:%{IP:err_dst_ip}(\\(%{DATA:err_dst_fwuser}\\))? \\(type %{INT:err_icmp_type}, code %{INT:err_icmp_code}\\) on %{DATA:interface} interface\\.  Original IP payload: %{WORD:protocol} src %{IP:orig_src_ip}/%{INT:orig_src_port}(\\(%{DATA:orig_src_fwuser}\\))? dst %{IP:orig_dst_ip}/%{INT:orig_dst_port}(\\(%{DATA:orig_dst_fwuser}\\))?"}
[2020-05-26T17:15:17,319][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOFW321001"=>"Resource '%{WORD:resource_name}' limit of %{POSINT:resource_limit} reached for system"}
[2020-05-26T17:15:17,329][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOFW402117"=>"%{WORD:protocol}: Received a non-IPSec packet \\(protocol= %{WORD:orig_protocol}\\) from %{IP:src_ip} to %{IP:dst_ip}"}
[2020-05-26T17:15:17,330][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOFW402119"=>"%{WORD:protocol}: Received an %{WORD:orig_protocol} packet \\(SPI= %{DATA:spi}, sequence number= %{DATA:seq_num}\\) from %{IP:src_ip} \\(user= %{DATA:user}\\) to %{IP:dst_ip} that failed anti-replay checking"}
[2020-05-26T17:15:17,332][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOFW419001"=>"%{CISCO_ACTION:action} %{WORD:protocol} packet from %{DATA:src_interface}:%{IP:src_ip}/%{INT:src_port} to %{DATA:dst_interface}:%{IP:dst_ip}/%{INT:dst_port}, reason: %{GREEDYDATA:reason}"}
[2020-05-26T17:15:17,334][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOFW419002"=>"%{CISCO_REASON:reason} from %{DATA:src_interface}:%{IP:src_ip}/%{INT:src_port} to %{DATA:dst_interface}:%{IP:dst_ip}/%{INT:dst_port} with different initial sequence number"}
[2020-05-26T17:15:17,335][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOFW500004"=>"%{CISCO_REASON:reason} for protocol=%{WORD:protocol}, from %{IP:src_ip}/%{INT:src_port} to %{IP:dst_ip}/%{INT:dst_port}"}
[2020-05-26T17:15:17,337][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOFW602303_602304"=>"%{WORD:protocol}: An %{CISCO_DIRECTION:direction} %{GREEDYDATA:tunnel_type} SA \\(SPI= %{DATA:spi}\\) between %{IP:src_ip} and %{IP:dst_ip} \\(user= %{DATA:user}\\) has been %{CISCO_ACTION:action}"}
[2020-05-26T17:15:17,344][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOFW710001_710002_710003_710005_710006"=>"%{WORD:protocol} (?:request|access) %{CISCO_ACTION:action} from %{IP:src_ip}/%{INT:src_port} to %{DATA:dst_interface}:%{IP:dst_ip}/%{INT:dst_port}"}
[2020-05-26T17:15:17,346][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOFW713172"=>"Group = %{GREEDYDATA:group}, IP = %{IP:src_ip}, Automatic NAT Detection Status:\\s+Remote end\\s*%{DATA:is_remote_natted}\\s*behind a NAT device\\s+This\\s+end\\s*%{DATA:is_local_natted}\\s*behind a NAT device"}
[2020-05-26T17:15:17,347][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOFW733100"=>"\\[\\s*%{DATA:drop_type}\\s*\\] drop %{DATA:drop_rate_id} exceeded. Current burst rate is %{INT:drop_rate_current_burst} per second, max configured rate is %{INT:drop_rate_max_burst}; Current average rate is %{INT:drop_rate_current_avg} per second, max configured rate is %{INT:drop_rate_max_avg}; Cumulative total count is %{INT:drop_total_count}"}
[2020-05-26T17:15:17,349][DEBUG][logstash.filters.grok    ][main] Adding pattern {"SHOREWALL"=>"(%{SYSLOGTIMESTAMP:timestamp}) (%{WORD:nf_host}) kernel:.*Shorewall:(%{WORD:nf_action1})?:(%{WORD:nf_action2})?.*IN=(%{USERNAME:nf_in_interface})?.*(OUT= *MAC=(%{COMMONMAC:nf_dst_mac}):(%{COMMONMAC:nf_src_mac})?|OUT=%{USERNAME:nf_out_interface}).*SRC=(%{IPV4:nf_src_ip}).*DST=(%{IPV4:nf_dst_ip}).*LEN=(%{WORD:nf_len}).?*TOS=(%{WORD:nf_tos}).?*PREC=(%{WORD:nf_prec}).?*TTL=(%{INT:nf_ttl}).?*ID=(%{INT:nf_id}).?*PROTO=(%{WORD:nf_protocol}).?*SPT=(%{INT:nf_src_port}?.*DPT=%{INT:nf_dst_port}?.*)"}
[2020-05-26T17:15:17,350][DEBUG][logstash.filters.grok    ][main] Adding pattern {"SFW2"=>"((%{SYSLOGTIMESTAMP})|(%{TIMESTAMP_ISO8601}))\\s*%{HOSTNAME}\\s*kernel\\S+\\s*%{NAGIOSTIME}\\s*SFW2\\-INext\\-%{NOTSPACE:nf_action}\\s*IN=%{USERNAME:nf_in_interface}.*OUT=((\\s*%{USERNAME:nf_out_interface})|(\\s*))MAC=((%{COMMONMAC:nf_dst_mac}:%{COMMONMAC:nf_src_mac})|(\\s*)).*SRC=%{IP:nf_src_ip}\\s*DST=%{IP:nf_dst_ip}.*PROTO=%{WORD:nf_protocol}((.*SPT=%{INT:nf_src_port}.*DPT=%{INT:nf_dst_port}.*)|())"}
[2020-05-26T17:15:17,352][DEBUG][logstash.filters.grok    ][main] Adding pattern {"USERNAME"=>"[a-zA-Z0-9._-]+"}
[2020-05-26T17:15:17,360][DEBUG][logstash.filters.grok    ][main] Adding pattern {"USER"=>"%{USERNAME}"}
[2020-05-26T17:15:17,362][DEBUG][logstash.filters.grok    ][main] Adding pattern {"EMAILLOCALPART"=>"[a-zA-Z][a-zA-Z0-9_.+-=:]+"}
[2020-05-26T17:15:17,363][DEBUG][logstash.filters.grok    ][main] Adding pattern {"EMAILADDRESS"=>"%{EMAILLOCALPART}@%{HOSTNAME}"}
[2020-05-26T17:15:17,364][DEBUG][logstash.filters.grok    ][main] Adding pattern {"INT"=>"(?:[+-]?(?:[0-9]+))"}
[2020-05-26T17:15:17,366][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BASE10NUM"=>"(?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\\.[0-9]+)?)|(?:\\.[0-9]+)))"}
[2020-05-26T17:15:17,368][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NUMBER"=>"(?:%{BASE10NUM})"}
[2020-05-26T17:15:17,377][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BASE16NUM"=>"(?<![0-9A-Fa-f])(?:[+-]?(?:0x)?(?:[0-9A-Fa-f]+))"}
[2020-05-26T17:15:17,378][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BASE16FLOAT"=>"\\b(?<![0-9A-Fa-f.])(?:[+-]?(?:0x)?(?:(?:[0-9A-Fa-f]+(?:\\.[0-9A-Fa-f]*)?)|(?:\\.[0-9A-Fa-f]+)))\\b"}
[2020-05-26T17:15:17,379][DEBUG][logstash.filters.grok    ][main] Adding pattern {"POSINT"=>"\\b(?:[1-9][0-9]*)\\b"}
[2020-05-26T17:15:17,380][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NONNEGINT"=>"\\b(?:[0-9]+)\\b"}
[2020-05-26T17:15:17,381][DEBUG][logstash.filters.grok    ][main] Adding pattern {"WORD"=>"\\b\\w+\\b"}
[2020-05-26T17:15:17,383][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NOTSPACE"=>"\\S+"}
[2020-05-26T17:15:17,384][DEBUG][logstash.filters.grok    ][main] Adding pattern {"SPACE"=>"\\s*"}
[2020-05-26T17:15:17,393][DEBUG][logstash.filters.grok    ][main] Adding pattern {"DATA"=>".*?"}
[2020-05-26T17:15:17,394][DEBUG][logstash.filters.grok    ][main] Adding pattern {"GREEDYDATA"=>".*"}
[2020-05-26T17:15:17,396][DEBUG][logstash.filters.grok    ][main] Adding pattern {"QUOTEDSTRING"=>"(?>(?<!\\\\)(?>\"(?>\\\\.|[^\\\\\"]+)+\"|\"\"|(?>'(?>\\\\.|[^\\\\']+)+')|''|(?>`(?>\\\\.|[^\\\\`]+)+`)|``))"}
[2020-05-26T17:15:17,398][DEBUG][logstash.filters.grok    ][main] Adding pattern {"UUID"=>"[A-Fa-f0-9]{8}-(?:[A-Fa-f0-9]{4}-){3}[A-Fa-f0-9]{12}"}
[2020-05-26T17:15:17,399][DEBUG][logstash.filters.grok    ][main] Adding pattern {"URN"=>"urn:[0-9A-Za-z][0-9A-Za-z-]{0,31}:(?:%[0-9a-fA-F]{2}|[0-9A-Za-z()+,.:=@;$_!*'/?#-])+"}
[2020-05-26T17:15:17,400][DEBUG][logstash.filters.grok    ][main] Adding pattern {"MAC"=>"(?:%{CISCOMAC}|%{WINDOWSMAC}|%{COMMONMAC})"}
[2020-05-26T17:15:17,402][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOMAC"=>"(?:(?:[A-Fa-f0-9]{4}\\.){2}[A-Fa-f0-9]{4})"}
[2020-05-26T17:15:17,411][DEBUG][logstash.filters.grok    ][main] Adding pattern {"WINDOWSMAC"=>"(?:(?:[A-Fa-f0-9]{2}-){5}[A-Fa-f0-9]{2})"}
[2020-05-26T17:15:17,412][DEBUG][logstash.filters.grok    ][main] Adding pattern {"COMMONMAC"=>"(?:(?:[A-Fa-f0-9]{2}:){5}[A-Fa-f0-9]{2})"}
[2020-05-26T17:15:17,413][DEBUG][logstash.filters.grok    ][main] Adding pattern {"IPV6"=>"((([0-9A-Fa-f]{1,4}:){7}([0-9A-Fa-f]{1,4}|:))|(([0-9A-Fa-f]{1,4}:){6}(:[0-9A-Fa-f]{1,4}|((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3})|:))|(([0-9A-Fa-f]{1,4}:){5}(((:[0-9A-Fa-f]{1,4}){1,2})|:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3})|:))|(([0-9A-Fa-f]{1,4}:){4}(((:[0-9A-Fa-f]{1,4}){1,3})|((:[0-9A-Fa-f]{1,4})?:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:))|(([0-9A-Fa-f]{1,4}:){3}(((:[0-9A-Fa-f]{1,4}){1,4})|((:[0-9A-Fa-f]{1,4}){0,2}:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:))|(([0-9A-Fa-f]{1,4}:){2}(((:[0-9A-Fa-f]{1,4}){1,5})|((:[0-9A-Fa-f]{1,4}){0,3}:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:))|(([0-9A-Fa-f]{1,4}:){1}(((:[0-9A-Fa-f]{1,4}){1,6})|((:[0-9A-Fa-f]{1,4}){0,4}:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:))|(:(((:[0-9A-Fa-f]{1,4}){1,7})|((:[0-9A-Fa-f]{1,4}){0,5}:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:)))(%.+)?"}
[2020-05-26T17:15:17,414][DEBUG][logstash.filters.grok    ][main] Adding pattern {"IPV4"=>"(?<![0-9])(?:(?:[0-1]?[0-9]{1,2}|2[0-4][0-9]|25[0-5])[.](?:[0-1]?[0-9]{1,2}|2[0-4][0-9]|25[0-5])[.](?:[0-1]?[0-9]{1,2}|2[0-4][0-9]|25[0-5])[.](?:[0-1]?[0-9]{1,2}|2[0-4][0-9]|25[0-5]))(?![0-9])"}
[2020-05-26T17:15:17,414][DEBUG][logstash.filters.grok    ][main] Adding pattern {"IP"=>"(?:%{IPV6}|%{IPV4})"}
[2020-05-26T17:15:17,415][DEBUG][logstash.filters.grok    ][main] Adding pattern {"HOSTNAME"=>"\\b(?:[0-9A-Za-z][0-9A-Za-z-]{0,62})(?:\\.(?:[0-9A-Za-z][0-9A-Za-z-]{0,62}))*(\\.?|\\b)"}
[2020-05-26T17:15:17,416][DEBUG][logstash.filters.grok    ][main] Adding pattern {"IPORHOST"=>"(?:%{IP}|%{HOSTNAME})"}
[2020-05-26T17:15:17,417][DEBUG][logstash.filters.grok    ][main] Adding pattern {"HOSTPORT"=>"%{IPORHOST}:%{POSINT}"}
[2020-05-26T17:15:17,419][DEBUG][logstash.filters.grok    ][main] Adding pattern {"PATH"=>"(?:%{UNIXPATH}|%{WINPATH})"}
[2020-05-26T17:15:17,427][DEBUG][logstash.filters.grok    ][main] Adding pattern {"UNIXPATH"=>"(/([\\w_%!$@:.,+~-]+|\\\\.)*)+"}
[2020-05-26T17:15:17,428][DEBUG][logstash.filters.grok    ][main] Adding pattern {"TTY"=>"(?:/dev/(pts|tty([pq])?)(\\w+)?/?(?:[0-9]+))"}
[2020-05-26T17:15:17,429][DEBUG][logstash.filters.grok    ][main] Adding pattern {"WINPATH"=>"(?>[A-Za-z]+:|\\\\)(?:\\\\[^\\\\?*]*)+"}
[2020-05-26T17:15:17,430][DEBUG][logstash.filters.grok    ][main] Adding pattern {"URIPROTO"=>"[A-Za-z]([A-Za-z0-9+\\-.]+)+"}
[2020-05-26T17:15:17,431][DEBUG][logstash.filters.grok    ][main] Adding pattern {"URIHOST"=>"%{IPORHOST}(?::%{POSINT:port})?"}
[2020-05-26T17:15:17,432][DEBUG][logstash.filters.grok    ][main] Adding pattern {"URIPATH"=>"(?:/[A-Za-z0-9$.+!*'(){},~:;=@#%&_\\-]*)+"}
[2020-05-26T17:15:17,433][DEBUG][logstash.filters.grok    ][main] Adding pattern {"URIPARAM"=>"\\?[A-Za-z0-9$.+!*'|(){},~@#%&/=:;_?\\-\\[\\]<>]*"}
[2020-05-26T17:15:17,434][DEBUG][logstash.filters.grok    ][main] Adding pattern {"URIPATHPARAM"=>"%{URIPATH}(?:%{URIPARAM})?"}
[2020-05-26T17:15:17,434][DEBUG][logstash.filters.grok    ][main] Adding pattern {"URI"=>"%{URIPROTO}://(?:%{USER}(?::[^@]*)?@)?(?:%{URIHOST})?(?:%{URIPATHPARAM})?"}
[2020-05-26T17:15:17,443][DEBUG][logstash.filters.grok    ][main] Adding pattern {"MONTH"=>"\\b(?:[Jj]an(?:uary|uar)?|[Ff]eb(?:ruary|ruar)?|[Mm](?:a|)?r(?:ch|z)?|[Aa]pr(?:il)?|[Mm]a(?:y|i)?|[Jj]un(?:e|i)?|[Jj]ul(?:y)?|[Aa]ug(?:ust)?|[Ss]ep(?:tember)?|[Oo](?:c|k)?t(?:ober)?|[Nn]ov(?:ember)?|[Dd]e(?:c|z)(?:ember)?)\\b"}
[2020-05-26T17:15:17,445][DEBUG][logstash.filters.grok    ][main] Adding pattern {"MONTHNUM"=>"(?:0?[1-9]|1[0-2])"}
[2020-05-26T17:15:17,446][DEBUG][logstash.filters.grok    ][main] Adding pattern {"MONTHNUM2"=>"(?:0[1-9]|1[0-2])"}
[2020-05-26T17:15:17,447][DEBUG][logstash.filters.grok    ][main] Adding pattern {"MONTHDAY"=>"(?:(?:0[1-9])|(?:[12][0-9])|(?:3[01])|[1-9])"}
[2020-05-26T17:15:17,449][DEBUG][logstash.filters.grok    ][main] Adding pattern {"DAY"=>"(?:Mon(?:day)?|Tue(?:sday)?|Wed(?:nesday)?|Thu(?:rsday)?|Fri(?:day)?|Sat(?:urday)?|Sun(?:day)?)"}
[2020-05-26T17:15:17,449][DEBUG][logstash.filters.grok    ][main] Adding pattern {"YEAR"=>"(?>\\d\\d){1,2}"}
[2020-05-26T17:15:17,450][DEBUG][logstash.filters.grok    ][main] Adding pattern {"HOUR"=>"(?:2[0123]|[01]?[0-9])"}
[2020-05-26T17:15:17,452][DEBUG][logstash.filters.grok    ][main] Adding pattern {"MINUTE"=>"(?:[0-5][0-9])"}
[2020-05-26T17:15:17,460][DEBUG][logstash.filters.grok    ][main] Adding pattern {"SECOND"=>"(?:(?:[0-5]?[0-9]|60)(?:[:.,][0-9]+)?)"}
[2020-05-26T17:15:17,461][DEBUG][logstash.filters.grok    ][main] Adding pattern {"TIME"=>"(?!<[0-9])%{HOUR}:%{MINUTE}(?::%{SECOND})(?![0-9])"}
[2020-05-26T17:15:17,462][DEBUG][logstash.filters.grok    ][main] Adding pattern {"DATE_US"=>"%{MONTHNUM}[/-]%{MONTHDAY}[/-]%{YEAR}"}
[2020-05-26T17:15:17,463][DEBUG][logstash.filters.grok    ][main] Adding pattern {"DATE_EU"=>"%{MONTHDAY}[./-]%{MONTHNUM}[./-]%{YEAR}"}
[2020-05-26T17:15:17,465][DEBUG][logstash.filters.grok    ][main] Adding pattern {"ISO8601_TIMEZONE"=>"(?:Z|[+-]%{HOUR}(?::?%{MINUTE}))"}
[2020-05-26T17:15:17,466][DEBUG][logstash.filters.grok    ][main] Adding pattern {"ISO8601_SECOND"=>"(?:%{SECOND}|60)"}
[2020-05-26T17:15:17,467][DEBUG][logstash.filters.grok    ][main] Adding pattern {"TIMESTAMP_ISO8601"=>"%{YEAR}-%{MONTHNUM}-%{MONTHDAY}[T ]%{HOUR}:?%{MINUTE}(?::?%{SECOND})?%{ISO8601_TIMEZONE}?"}
[2020-05-26T17:15:17,469][DEBUG][logstash.filters.grok    ][main] Adding pattern {"DATE"=>"%{DATE_US}|%{DATE_EU}"}
[2020-05-26T17:15:17,477][DEBUG][logstash.filters.grok    ][main] Adding pattern {"DATESTAMP"=>"%{DATE}[- ]%{TIME}"}
[2020-05-26T17:15:17,478][DEBUG][logstash.filters.grok    ][main] Adding pattern {"TZ"=>"(?:[APMCE][SD]T|UTC)"}
[2020-05-26T17:15:17,479][DEBUG][logstash.filters.grok    ][main] Adding pattern {"DATESTAMP_RFC822"=>"%{DAY} %{MONTH} %{MONTHDAY} %{YEAR} %{TIME} %{TZ}"}
[2020-05-26T17:15:17,480][DEBUG][logstash.filters.grok    ][main] Adding pattern {"DATESTAMP_RFC2822"=>"%{DAY}, %{MONTHDAY} %{MONTH} %{YEAR} %{TIME} %{ISO8601_TIMEZONE}"}
[2020-05-26T17:15:17,481][DEBUG][logstash.filters.grok    ][main] Adding pattern {"DATESTAMP_OTHER"=>"%{DAY} %{MONTH} %{MONTHDAY} %{TIME} %{TZ} %{YEAR}"}
[2020-05-26T17:15:17,482][DEBUG][logstash.filters.grok    ][main] Adding pattern {"DATESTAMP_EVENTLOG"=>"%{YEAR}%{MONTHNUM2}%{MONTHDAY}%{HOUR}%{MINUTE}%{SECOND}"}
[2020-05-26T17:15:17,484][DEBUG][logstash.filters.grok    ][main] Adding pattern {"SYSLOGTIMESTAMP"=>"%{MONTH} +%{MONTHDAY} %{TIME}"}
[2020-05-26T17:15:17,493][DEBUG][logstash.filters.grok    ][main] Adding pattern {"PROG"=>"[\\x21-\\x5a\\x5c\\x5e-\\x7e]+"}
[2020-05-26T17:15:17,494][DEBUG][logstash.filters.grok    ][main] Adding pattern {"SYSLOGPROG"=>"%{PROG:program}(?:\\[%{POSINT:pid}\\])?"}
[2020-05-26T17:15:17,495][DEBUG][logstash.filters.grok    ][main] Adding pattern {"SYSLOGHOST"=>"%{IPORHOST}"}
[2020-05-26T17:15:17,496][DEBUG][logstash.filters.grok    ][main] Adding pattern {"SYSLOGFACILITY"=>"<%{NONNEGINT:facility}.%{NONNEGINT:priority}>"}
[2020-05-26T17:15:17,497][DEBUG][logstash.filters.grok    ][main] Adding pattern {"HTTPDATE"=>"%{MONTHDAY}/%{MONTH}/%{YEAR}:%{TIME} %{INT}"}
[2020-05-26T17:15:17,528][DEBUG][logstash.filters.grok    ][main] Adding pattern {"QS"=>"%{QUOTEDSTRING}"}
[2020-05-26T17:15:17,529][DEBUG][logstash.filters.grok    ][main] Adding pattern {"SYSLOGBASE"=>"%{SYSLOGTIMESTAMP:timestamp} (?:%{SYSLOGFACILITY} )?%{SYSLOGHOST:logsource} %{SYSLOGPROG}:"}
[2020-05-26T17:15:17,531][DEBUG][logstash.filters.grok    ][main] Adding pattern {"LOGLEVEL"=>"([Aa]lert|ALERT|[Tt]race|TRACE|[Dd]ebug|DEBUG|[Nn]otice|NOTICE|[Ii]nfo|INFO|[Ww]arn?(?:ing)?|WARN?(?:ING)?|[Ee]rr?(?:or)?|ERR?(?:OR)?|[Cc]rit?(?:ical)?|CRIT?(?:ICAL)?|[Ff]atal|FATAL|[Ss]evere|SEVERE|EMERG(?:ENCY)?|[Ee]merg(?:ency)?)"}
[2020-05-26T17:15:17,533][DEBUG][logstash.filters.grok    ][main] Adding pattern {"HAPROXYTIME"=>"(?!<[0-9])%{HOUR:haproxy_hour}:%{MINUTE:haproxy_minute}(?::%{SECOND:haproxy_second})(?![0-9])"}
[2020-05-26T17:15:17,534][DEBUG][logstash.filters.grok    ][main] Adding pattern {"HAPROXYDATE"=>"%{MONTHDAY:haproxy_monthday}/%{MONTH:haproxy_month}/%{YEAR:haproxy_year}:%{HAPROXYTIME:haproxy_time}.%{INT:haproxy_milliseconds}"}
[2020-05-26T17:15:17,535][DEBUG][logstash.filters.grok    ][main] Adding pattern {"HAPROXYCAPTUREDREQUESTHEADERS"=>"%{DATA:captured_request_headers}"}
[2020-05-26T17:15:17,537][DEBUG][logstash.filters.grok    ][main] Adding pattern {"HAPROXYCAPTUREDRESPONSEHEADERS"=>"%{DATA:captured_response_headers}"}
[2020-05-26T17:15:17,546][DEBUG][logstash.filters.grok    ][main] Adding pattern {"HAPROXYHTTPBASE"=>"%{IP:client_ip}:%{INT:client_port} \\[%{HAPROXYDATE:accept_date}\\] %{NOTSPACE:frontend_name} %{NOTSPACE:backend_name}/%{NOTSPACE:server_name} %{INT:time_request}/%{INT:time_queue}/%{INT:time_backend_connect}/%{INT:time_backend_response}/%{NOTSPACE:time_duration} %{INT:http_status_code} %{NOTSPACE:bytes_read} %{DATA:captured_request_cookie} %{DATA:captured_response_cookie} %{NOTSPACE:termination_state} %{INT:actconn}/%{INT:feconn}/%{INT:beconn}/%{INT:srvconn}/%{NOTSPACE:retries} %{INT:srv_queue}/%{INT:backend_queue} (\\{%{HAPROXYCAPTUREDREQUESTHEADERS}\\})?( )?(\\{%{HAPROXYCAPTUREDRESPONSEHEADERS}\\})?( )?\"(<BADREQ>|(%{WORD:http_verb} (%{URIPROTO:http_proto}://)?(?:%{USER:http_user}(?::[^@]*)?@)?(?:%{URIHOST:http_host})?(?:%{URIPATHPARAM:http_request})?( HTTP/%{NUMBER:http_version})?))?\""}
[2020-05-26T17:15:17,548][DEBUG][logstash.filters.grok    ][main] Adding pattern {"HAPROXYHTTP"=>"(?:%{SYSLOGTIMESTAMP:syslog_timestamp}|%{TIMESTAMP_ISO8601:timestamp8601}) %{IPORHOST:syslog_server} %{SYSLOGPROG}: %{HAPROXYHTTPBASE}"}
[2020-05-26T17:15:17,549][DEBUG][logstash.filters.grok    ][main] Adding pattern {"HAPROXYTCP"=>"(?:%{SYSLOGTIMESTAMP:syslog_timestamp}|%{TIMESTAMP_ISO8601:timestamp8601}) %{IPORHOST:syslog_server} %{SYSLOGPROG}: %{IP:client_ip}:%{INT:client_port} \\[%{HAPROXYDATE:accept_date}\\] %{NOTSPACE:frontend_name} %{NOTSPACE:backend_name}/%{NOTSPACE:server_name} %{INT:time_queue}/%{INT:time_backend_connect}/%{NOTSPACE:time_duration} %{NOTSPACE:bytes_read} %{NOTSPACE:termination_state} %{INT:actconn}/%{INT:feconn}/%{INT:beconn}/%{INT:srvconn}/%{NOTSPACE:retries} %{INT:srv_queue}/%{INT:backend_queue}"}
[2020-05-26T17:15:17,550][DEBUG][logstash.filters.grok    ][main] Adding pattern {"HTTPDUSER"=>"%{EMAILADDRESS}|%{USER}"}
[2020-05-26T17:15:17,552][DEBUG][logstash.filters.grok    ][main] Adding pattern {"HTTPDERROR_DATE"=>"%{DAY} %{MONTH} %{MONTHDAY} %{TIME} %{YEAR}"}
[2020-05-26T17:15:17,553][DEBUG][logstash.filters.grok    ][main] Adding pattern {"HTTPD_COMMONLOG"=>"%{IPORHOST:clientip} %{HTTPDUSER:ident} %{HTTPDUSER:auth} \\[%{HTTPDATE:timestamp}\\] \"(?:%{WORD:verb} %{NOTSPACE:request}(?: HTTP/%{NUMBER:httpversion})?|%{DATA:rawrequest})\" %{NUMBER:response} (?:%{NUMBER:bytes}|-)"}
[2020-05-26T17:15:17,562][DEBUG][logstash.filters.grok    ][main] Adding pattern {"HTTPD_COMBINEDLOG"=>"%{HTTPD_COMMONLOG} %{QS:referrer} %{QS:agent}"}
[2020-05-26T17:15:17,563][DEBUG][logstash.filters.grok    ][main] Adding pattern {"HTTPD20_ERRORLOG"=>"\\[%{HTTPDERROR_DATE:timestamp}\\] \\[%{LOGLEVEL:loglevel}\\] (?:\\[client %{IPORHOST:clientip}\\] ){0,1}%{GREEDYDATA:message}"}
[2020-05-26T17:15:17,564][DEBUG][logstash.filters.grok    ][main] Adding pattern {"HTTPD24_ERRORLOG"=>"\\[%{HTTPDERROR_DATE:timestamp}\\] \\[%{WORD:module}:%{LOGLEVEL:loglevel}\\] \\[pid %{POSINT:pid}(:tid %{NUMBER:tid})?\\]( \\(%{POSINT:proxy_errorcode}\\)%{DATA:proxy_message}:)?( \\[client %{IPORHOST:clientip}:%{POSINT:clientport}\\])?( %{DATA:errorcode}:)? %{GREEDYDATA:message}"}
[2020-05-26T17:15:17,566][DEBUG][logstash.filters.grok    ][main] Adding pattern {"HTTPD_ERRORLOG"=>"%{HTTPD20_ERRORLOG}|%{HTTPD24_ERRORLOG}"}
[2020-05-26T17:15:17,567][DEBUG][logstash.filters.grok    ][main] Adding pattern {"COMMONAPACHELOG"=>"%{HTTPD_COMMONLOG}"}
[2020-05-26T17:15:17,568][DEBUG][logstash.filters.grok    ][main] Adding pattern {"COMBINEDAPACHELOG"=>"%{HTTPD_COMBINEDLOG}"}
[2020-05-26T17:15:17,570][DEBUG][logstash.filters.grok    ][main] Adding pattern {"JAVACLASS"=>"(?:[a-zA-Z$_][a-zA-Z$_0-9]*\\.)*[a-zA-Z$_][a-zA-Z$_0-9]*"}
[2020-05-26T17:15:17,577][DEBUG][logstash.filters.grok    ][main] Adding pattern {"JAVAFILE"=>"(?:[A-Za-z0-9_. -]+)"}
[2020-05-26T17:15:17,578][DEBUG][logstash.filters.grok    ][main] Adding pattern {"JAVAMETHOD"=>"(?:(<(?:cl)?init>)|[a-zA-Z$_][a-zA-Z$_0-9]*)"}
[2020-05-26T17:15:17,579][DEBUG][logstash.filters.grok    ][main] Adding pattern {"JAVASTACKTRACEPART"=>"%{SPACE}at %{JAVACLASS:class}\\.%{JAVAMETHOD:method}\\(%{JAVAFILE:file}(?::%{NUMBER:line})?\\)"}
[2020-05-26T17:15:17,580][DEBUG][logstash.filters.grok    ][main] Adding pattern {"JAVATHREAD"=>"(?:[A-Z]{2}-Processor[\\d]+)"}
[2020-05-26T17:15:17,581][DEBUG][logstash.filters.grok    ][main] Adding pattern {"JAVACLASS"=>"(?:[a-zA-Z0-9-]+\\.)+[A-Za-z0-9$]+"}
[2020-05-26T17:15:17,582][DEBUG][logstash.filters.grok    ][main] Adding pattern {"JAVAFILE"=>"(?:[A-Za-z0-9_.-]+)"}
[2020-05-26T17:15:17,582][DEBUG][logstash.filters.grok    ][main] Adding pattern {"JAVALOGMESSAGE"=>"(.*)"}
[2020-05-26T17:15:17,583][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CATALINA_DATESTAMP"=>"%{MONTH} %{MONTHDAY}, 20%{YEAR} %{HOUR}:?%{MINUTE}(?::?%{SECOND}) (?:AM|PM)"}
[2020-05-26T17:15:17,584][DEBUG][logstash.filters.grok    ][main] Adding pattern {"TOMCAT_DATESTAMP"=>"20%{YEAR}-%{MONTHNUM}-%{MONTHDAY} %{HOUR}:?%{MINUTE}(?::?%{SECOND}) %{ISO8601_TIMEZONE}"}
[2020-05-26T17:15:17,585][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CATALINALOG"=>"%{CATALINA_DATESTAMP:timestamp} %{JAVACLASS:class} %{JAVALOGMESSAGE:logmessage}"}
[2020-05-26T17:15:17,593][DEBUG][logstash.filters.grok    ][main] Adding pattern {"TOMCATLOG"=>"%{TOMCAT_DATESTAMP:timestamp} \\| %{LOGLEVEL:level} \\| %{JAVACLASS:class} - %{JAVALOGMESSAGE:logmessage}"}
[2020-05-26T17:15:17,595][DEBUG][logstash.filters.grok    ][main] Adding pattern {"RT_FLOW_EVENT"=>"(RT_FLOW_SESSION_CREATE|RT_FLOW_SESSION_CLOSE|RT_FLOW_SESSION_DENY)"}
[2020-05-26T17:15:17,596][DEBUG][logstash.filters.grok    ][main] Adding pattern {"RT_FLOW1"=>"%{RT_FLOW_EVENT:event}: %{GREEDYDATA:close-reason}: %{IP:src-ip}/%{INT:src-port}->%{IP:dst-ip}/%{INT:dst-port} %{DATA:service} %{IP:nat-src-ip}/%{INT:nat-src-port}->%{IP:nat-dst-ip}/%{INT:nat-dst-port} %{DATA:src-nat-rule-name} %{DATA:dst-nat-rule-name} %{INT:protocol-id} %{DATA:policy-name} %{DATA:from-zone} %{DATA:to-zone} %{INT:session-id} \\d+\\(%{DATA:sent}\\) \\d+\\(%{DATA:received}\\) %{INT:elapsed-time} .*"}
[2020-05-26T17:15:17,597][DEBUG][logstash.filters.grok    ][main] Adding pattern {"RT_FLOW2"=>"%{RT_FLOW_EVENT:event}: session created %{IP:src-ip}/%{INT:src-port}->%{IP:dst-ip}/%{INT:dst-port} %{DATA:service} %{IP:nat-src-ip}/%{INT:nat-src-port}->%{IP:nat-dst-ip}/%{INT:nat-dst-port} %{DATA:src-nat-rule-name} %{DATA:dst-nat-rule-name} %{INT:protocol-id} %{DATA:policy-name} %{DATA:from-zone} %{DATA:to-zone} %{INT:session-id} .*"}
[2020-05-26T17:15:17,598][DEBUG][logstash.filters.grok    ][main] Adding pattern {"RT_FLOW3"=>"%{RT_FLOW_EVENT:event}: session denied %{IP:src-ip}/%{INT:src-port}->%{IP:dst-ip}/%{INT:dst-port} %{DATA:service} %{INT:protocol-id}\\(\\d\\) %{DATA:policy-name} %{DATA:from-zone} %{DATA:to-zone} .*"}
[2020-05-26T17:15:17,600][DEBUG][logstash.filters.grok    ][main] Adding pattern {"SYSLOG5424PRINTASCII"=>"[!-~]+"}
[2020-05-26T17:15:17,602][DEBUG][logstash.filters.grok    ][main] Adding pattern {"SYSLOGBASE2"=>"(?:%{SYSLOGTIMESTAMP:timestamp}|%{TIMESTAMP_ISO8601:timestamp8601}) (?:%{SYSLOGFACILITY} )?%{SYSLOGHOST:logsource}+(?: %{SYSLOGPROG}:|)"}
[2020-05-26T17:15:17,611][DEBUG][logstash.filters.grok    ][main] Adding pattern {"SYSLOGPAMSESSION"=>"%{SYSLOGBASE} (?=%{GREEDYDATA:message})%{WORD:pam_module}\\(%{DATA:pam_caller}\\): session %{WORD:pam_session_state} for user %{USERNAME:username}(?: by %{GREEDYDATA:pam_by})?"}
[2020-05-26T17:15:17,612][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CRON_ACTION"=>"[A-Z ]+"}
[2020-05-26T17:15:17,613][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CRONLOG"=>"%{SYSLOGBASE} \\(%{USER:user}\\) %{CRON_ACTION:action} \\(%{DATA:message}\\)"}
[2020-05-26T17:15:17,615][DEBUG][logstash.filters.grok    ][main] Adding pattern {"SYSLOGLINE"=>"%{SYSLOGBASE2} %{GREEDYDATA:message}"}
[2020-05-26T17:15:17,616][DEBUG][logstash.filters.grok    ][main] Adding pattern {"SYSLOG5424PRI"=>"<%{NONNEGINT:syslog5424_pri}>"}
[2020-05-26T17:15:17,617][DEBUG][logstash.filters.grok    ][main] Adding pattern {"SYSLOG5424SD"=>"\\[%{DATA}\\]+"}
[2020-05-26T17:15:17,618][DEBUG][logstash.filters.grok    ][main] Adding pattern {"SYSLOG5424BASE"=>"%{SYSLOG5424PRI}%{NONNEGINT:syslog5424_ver} +(?:%{TIMESTAMP_ISO8601:syslog5424_ts}|-) +(?:%{IPORHOST:syslog5424_host}|-) +(-|%{SYSLOG5424PRINTASCII:syslog5424_app}) +(-|%{SYSLOG5424PRINTASCII:syslog5424_proc}) +(-|%{SYSLOG5424PRINTASCII:syslog5424_msgid}) +(?:%{SYSLOG5424SD:syslog5424_sd}|-|)"}
[2020-05-26T17:15:17,626][DEBUG][logstash.filters.grok    ][main] Adding pattern {"SYSLOG5424LINE"=>"%{SYSLOG5424BASE} +%{GREEDYDATA:syslog5424_msg}"}
[2020-05-26T17:15:17,627][DEBUG][logstash.filters.grok    ][main] Adding pattern {"MAVEN_VERSION"=>"(?:(\\d+)\\.)?(?:(\\d+)\\.)?(\\*|\\d+)(?:[.-](RELEASE|SNAPSHOT))?"}
[2020-05-26T17:15:17,629][DEBUG][logstash.filters.grok    ][main] Adding pattern {"MCOLLECTIVEAUDIT"=>"%{TIMESTAMP_ISO8601:timestamp}:"}
[2020-05-26T17:15:17,630][DEBUG][logstash.filters.grok    ][main] Adding pattern {"MCOLLECTIVE"=>"., \\[%{TIMESTAMP_ISO8601:timestamp} #%{POSINT:pid}\\]%{SPACE}%{LOGLEVEL:event_level}"}
[2020-05-26T17:15:17,631][DEBUG][logstash.filters.grok    ][main] Adding pattern {"MCOLLECTIVEAUDIT"=>"%{TIMESTAMP_ISO8601:timestamp}:"}
[2020-05-26T17:15:17,633][DEBUG][logstash.filters.grok    ][main] Adding pattern {"MONGO_LOG"=>"%{SYSLOGTIMESTAMP:timestamp} \\[%{WORD:component}\\] %{GREEDYDATA:message}"}
[2020-05-26T17:15:17,634][DEBUG][logstash.filters.grok    ][main] Adding pattern {"MONGO_QUERY"=>"\\{ (?<={ ).*(?= } ntoreturn:) \\}"}
[2020-05-26T17:15:17,643][DEBUG][logstash.filters.grok    ][main] Adding pattern {"MONGO_SLOWQUERY"=>"%{WORD} %{MONGO_WORDDASH:database}\\.%{MONGO_WORDDASH:collection} %{WORD}: %{MONGO_QUERY:query} %{WORD}:%{NONNEGINT:ntoreturn} %{WORD}:%{NONNEGINT:ntoskip} %{WORD}:%{NONNEGINT:nscanned}.*nreturned:%{NONNEGINT:nreturned}..+ (?<duration>[0-9]+)ms"}
[2020-05-26T17:15:17,644][DEBUG][logstash.filters.grok    ][main] Adding pattern {"MONGO_WORDDASH"=>"\\b[\\w-]+\\b"}
[2020-05-26T17:15:17,645][DEBUG][logstash.filters.grok    ][main] Adding pattern {"MONGO3_SEVERITY"=>"\\w"}
[2020-05-26T17:15:17,646][DEBUG][logstash.filters.grok    ][main] Adding pattern {"MONGO3_COMPONENT"=>"%{WORD}|-"}
[2020-05-26T17:15:17,646][DEBUG][logstash.filters.grok    ][main] Adding pattern {"MONGO3_LOG"=>"%{TIMESTAMP_ISO8601:timestamp} %{MONGO3_SEVERITY:severity} %{MONGO3_COMPONENT:component}%{SPACE}(?:\\[%{DATA:context}\\])? %{GREEDYDATA:message}"}
[2020-05-26T17:15:17,648][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOSTIME"=>"\\[%{NUMBER:nagios_epoch}\\]"}
[2020-05-26T17:15:17,649][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_TYPE_CURRENT_SERVICE_STATE"=>"CURRENT SERVICE STATE"}
[2020-05-26T17:15:17,650][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_TYPE_CURRENT_HOST_STATE"=>"CURRENT HOST STATE"}
[2020-05-26T17:15:17,650][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_TYPE_SERVICE_NOTIFICATION"=>"SERVICE NOTIFICATION"}
[2020-05-26T17:15:17,651][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_TYPE_HOST_NOTIFICATION"=>"HOST NOTIFICATION"}
[2020-05-26T17:15:17,660][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_TYPE_SERVICE_ALERT"=>"SERVICE ALERT"}
[2020-05-26T17:15:17,661][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_TYPE_HOST_ALERT"=>"HOST ALERT"}
[2020-05-26T17:15:17,662][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_TYPE_SERVICE_FLAPPING_ALERT"=>"SERVICE FLAPPING ALERT"}
[2020-05-26T17:15:17,663][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_TYPE_HOST_FLAPPING_ALERT"=>"HOST FLAPPING ALERT"}
[2020-05-26T17:15:17,664][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_TYPE_SERVICE_DOWNTIME_ALERT"=>"SERVICE DOWNTIME ALERT"}
[2020-05-26T17:15:17,665][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_TYPE_HOST_DOWNTIME_ALERT"=>"HOST DOWNTIME ALERT"}
[2020-05-26T17:15:17,666][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_TYPE_PASSIVE_SERVICE_CHECK"=>"PASSIVE SERVICE CHECK"}
[2020-05-26T17:15:17,667][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_TYPE_PASSIVE_HOST_CHECK"=>"PASSIVE HOST CHECK"}
[2020-05-26T17:15:17,676][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_TYPE_SERVICE_EVENT_HANDLER"=>"SERVICE EVENT HANDLER"}
[2020-05-26T17:15:17,676][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_TYPE_HOST_EVENT_HANDLER"=>"HOST EVENT HANDLER"}
[2020-05-26T17:15:17,677][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_TYPE_EXTERNAL_COMMAND"=>"EXTERNAL COMMAND"}
[2020-05-26T17:15:17,678][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_TYPE_TIMEPERIOD_TRANSITION"=>"TIMEPERIOD TRANSITION"}
[2020-05-26T17:15:17,678][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_EC_DISABLE_SVC_CHECK"=>"DISABLE_SVC_CHECK"}
[2020-05-26T17:15:17,679][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_EC_ENABLE_SVC_CHECK"=>"ENABLE_SVC_CHECK"}
[2020-05-26T17:15:17,680][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_EC_DISABLE_HOST_CHECK"=>"DISABLE_HOST_CHECK"}
[2020-05-26T17:15:17,680][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_EC_ENABLE_HOST_CHECK"=>"ENABLE_HOST_CHECK"}
[2020-05-26T17:15:17,681][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_EC_PROCESS_SERVICE_CHECK_RESULT"=>"PROCESS_SERVICE_CHECK_RESULT"}
[2020-05-26T17:15:17,682][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_EC_PROCESS_HOST_CHECK_RESULT"=>"PROCESS_HOST_CHECK_RESULT"}
[2020-05-26T17:15:17,682][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_EC_SCHEDULE_SERVICE_DOWNTIME"=>"SCHEDULE_SERVICE_DOWNTIME"}
[2020-05-26T17:15:17,683][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_EC_SCHEDULE_HOST_DOWNTIME"=>"SCHEDULE_HOST_DOWNTIME"}
[2020-05-26T17:15:17,684][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_EC_DISABLE_HOST_SVC_NOTIFICATIONS"=>"DISABLE_HOST_SVC_NOTIFICATIONS"}
[2020-05-26T17:15:17,692][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_EC_ENABLE_HOST_SVC_NOTIFICATIONS"=>"ENABLE_HOST_SVC_NOTIFICATIONS"}
[2020-05-26T17:15:17,693][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_EC_DISABLE_HOST_NOTIFICATIONS"=>"DISABLE_HOST_NOTIFICATIONS"}
[2020-05-26T17:15:17,694][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_EC_ENABLE_HOST_NOTIFICATIONS"=>"ENABLE_HOST_NOTIFICATIONS"}
[2020-05-26T17:15:17,695][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_EC_DISABLE_SVC_NOTIFICATIONS"=>"DISABLE_SVC_NOTIFICATIONS"}
[2020-05-26T17:15:17,695][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_EC_ENABLE_SVC_NOTIFICATIONS"=>"ENABLE_SVC_NOTIFICATIONS"}
[2020-05-26T17:15:17,696][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_WARNING"=>"Warning:%{SPACE}%{GREEDYDATA:nagios_message}"}
[2020-05-26T17:15:17,697][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_CURRENT_SERVICE_STATE"=>"%{NAGIOS_TYPE_CURRENT_SERVICE_STATE:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_service};%{DATA:nagios_state};%{DATA:nagios_statetype};%{DATA:nagios_statecode};%{GREEDYDATA:nagios_message}"}
[2020-05-26T17:15:17,697][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_CURRENT_HOST_STATE"=>"%{NAGIOS_TYPE_CURRENT_HOST_STATE:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_state};%{DATA:nagios_statetype};%{DATA:nagios_statecode};%{GREEDYDATA:nagios_message}"}
[2020-05-26T17:15:17,698][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_SERVICE_NOTIFICATION"=>"%{NAGIOS_TYPE_SERVICE_NOTIFICATION:nagios_type}: %{DATA:nagios_notifyname};%{DATA:nagios_hostname};%{DATA:nagios_service};%{DATA:nagios_state};%{DATA:nagios_contact};%{GREEDYDATA:nagios_message}"}
[2020-05-26T17:15:17,699][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_HOST_NOTIFICATION"=>"%{NAGIOS_TYPE_HOST_NOTIFICATION:nagios_type}: %{DATA:nagios_notifyname};%{DATA:nagios_hostname};%{DATA:nagios_state};%{DATA:nagios_contact};%{GREEDYDATA:nagios_message}"}
[2020-05-26T17:15:17,700][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_SERVICE_ALERT"=>"%{NAGIOS_TYPE_SERVICE_ALERT:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_service};%{DATA:nagios_state};%{DATA:nagios_statelevel};%{NUMBER:nagios_attempt};%{GREEDYDATA:nagios_message}"}
[2020-05-26T17:15:17,701][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_HOST_ALERT"=>"%{NAGIOS_TYPE_HOST_ALERT:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_state};%{DATA:nagios_statelevel};%{NUMBER:nagios_attempt};%{GREEDYDATA:nagios_message}"}
[2020-05-26T17:15:17,710][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_SERVICE_FLAPPING_ALERT"=>"%{NAGIOS_TYPE_SERVICE_FLAPPING_ALERT:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_service};%{DATA:nagios_state};%{GREEDYDATA:nagios_message}"}
[2020-05-26T17:15:17,711][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_HOST_FLAPPING_ALERT"=>"%{NAGIOS_TYPE_HOST_FLAPPING_ALERT:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_state};%{GREEDYDATA:nagios_message}"}
[2020-05-26T17:15:17,712][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_SERVICE_DOWNTIME_ALERT"=>"%{NAGIOS_TYPE_SERVICE_DOWNTIME_ALERT:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_service};%{DATA:nagios_state};%{GREEDYDATA:nagios_comment}"}
[2020-05-26T17:15:17,713][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_HOST_DOWNTIME_ALERT"=>"%{NAGIOS_TYPE_HOST_DOWNTIME_ALERT:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_state};%{GREEDYDATA:nagios_comment}"}
[2020-05-26T17:15:17,714][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_PASSIVE_SERVICE_CHECK"=>"%{NAGIOS_TYPE_PASSIVE_SERVICE_CHECK:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_service};%{DATA:nagios_state};%{GREEDYDATA:nagios_comment}"}
[2020-05-26T17:15:17,715][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_PASSIVE_HOST_CHECK"=>"%{NAGIOS_TYPE_PASSIVE_HOST_CHECK:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_state};%{GREEDYDATA:nagios_comment}"}
[2020-05-26T17:15:17,716][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_SERVICE_EVENT_HANDLER"=>"%{NAGIOS_TYPE_SERVICE_EVENT_HANDLER:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_service};%{DATA:nagios_state};%{DATA:nagios_statelevel};%{DATA:nagios_event_handler_name}"}
[2020-05-26T17:15:17,717][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_HOST_EVENT_HANDLER"=>"%{NAGIOS_TYPE_HOST_EVENT_HANDLER:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_state};%{DATA:nagios_statelevel};%{DATA:nagios_event_handler_name}"}
[2020-05-26T17:15:17,718][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_TIMEPERIOD_TRANSITION"=>"%{NAGIOS_TYPE_TIMEPERIOD_TRANSITION:nagios_type}: %{DATA:nagios_service};%{DATA:nagios_unknown1};%{DATA:nagios_unknown2}"}
[2020-05-26T17:15:17,726][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_EC_LINE_DISABLE_SVC_CHECK"=>"%{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_DISABLE_SVC_CHECK:nagios_command};%{DATA:nagios_hostname};%{DATA:nagios_service}"}
[2020-05-26T17:15:17,727][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_EC_LINE_DISABLE_HOST_CHECK"=>"%{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_DISABLE_HOST_CHECK:nagios_command};%{DATA:nagios_hostname}"}
[2020-05-26T17:15:17,728][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_EC_LINE_ENABLE_SVC_CHECK"=>"%{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_ENABLE_SVC_CHECK:nagios_command};%{DATA:nagios_hostname};%{DATA:nagios_service}"}
[2020-05-26T17:15:17,729][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_EC_LINE_ENABLE_HOST_CHECK"=>"%{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_ENABLE_HOST_CHECK:nagios_command};%{DATA:nagios_hostname}"}
[2020-05-26T17:15:17,731][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_EC_LINE_PROCESS_SERVICE_CHECK_RESULT"=>"%{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_PROCESS_SERVICE_CHECK_RESULT:nagios_command};%{DATA:nagios_hostname};%{DATA:nagios_service};%{DATA:nagios_state};%{GREEDYDATA:nagios_check_result}"}
[2020-05-26T17:15:17,732][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_EC_LINE_PROCESS_HOST_CHECK_RESULT"=>"%{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_PROCESS_HOST_CHECK_RESULT:nagios_command};%{DATA:nagios_hostname};%{DATA:nagios_state};%{GREEDYDATA:nagios_check_result}"}
[2020-05-26T17:15:17,733][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_EC_LINE_DISABLE_HOST_SVC_NOTIFICATIONS"=>"%{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_DISABLE_HOST_SVC_NOTIFICATIONS:nagios_command};%{GREEDYDATA:nagios_hostname}"}
[2020-05-26T17:15:17,734][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_EC_LINE_DISABLE_HOST_NOTIFICATIONS"=>"%{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_DISABLE_HOST_NOTIFICATIONS:nagios_command};%{GREEDYDATA:nagios_hostname}"}
[2020-05-26T17:15:17,743][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_EC_LINE_DISABLE_SVC_NOTIFICATIONS"=>"%{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_DISABLE_SVC_NOTIFICATIONS:nagios_command};%{DATA:nagios_hostname};%{GREEDYDATA:nagios_service}"}
[2020-05-26T17:15:17,744][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_EC_LINE_ENABLE_HOST_SVC_NOTIFICATIONS"=>"%{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_ENABLE_HOST_SVC_NOTIFICATIONS:nagios_command};%{GREEDYDATA:nagios_hostname}"}
[2020-05-26T17:15:17,745][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_EC_LINE_ENABLE_HOST_NOTIFICATIONS"=>"%{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_ENABLE_HOST_NOTIFICATIONS:nagios_command};%{GREEDYDATA:nagios_hostname}"}
[2020-05-26T17:15:17,746][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_EC_LINE_ENABLE_SVC_NOTIFICATIONS"=>"%{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_ENABLE_SVC_NOTIFICATIONS:nagios_command};%{DATA:nagios_hostname};%{GREEDYDATA:nagios_service}"}
[2020-05-26T17:15:17,747][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_EC_LINE_SCHEDULE_HOST_DOWNTIME"=>"%{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_SCHEDULE_HOST_DOWNTIME:nagios_command};%{DATA:nagios_hostname};%{NUMBER:nagios_start_time};%{NUMBER:nagios_end_time};%{NUMBER:nagios_fixed};%{NUMBER:nagios_trigger_id};%{NUMBER:nagios_duration};%{DATA:author};%{DATA:comment}"}
[2020-05-26T17:15:17,748][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOSLOGLINE"=>"%{NAGIOSTIME} (?:%{NAGIOS_WARNING}|%{NAGIOS_CURRENT_SERVICE_STATE}|%{NAGIOS_CURRENT_HOST_STATE}|%{NAGIOS_SERVICE_NOTIFICATION}|%{NAGIOS_HOST_NOTIFICATION}|%{NAGIOS_SERVICE_ALERT}|%{NAGIOS_HOST_ALERT}|%{NAGIOS_SERVICE_FLAPPING_ALERT}|%{NAGIOS_HOST_FLAPPING_ALERT}|%{NAGIOS_SERVICE_DOWNTIME_ALERT}|%{NAGIOS_HOST_DOWNTIME_ALERT}|%{NAGIOS_PASSIVE_SERVICE_CHECK}|%{NAGIOS_PASSIVE_HOST_CHECK}|%{NAGIOS_SERVICE_EVENT_HANDLER}|%{NAGIOS_HOST_EVENT_HANDLER}|%{NAGIOS_TIMEPERIOD_TRANSITION}|%{NAGIOS_EC_LINE_DISABLE_SVC_CHECK}|%{NAGIOS_EC_LINE_ENABLE_SVC_CHECK}|%{NAGIOS_EC_LINE_DISABLE_HOST_CHECK}|%{NAGIOS_EC_LINE_ENABLE_HOST_CHECK}|%{NAGIOS_EC_LINE_PROCESS_HOST_CHECK_RESULT}|%{NAGIOS_EC_LINE_PROCESS_SERVICE_CHECK_RESULT}|%{NAGIOS_EC_LINE_SCHEDULE_HOST_DOWNTIME}|%{NAGIOS_EC_LINE_DISABLE_HOST_SVC_NOTIFICATIONS}|%{NAGIOS_EC_LINE_ENABLE_HOST_SVC_NOTIFICATIONS}|%{NAGIOS_EC_LINE_DISABLE_HOST_NOTIFICATIONS}|%{NAGIOS_EC_LINE_ENABLE_HOST_NOTIFICATIONS}|%{NAGIOS_EC_LINE_DISABLE_SVC_NOTIFICATIONS}|%{NAGIOS_EC_LINE_ENABLE_SVC_NOTIFICATIONS})"}
[2020-05-26T17:15:17,749][DEBUG][logstash.filters.grok    ][main] Adding pattern {"POSTGRESQL"=>"%{DATESTAMP:timestamp} %{TZ} %{DATA:user_id} %{GREEDYDATA:connection_id} %{POSINT:pid}"}
[2020-05-26T17:15:17,751][DEBUG][logstash.filters.grok    ][main] Adding pattern {"RUUID"=>"\\h{32}"}
[2020-05-26T17:15:17,760][DEBUG][logstash.filters.grok    ][main] Adding pattern {"RCONTROLLER"=>"(?<controller>[^#]+)#(?<action>\\w+)"}
[2020-05-26T17:15:17,761][DEBUG][logstash.filters.grok    ][main] Adding pattern {"RAILS3HEAD"=>"(?m)Started %{WORD:verb} \"%{URIPATHPARAM:request}\" for %{IPORHOST:clientip} at (?<timestamp>%{YEAR}-%{MONTHNUM}-%{MONTHDAY} %{HOUR}:%{MINUTE}:%{SECOND} %{ISO8601_TIMEZONE})"}
[2020-05-26T17:15:17,762][DEBUG][logstash.filters.grok    ][main] Adding pattern {"RPROCESSING"=>"\\W*Processing by %{RCONTROLLER} as (?<format>\\S+)(?:\\W*Parameters: {%{DATA:params}}\\W*)?"}
[2020-05-26T17:15:17,764][DEBUG][logstash.filters.grok    ][main] Adding pattern {"RAILS3FOOT"=>"Completed %{NUMBER:response}%{DATA} in %{NUMBER:totalms}ms %{RAILS3PROFILE}%{GREEDYDATA}"}
[2020-05-26T17:15:17,765][DEBUG][logstash.filters.grok    ][main] Adding pattern {"RAILS3PROFILE"=>"(?:\\(Views: %{NUMBER:viewms}ms \\| ActiveRecord: %{NUMBER:activerecordms}ms|\\(ActiveRecord: %{NUMBER:activerecordms}ms)?"}
[2020-05-26T17:15:17,766][DEBUG][logstash.filters.grok    ][main] Adding pattern {"RAILS3"=>"%{RAILS3HEAD}(?:%{RPROCESSING})?(?<context>(?:%{DATA}\\n)*)(?:%{RAILS3FOOT})?"}
[2020-05-26T17:15:17,767][DEBUG][logstash.filters.grok    ][main] Adding pattern {"REDISTIMESTAMP"=>"%{MONTHDAY} %{MONTH} %{TIME}"}
[2020-05-26T17:15:17,769][DEBUG][logstash.filters.grok    ][main] Adding pattern {"REDISLOG"=>"\\[%{POSINT:pid}\\] %{REDISTIMESTAMP:timestamp} \\* "}
[2020-05-26T17:15:17,778][DEBUG][logstash.filters.grok    ][main] Adding pattern {"REDISMONLOG"=>"%{NUMBER:timestamp} \\[%{INT:database} %{IP:client}:%{NUMBER:port}\\] \"%{WORD:command}\"\\s?%{GREEDYDATA:params}"}
[2020-05-26T17:15:17,779][DEBUG][logstash.filters.grok    ][main] Adding pattern {"RUBY_LOGLEVEL"=>"(?:DEBUG|FATAL|ERROR|WARN|INFO)"}
[2020-05-26T17:15:17,781][DEBUG][logstash.filters.grok    ][main] Adding pattern {"RUBY_LOGGER"=>"[DFEWI], \\[%{TIMESTAMP_ISO8601:timestamp} #%{POSINT:pid}\\] *%{RUBY_LOGLEVEL:loglevel} -- +%{DATA:progname}: %{GREEDYDATA:message}"}
[2020-05-26T17:15:17,782][DEBUG][logstash.filters.grok    ][main] Adding pattern {"SQUID3"=>"%{NUMBER:timestamp}\\s+%{NUMBER:duration}\\s%{IP:client_address}\\s%{WORD:cache_result}/%{POSINT:status_code}\\s%{NUMBER:bytes}\\s%{WORD:request_method}\\s%{NOTSPACE:url}\\s(%{NOTSPACE:user}|-)\\s%{WORD:hierarchy_code}/%{IPORHOST:server}\\s%{NOTSPACE:content_type}"}
[2020-05-26T17:15:17,800][DEBUG][logstash.filters.grok    ][main] replacement_pattern => (?<TIME:log_time>(?!<[0-9])%{HOUR}:%{MINUTE}(?::%{SECOND})(?![0-9]))
[2020-05-26T17:15:17,802][DEBUG][logstash.filters.grok    ][main] replacement_pattern => (?:(?:2[0123]|[01]?[0-9]))
[2020-05-26T17:15:17,805][DEBUG][logstash.filters.grok    ][main] replacement_pattern => (?:(?:[0-5][0-9]))
[2020-05-26T17:15:17,810][DEBUG][logstash.filters.grok    ][main] replacement_pattern => (?:(?:(?:[0-5]?[0-9]|60)(?:[:.,][0-9]+)?))
[2020-05-26T17:15:17,811][DEBUG][logstash.filters.grok    ][main] replacement_pattern => (?<LOGLEVEL:log_level>([Aa]lert|ALERT|[Tt]race|TRACE|[Dd]ebug|DEBUG|[Nn]otice|NOTICE|[Ii]nfo|INFO|[Ww]arn?(?:ing)?|WARN?(?:ING)?|[Ee]rr?(?:or)?|ERR?(?:OR)?|[Cc]rit?(?:ical)?|CRIT?(?:ICAL)?|[Ff]atal|FATAL|[Ss]evere|SEVERE|EMERG(?:ENCY)?|[Ee]merg(?:ency)?))
[2020-05-26T17:15:17,812][DEBUG][logstash.filters.grok    ][main] replacement_pattern => (?<DATA:class_name>.*?)
[2020-05-26T17:15:17,814][DEBUG][logstash.filters.grok    ][main] replacement_pattern => (?:.*)
[2020-05-26T17:15:17,822][DEBUG][logstash.filters.grok    ][main] Grok compiled OK {:pattern=>"%{TIME:log_time} %{LOGLEVEL:log_level} %{DATA:class_name} %{GREEDYDATA :threadName}", :expanded_pattern=>"(?<TIME:log_time>(?!<[0-9])(?:(?:2[0123]|[01]?[0-9])):(?:(?:[0-5][0-9]))(?::(?:(?:(?:[0-5]?[0-9]|60)(?:[:.,][0-9]+)?)))(?![0-9])) (?<LOGLEVEL:log_level>([Aa]lert|ALERT|[Tt]race|TRACE|[Dd]ebug|DEBUG|[Nn]otice|NOTICE|[Ii]nfo|INFO|[Ww]arn?(?:ing)?|WARN?(?:ING)?|[Ee]rr?(?:or)?|ERR?(?:OR)?|[Cc]rit?(?:ical)?|CRIT?(?:ICAL)?|[Ff]atal|FATAL|[Ss]evere|SEVERE|EMERG(?:ENCY)?|[Ee]merg(?:ency)?)) (?<DATA:class_name>.*?) (?:.*)"}
[2020-05-26T17:15:17,932][WARN ][org.logstash.instrument.metrics.gauge.LazyDelegatingGauge][main] A gauge metric of an unknown type (org.jruby.RubyArray) has been created for key: cluster_uuids. This may result in invalid serialization.  It is recommended to log an issue to the responsible developer/development team.
[2020-05-26T17:15:17,940][INFO ][logstash.javapipeline    ][main] Starting pipeline {:pipeline_id=>"main", "pipeline.workers"=>8, "pipeline.batch.size"=>125, "pipeline.batch.delay"=>50, "pipeline.max_inflight"=>1000, "pipeline.sources"=>["D:/Softwares/logstash-7.7.0/config/logstash-sample.conf"], :thread=>"#<Thread:0x7aa6b842 run>"}
[2020-05-26T17:15:18,519][DEBUG][org.logstash.config.ir.CompiledPipeline][main] Compiled conditional
 [if (event.getField('[type]')=='applog')] 
 into 
 org.logstash.config.ir.compiler.ComputeStepSyntaxElement@48bce41f
[2020-05-26T17:15:18,519][DEBUG][org.logstash.config.ir.CompiledPipeline][main] Compiled conditional
 [if (event.getField('[type]')=='applog')] 
 into 
 org.logstash.config.ir.compiler.ComputeStepSyntaxElement@48bce41f
[2020-05-26T17:15:18,520][DEBUG][org.logstash.config.ir.CompiledPipeline][main] Compiled conditional
 [if (event.getField('[type]')=='applog')] 
 into 
 org.logstash.config.ir.compiler.ComputeStepSyntaxElement@48bce41f
[2020-05-26T17:15:18,520][DEBUG][org.logstash.config.ir.CompiledPipeline][main] Compiled conditional
 [if (event.getField('[type]')=='applog')] 
 into 
 org.logstash.config.ir.compiler.ComputeStepSyntaxElement@48bce41f
[2020-05-26T17:15:18,519][DEBUG][org.logstash.config.ir.CompiledPipeline][main] Compiled conditional
 [if (event.getField('[type]')=='applog')] 
 into 
 org.logstash.config.ir.compiler.ComputeStepSyntaxElement@48bce41f
[2020-05-26T17:15:18,519][DEBUG][org.logstash.config.ir.CompiledPipeline][main] Compiled conditional
 [if (event.getField('[type]')=='applog')] 
 into 
 org.logstash.config.ir.compiler.ComputeStepSyntaxElement@48bce41f
[2020-05-26T17:15:18,519][DEBUG][org.logstash.config.ir.CompiledPipeline][main] Compiled conditional
 [if (event.getField('[type]')=='applog')] 
 into 
 org.logstash.config.ir.compiler.ComputeStepSyntaxElement@48bce41f
[2020-05-26T17:15:18,519][DEBUG][org.logstash.config.ir.CompiledPipeline][main] Compiled conditional
 [if (event.getField('[type]')=='applog')] 
 into 
 org.logstash.config.ir.compiler.ComputeStepSyntaxElement@48bce41f
[2020-05-26T17:15:18,679][DEBUG][org.logstash.config.ir.CompiledPipeline][main] Compiled filter
 P[filter-grok{"match"=>{"message"=>"%{TIME:log_time} %{LOGLEVEL:log_level} %{DATA:class_name} %{GREEDYDATA :threadName}"}}|[file]D:/Softwares/logstash-7.7.0/config/logstash-sample.conf:11:3:```
grok {
        	match => { "message" => "%{TIME:log_time} %{LOGLEVEL:log_level} %{DATA:class_name} %{GREEDYDATA :threadName}"}
    	}
```] 
 into 
 org.logstash.config.ir.compiler.ComputeStepSyntaxElement@151319e5
[2020-05-26T17:15:18,686][DEBUG][org.logstash.config.ir.CompiledPipeline][main] Compiled filter
 P[filter-grok{"match"=>{"message"=>"%{TIME:log_time} %{LOGLEVEL:log_level} %{DATA:class_name} %{GREEDYDATA :threadName}"}}|[file]D:/Softwares/logstash-7.7.0/config/logstash-sample.conf:11:3:```
grok {
        	match => { "message" => "%{TIME:log_time} %{LOGLEVEL:log_level} %{DATA:class_name} %{GREEDYDATA :threadName}"}
    	}
```] 
 into 
 org.logstash.config.ir.compiler.ComputeStepSyntaxElement@151319e5
[2020-05-26T17:15:18,686][DEBUG][org.logstash.config.ir.CompiledPipeline][main] Compiled filter
 P[filter-grok{"match"=>{"message"=>"%{TIME:log_time} %{LOGLEVEL:log_level} %{DATA:class_name} %{GREEDYDATA :threadName}"}}|[file]D:/Softwares/logstash-7.7.0/config/logstash-sample.conf:11:3:```
grok {
        	match => { "message" => "%{TIME:log_time} %{LOGLEVEL:log_level} %{DATA:class_name} %{GREEDYDATA :threadName}"}
    	}
```] 
 into 
 org.logstash.config.ir.compiler.ComputeStepSyntaxElement@151319e5
[2020-05-26T17:15:18,683][DEBUG][org.logstash.config.ir.CompiledPipeline][main] Compiled filter
 P[filter-grok{"match"=>{"message"=>"%{TIME:log_time} %{LOGLEVEL:log_level} %{DATA:class_name} %{GREEDYDATA :threadName}"}}|[file]D:/Softwares/logstash-7.7.0/config/logstash-sample.conf:11:3:```
grok {
        	match => { "message" => "%{TIME:log_time} %{LOGLEVEL:log_level} %{DATA:class_name} %{GREEDYDATA :threadName}"}
    	}
```] 
 into 
 org.logstash.config.ir.compiler.ComputeStepSyntaxElement@151319e5
[2020-05-26T17:15:18,682][DEBUG][org.logstash.config.ir.CompiledPipeline][main] Compiled filter
 P[filter-grok{"match"=>{"message"=>"%{TIME:log_time} %{LOGLEVEL:log_level} %{DATA:class_name} %{GREEDYDATA :threadName}"}}|[file]D:/Softwares/logstash-7.7.0/config/logstash-sample.conf:11:3:```
grok {
        	match => { "message" => "%{TIME:log_time} %{LOGLEVEL:log_level} %{DATA:class_name} %{GREEDYDATA :threadName}"}
    	}
```] 
 into 
 org.logstash.config.ir.compiler.ComputeStepSyntaxElement@151319e5
[2020-05-26T17:15:18,682][DEBUG][org.logstash.config.ir.CompiledPipeline][main] Compiled filter
 P[filter-grok{"match"=>{"message"=>"%{TIME:log_time} %{LOGLEVEL:log_level} %{DATA:class_name} %{GREEDYDATA :threadName}"}}|[file]D:/Softwares/logstash-7.7.0/config/logstash-sample.conf:11:3:```
grok {
        	match => { "message" => "%{TIME:log_time} %{LOGLEVEL:log_level} %{DATA:class_name} %{GREEDYDATA :threadName}"}
    	}
```] 
 into 
 org.logstash.config.ir.compiler.ComputeStepSyntaxElement@151319e5
[2020-05-26T17:15:18,680][DEBUG][org.logstash.config.ir.CompiledPipeline][main] Compiled filter
 P[filter-grok{"match"=>{"message"=>"%{TIME:log_time} %{LOGLEVEL:log_level} %{DATA:class_name} %{GREEDYDATA :threadName}"}}|[file]D:/Softwares/logstash-7.7.0/config/logstash-sample.conf:11:3:```
grok {
        	match => { "message" => "%{TIME:log_time} %{LOGLEVEL:log_level} %{DATA:class_name} %{GREEDYDATA :threadName}"}
    	}
```] 
 into 
 org.logstash.config.ir.compiler.ComputeStepSyntaxElement@151319e5
[2020-05-26T17:15:18,670][DEBUG][org.logstash.config.ir.CompiledPipeline][main] Compiled filter
 P[filter-grok{"match"=>{"message"=>"%{TIME:log_time} %{LOGLEVEL:log_level} %{DATA:class_name} %{GREEDYDATA :threadName}"}}|[file]D:/Softwares/logstash-7.7.0/config/logstash-sample.conf:11:3:```
grok {
        	match => { "message" => "%{TIME:log_time} %{LOGLEVEL:log_level} %{DATA:class_name} %{GREEDYDATA :threadName}"}
    	}
```] 
 into 
 org.logstash.config.ir.compiler.ComputeStepSyntaxElement@151319e5
[2020-05-26T17:15:18,751][DEBUG][org.logstash.config.ir.CompiledPipeline][main] Compiled output
 P[output-file{"path"=>"C:/Users/karan/Desktop/output.txt"}|[file]D:/Softwares/logstash-7.7.0/config/logstash-sample.conf:21:5:```
file {
        path => "C:/Users/karan/Desktop/output.txt"
    }
```] 
 into 
 org.logstash.config.ir.compiler.ComputeStepSyntaxElement@4142f13c
[2020-05-26T17:15:18,756][DEBUG][org.logstash.config.ir.CompiledPipeline][main] Compiled output
 P[output-file{"path"=>"C:/Users/karan/Desktop/output.txt"}|[file]D:/Softwares/logstash-7.7.0/config/logstash-sample.conf:21:5:```
file {
        path => "C:/Users/karan/Desktop/output.txt"
    }
```] 
 into 
 org.logstash.config.ir.compiler.ComputeStepSyntaxElement@4142f13c
[2020-05-26T17:15:18,764][DEBUG][org.logstash.config.ir.CompiledPipeline][main] Compiled output
 P[output-file{"path"=>"C:/Users/karan/Desktop/output.txt"}|[file]D:/Softwares/logstash-7.7.0/config/logstash-sample.conf:21:5:```
file {
        path => "C:/Users/karan/Desktop/output.txt"
    }
```] 
 into 
 org.logstash.config.ir.compiler.ComputeStepSyntaxElement@4142f13c
[2020-05-26T17:15:18,754][DEBUG][org.logstash.config.ir.CompiledPipeline][main] Compiled output
 P[output-file{"path"=>"C:/Users/karan/Desktop/output.txt"}|[file]D:/Softwares/logstash-7.7.0/config/logstash-sample.conf:21:5:```
file {
        path => "C:/Users/karan/Desktop/output.txt"
    }
```] 
 into 
 org.logstash.config.ir.compiler.ComputeStepSyntaxElement@4142f13c
[2020-05-26T17:15:18,754][DEBUG][org.logstash.config.ir.CompiledPipeline][main] Compiled output
 P[output-file{"path"=>"C:/Users/karan/Desktop/output.txt"}|[file]D:/Softwares/logstash-7.7.0/config/logstash-sample.conf:21:5:```
file {
        path => "C:/Users/karan/Desktop/output.txt"
    }
```] 
 into 
 org.logstash.config.ir.compiler.ComputeStepSyntaxElement@4142f13c
[2020-05-26T17:15:18,753][DEBUG][org.logstash.config.ir.CompiledPipeline][main] Compiled output
 P[output-file{"path"=>"C:/Users/karan/Desktop/output.txt"}|[file]D:/Softwares/logstash-7.7.0/config/logstash-sample.conf:21:5:```
file {
        path => "C:/Users/karan/Desktop/output.txt"
    }
```] 
 into 
 org.logstash.config.ir.compiler.ComputeStepSyntaxElement@4142f13c
[2020-05-26T17:15:18,766][DEBUG][org.logstash.config.ir.CompiledPipeline][main] Compiled output
 P[output-file{"path"=>"C:/Users/karan/Desktop/output.txt"}|[file]D:/Softwares/logstash-7.7.0/config/logstash-sample.conf:21:5:```
file {
        path => "C:/Users/karan/Desktop/output.txt"
    }
```] 
 into 
 org.logstash.config.ir.compiler.ComputeStepSyntaxElement@4142f13c
[2020-05-26T17:15:18,765][DEBUG][org.logstash.config.ir.CompiledPipeline][main] Compiled output
 P[output-file{"path"=>"C:/Users/karan/Desktop/output.txt"}|[file]D:/Softwares/logstash-7.7.0/config/logstash-sample.conf:21:5:```
file {
        path => "C:/Users/karan/Desktop/output.txt"
    }
```] 
 into 
 org.logstash.config.ir.compiler.ComputeStepSyntaxElement@4142f13c
[2020-05-26T17:15:18,861][DEBUG][logstash.instrument.periodicpoller.cgroup] One or more required cgroup files or directories not found: /proc/self/cgroup, /sys/fs/cgroup/cpuacct, /sys/fs/cgroup/cpu
[2020-05-26T17:15:18,897][DEBUG][logstash.outputs.file    ][main] Starting flush cycle
[2020-05-26T17:15:19,001][DEBUG][logstash.instrument.periodicpoller.jvm] collector name {:name=>"ParNew"}
[2020-05-26T17:15:19,004][DEBUG][logstash.instrument.periodicpoller.jvm] collector name {:name=>"ConcurrentMarkSweep"}
[2020-05-26T17:15:19,338][INFO ][logstash.inputs.file     ][main] No sincedb_path set, generating one based on the "path" setting {:sincedb_path=>"D:/Softwares/logstash-7.7.0/data/plugins/inputs/file/.sincedb_1bcc6f92ecaee21dfd4559d640276863", :path=>["C:/Users/karan/Desktop/app.log"]}
[2020-05-26T17:15:19,369][INFO ][logstash.javapipeline    ][main] Pipeline started {"pipeline.id"=>"main"}
[2020-05-26T17:15:19,377][DEBUG][logstash.javapipeline    ] Pipeline started successfully {:pipeline_id=>"main", :thread=>"#<Thread:0x7aa6b842 run>"}
[2020-05-26T17:15:19,390][DEBUG][org.logstash.execution.PeriodicFlush][main] Pushing flush onto pipeline.
[2020-05-26T17:15:19,426][INFO ][filewatch.observingtail  ][main][e7a181030b83e1a5f6683c0258731fbbd14e4985f3cd81c1d9a872faec8ade6d] START, creating Discoverer, Watch with file and sincedb collections
[2020-05-26T17:15:19,432][INFO ][logstash.agent           ] Pipelines running {:count=>1, :running_pipelines=>[:main], :non_running_pipelines=>[]}
[2020-05-26T17:15:19,464][DEBUG][logstash.agent           ] Starting puma
[2020-05-26T17:15:19,483][DEBUG][logstash.agent           ] Trying to start WebServer {:port=>9600}
[2020-05-26T17:15:19,532][DEBUG][logstash.api.service     ] [api-service] start
[2020-05-26T17:15:19,746][INFO ][logstash.agent           ] Successfully started Logstash API endpoint {:port=>9600}
[2020-05-26T17:15:20,916][DEBUG][logstash.outputs.file    ][main] Starting flush cycle
[2020-05-26T17:15:22,917][DEBUG][logstash.outputs.file    ][main] Starting flush cycle
[2020-05-26T17:15:23,913][DEBUG][logstash.instrument.periodicpoller.cgroup] One or more required cgroup files or directories not found: /proc/self/cgroup, /sys/fs/cgroup/cpuacct, /sys/fs/cgroup/cpu
[2020-05-26T17:15:24,016][DEBUG][logstash.instrument.periodicpoller.jvm] collector name {:name=>"ParNew"}
[2020-05-26T17:15:24,023][DEBUG][logstash.instrument.periodicpoller.jvm] collector name {:name=>"ConcurrentMarkSweep"}
[2020-05-26T17:15:24,390][DEBUG][org.logstash.execution.PeriodicFlush][main] Pushing flush onto pipeline.
[2020-05-26T17:15:24,919][DEBUG][logstash.outputs.file    ][main] Starting flush cycle
[2020-05-26T17:15:26,888][DEBUG][logstash.outputs.file    ][main][005d7a6a7c2eabd470f6c2a0dda909b7078cf3af678917e60d710782155df230] Starting stale files cleanup cycle {:files=>{}}
[2020-05-26T17:15:26,900][DEBUG][logstash.outputs.file    ][main][005d7a6a7c2eabd470f6c2a0dda909b7078cf3af678917e60d710782155df230] 0 stale files found {:inactive_files=>{}}
[2020-05-26T17:15:26,921][DEBUG][logstash.outputs.file    ][main] Starting flush cycle
[2020-05-26T17:15:28,916][DEBUG][logstash.instrument.periodicpoller.cgroup] One or more required cgroup files or directories not found: /proc/self/cgroup, /sys/fs/cgroup/cpuacct, /sys/fs/cgroup/cpu
[2020-05-26T17:15:28,923][DEBUG][logstash.outputs.file    ][main] Starting flush cycle
[2020-05-26T17:15:29,029][DEBUG][logstash.instrument.periodicpoller.jvm] collector name {:name=>"ParNew"}
[2020-05-26T17:15:29,030][DEBUG][logstash.instrument.periodicpoller.jvm] collector name {:name=>"ConcurrentMarkSweep"}
[2020-05-26T17:15:29,389][DEBUG][org.logstash.execution.PeriodicFlush][main] Pushing flush onto pipeline.
[2020-05-26T17:15:30,926][DEBUG][logstash.outputs.file    ][main] Starting flush cycle
[2020-05-26T17:15:32,928][DEBUG][logstash.outputs.file    ][main] Starting flush cycle
[2020-05-26T17:15:33,919][DEBUG][logstash.instrument.periodicpoller.cgroup] One or more required cgroup files or directories not found: /proc/self/cgroup, /sys/fs/cgroup/cpuacct, /sys/fs/cgroup/cpu
[2020-05-26T17:15:34,044][DEBUG][logstash.instrument.periodicpoller.jvm] collector name {:name=>"ParNew"}
[2020-05-26T17:15:34,045][DEBUG][logstash.instrument.periodicpoller.jvm] collector name {:name=>"ConcurrentMarkSweep"}
[2020-05-26T17:15:34,389][DEBUG][org.logstash.execution.PeriodicFlush][main] Pushing flush onto pipeline.
[2020-05-26T17:15:34,931][DEBUG][logstash.outputs.file    ][main] Starting flush cycle
[2020-05-26T17:15:36,899][DEBUG][logstash.outputs.file    ][main][005d7a6a7c2eabd470f6c2a0dda909b7078cf3af678917e60d710782155df230] Starting stale files cleanup cycle {:files=>{}}
[2020-05-26T17:15:36,909][DEBUG][logstash.outputs.file    ][main][005d7a6a7c2eabd470f6c2a0dda909b7078cf3af678917e60d710782155df230] 0 stale files found {:inactive_files=>{}}
[2020-05-26T17:15:36,933][DEBUG][logstash.outputs.file    ][main] Starting flush cycle
[2020-05-26T17:15:38,926][DEBUG][logstash.instrument.periodicpoller.cgroup] One or more required cgroup files or directories not found: /proc/self/cgroup, /sys/fs/cgroup/cpuacct, /sys/fs/cgroup/cpu
[2020-05-26T17:15:38,934][DEBUG][logstash.outputs.file    ][main] Starting flush cycle
[2020-05-26T17:15:39,058][DEBUG][logstash.instrument.periodicpoller.jvm] collector name {:name=>"ParNew"}
[2020-05-26T17:15:39,059][DEBUG][logstash.instrument.periodicpoller.jvm] collector name {:name=>"ConcurrentMarkSweep"}
[2020-05-26T17:15:39,389][DEBUG][org.logstash.execution.PeriodicFlush][main] Pushing flush onto pipeline.
[2020-05-26T17:15:40,940][DEBUG][logstash.outputs.file    ][main] Starting flush cycle
[2020-05-26T17:15:42,942][DEBUG][logstash.outputs.file    ][main] Starting flush cycle
[2020-05-26T17:15:43,932][DEBUG][logstash.instrument.periodicpoller.cgroup] One or more required cgroup files or directories not found: /proc/self/cgroup, /sys/fs/cgroup/cpuacct, /sys/fs/cgroup/cpu
[2020-05-26T17:15:44,073][DEBUG][logstash.instrument.periodicpoller.jvm] collector name {:name=>"ParNew"}
[2020-05-26T17:15:44,076][DEBUG][logstash.instrument.periodicpoller.jvm] collector name {:name=>"ConcurrentMarkSweep"}
[2020-05-26T17:15:44,389][DEBUG][org.logstash.execution.PeriodicFlush][main] Pushing flush onto pipeline.
[2020-05-26T17:15:44,944][DEBUG][logstash.outputs.file    ][main] Starting flush cycle
[2020-05-26T17:15:46,902][DEBUG][logstash.outputs.file    ][main][005d7a6a7c2eabd470f6c2a0dda909b7078cf3af678917e60d710782155df230] Starting stale files cleanup cycle {:files=>{}}
[2020-05-26T17:15:46,907][DEBUG][logstash.outputs.file    ][main][005d7a6a7c2eabd470f6c2a0dda909b7078cf3af678917e60d710782155df230] 0 stale files found {:inactive_files=>{}}
[2020-05-26T17:15:46,949][DEBUG][logstash.outputs.file    ][main] Starting flush cycle
[2020-05-26T17:15:48,937][DEBUG][logstash.instrument.periodicpoller.cgroup] One or more required cgroup files or directories not found: /proc/self/cgroup, /sys/fs/cgroup/cpuacct, /sys/fs/cgroup/cpu
[2020-05-26T17:15:48,952][DEBUG][logstash.outputs.file    ][main] Starting flush cycle
[2020-05-26T17:15:49,090][DEBUG][logstash.instrument.periodicpoller.jvm] collector name {:name=>"ParNew"}
[2020-05-26T17:15:49,093][DEBUG][logstash.instrument.periodicpoller.jvm] collector name {:name=>"ConcurrentMarkSweep"}
[2020-05-26T17:15:49,389][DEBUG][org.logstash.execution.PeriodicFlush][main] Pushing flush onto pipeline.
[2020-05-26T17:15:50,953][DEBUG][logstash.outputs.file    ][main] Starting flush cycle
[2020-05-26T17:15:52,956][DEBUG][logstash.outputs.file    ][main] Starting flush cycle
[2020-05-26T17:15:53,941][DEBUG][logstash.instrument.periodicpoller.cgroup] One or more required cgroup files or directories not found: /proc/self/cgroup, /sys/fs/cgroup/cpuacct, /sys/fs/cgroup/cpu
[2020-05-26T17:15:54,107][DEBUG][logstash.instrument.periodicpoller.jvm] collector name {:name=>"ParNew"}
[2020-05-26T17:15:54,109][DEBUG][logstash.instrument.periodicpoller.jvm] collector name {:name=>"ConcurrentMarkSweep"}
[2020-05-26T17:15:54,389][DEBUG][org.logstash.execution.PeriodicFlush][main] Pushing flush onto pipeline.
[2020-05-26T17:15:54,957][DEBUG][logstash.outputs.file    ][main] Starting flush cycle
[2020-05-26T17:15:56,907][DEBUG][logstash.outputs.file    ][main][005d7a6a7c2eabd470f6c2a0dda909b7078cf3af678917e60d710782155df230] Starting stale files cleanup cycle {:files=>{}}
[2020-05-26T17:15:56,909][DEBUG][logstash.outputs.file    ][main][005d7a6a7c2eabd470f6c2a0dda909b7078cf3af678917e60d710782155df230] 0 stale files found {:inactive_files=>{}}
[2020-05-26T17:15:56,958][DEBUG][logstash.outputs.file    ][main] Starting flush cycle
[2020-05-26T17:15:58,944][DEBUG][logstash.instrument.periodicpoller.cgroup] One or more required cgroup files or directories not found: /proc/self/cgroup, /sys/fs/cgroup/cpuacct, /sys/fs/cgroup/cpu
[2020-05-26T17:15:58,961][DEBUG][logstash.outputs.file    ][main] Starting flush cycle
[2020-05-26T17:15:59,121][DEBUG][logstash.instrument.periodicpoller.jvm] collector name {:name=>"ParNew"}
[2020-05-26T17:15:59,123][DEBUG][logstash.instrument.periodicpoller.jvm] collector name {:name=>"ConcurrentMarkSweep"}
[2020-05-26T17:15:59,389][DEBUG][org.logstash.execution.PeriodicFlush][main] Pushing flush onto pipeline.
[2020-05-26T17:16:00,396][WARN ][logstash.runner          ] SIGINT received. Shutting down.
[2020-05-26T17:16:00,417][DEBUG][logstash.instrument.periodicpoller.os] Stopping
[2020-05-26T17:16:00,427][DEBUG][logstash.instrument.periodicpoller.jvm] Stopping
[2020-05-26T17:16:00,429][DEBUG][logstash.instrument.periodicpoller.persistentqueue] Stopping
[2020-05-26T17:16:00,430][DEBUG][logstash.instrument.periodicpoller.deadletterqueue] Stopping
[2020-05-26T17:16:00,440][DEBUG][logstash.agent           ] Shutting down all pipelines {:pipelines_count=>1}
[2020-05-26T17:16:00,445][DEBUG][logstash.agent           ] Converging pipelines state {:actions_count=>1}
[2020-05-26T17:16:00,458][DEBUG][logstash.agent           ] Executing action {:action=>LogStash::PipelineAction::Stop/pipeline_id:main}
[2020-05-26T17:16:00,477][DEBUG][logstash.javapipeline    ] Closing inputs {:pipeline_id=>"main", :thread=>"#<Thread:0x7aa6b842 sleep>"}
[2020-05-26T17:16:00,480][DEBUG][logstash.inputs.file     ] Stopping {:plugin=>"LogStash::Inputs::File"}
[2020-05-26T17:16:00,495][INFO ][filewatch.observingtail  ] QUIT - closing all files and shutting down.
[2020-05-26T17:16:00,498][DEBUG][logstash.javapipeline    ] Closed inputs {:pipeline_id=>"main", :thread=>"#<Thread:0x7aa6b842 sleep>"}
[2020-05-26T17:16:00,504][DEBUG][logstash.javapipeline    ] Closing inputs {:pipeline_id=>"main", :thread=>"#<Thread:0x7aa6b842 sleep>"}
[2020-05-26T17:16:00,905][DEBUG][logstash.javapipeline    ][main] Input plugins stopped! Will shutdown filter/output workers. {:pipeline_id=>"main", :thread=>"#<Thread:0x7aa6b842 run>"}
[2020-05-26T17:16:00,913][DEBUG][logstash.javapipeline    ][main] Shutdown waiting for worker thread {:pipeline_id=>"main", :thread=>"#<Thread:0x3a170f83 run>"}
[2020-05-26T17:16:00,964][DEBUG][logstash.outputs.file    ][main] Starting flush cycle
[2020-05-26T17:16:00,990][DEBUG][logstash.javapipeline    ][main] Shutdown waiting for worker thread {:pipeline_id=>"main", :thread=>"#<Thread:0x262faa16 run>"}
[2020-05-26T17:16:01,007][DEBUG][logstash.javapipeline    ][main] Shutdown waiting for worker thread {:pipeline_id=>"main", :thread=>"#<Thread:0x6b1a805 dead>"}
[2020-05-26T17:16:01,008][DEBUG][logstash.javapipeline    ] Worker closed {:pipeline_id=>"main", :thread=>"#<Thread:0x7aa6b842 run>"}
[2020-05-26T17:16:01,008][DEBUG][logstash.javapipeline    ][main] Shutdown waiting for worker thread {:pipeline_id=>"main", :thread=>"#<Thread:0x19d7678c dead>"}
[2020-05-26T17:16:01,010][DEBUG][logstash.javapipeline    ][main] Shutdown waiting for worker thread {:pipeline_id=>"main", :thread=>"#<Thread:0x5017f2d0 dead>"}
[2020-05-26T17:16:01,016][DEBUG][logstash.javapipeline    ][main] Shutdown waiting for worker thread {:pipeline_id=>"main", :thread=>"#<Thread:0x28215ab0 dead>"}
[2020-05-26T17:16:01,034][DEBUG][logstash.javapipeline    ][main] Shutdown waiting for worker thread {:pipeline_id=>"main", :thread=>"#<Thread:0x70d4cb81 dead>"}
[2020-05-26T17:16:01,040][INFO ][logstash.javapipeline    ] Pipeline terminated {"pipeline.id"=>"main"}
[2020-05-26T17:16:01,044][DEBUG][logstash.javapipeline    ][main] Shutdown waiting for worker thread {:pipeline_id=>"main", :thread=>"#<Thread:0x1d1e38e9 dead>"}
[2020-05-26T17:16:01,052][DEBUG][logstash.filters.grok    ][main] Closing {:plugin=>"LogStash::Filters::Grok"}
[2020-05-26T17:16:01,056][DEBUG][logstash.pluginmetadata  ][main] Removing metadata for plugin 449d117e0db14733ffa7c115ab6a07f5f311aa4367a21e085c9d69fc6ae85cc9
[2020-05-26T17:16:01,058][DEBUG][logstash.outputs.file    ][main] Closing {:plugin=>"LogStash::Outputs::File"}
[2020-05-26T17:16:02,969][DEBUG][logstash.outputs.file    ][main] Close: closing files
[2020-05-26T17:16:02,973][DEBUG][logstash.pluginmetadata  ][main] Removing metadata for plugin 005d7a6a7c2eabd470f6c2a0dda909b7078cf3af678917e60d710782155df230
[2020-05-26T17:16:02,977][DEBUG][logstash.javapipeline    ][main] Pipeline has been shutdown {:pipeline_id=>"main", :thread=>"#<Thread:0x7aa6b842 run>"}
[2020-05-26T17:16:03,027][INFO ][logstash.runner          ] Logstash shut down.
[2020-05-26T17:17:55,829][DEBUG][logstash.modules.scaffold] Found module {:module_name=>"fb_apache", :directory=>"D:/Softwares/logstash-7.7.0/modules/fb_apache/configuration"}
[2020-05-26T17:17:55,916][DEBUG][logstash.plugins.registry] Adding plugin to the registry {:name=>"fb_apache", :type=>:modules, :class=>#<LogStash::Modules::Scaffold:0x630f24a5 @directory="D:/Softwares/logstash-7.7.0/modules/fb_apache/configuration", @module_name="fb_apache", @kibana_version_parts=["6", "0", "0"]>}
[2020-05-26T17:17:55,923][DEBUG][logstash.modules.scaffold] Found module {:module_name=>"netflow", :directory=>"D:/Softwares/logstash-7.7.0/modules/netflow/configuration"}
[2020-05-26T17:17:55,926][DEBUG][logstash.plugins.registry] Adding plugin to the registry {:name=>"netflow", :type=>:modules, :class=>#<LogStash::Modules::Scaffold:0x34ceabf1 @directory="D:/Softwares/logstash-7.7.0/modules/netflow/configuration", @module_name="netflow", @kibana_version_parts=["6", "0", "0"]>}
[2020-05-26T17:17:56,007][DEBUG][logstash.runner          ] -------- Logstash Settings (* means modified) ---------
[2020-05-26T17:17:56,010][DEBUG][logstash.runner          ] node.name: "DESKTOP-IU1N73S"
[2020-05-26T17:17:56,012][DEBUG][logstash.runner          ] *path.config: "config\\logstash-sample.conf"
[2020-05-26T17:17:56,014][DEBUG][logstash.runner          ] path.data: "D:/Softwares/logstash-7.7.0/data"
[2020-05-26T17:17:56,015][DEBUG][logstash.runner          ] modules.cli: []
[2020-05-26T17:17:56,017][DEBUG][logstash.runner          ] modules: []
[2020-05-26T17:17:56,018][DEBUG][logstash.runner          ] modules_list: []
[2020-05-26T17:17:56,020][DEBUG][logstash.runner          ] modules_variable_list: []
[2020-05-26T17:17:56,024][DEBUG][logstash.runner          ] modules_setup: false
[2020-05-26T17:17:56,026][DEBUG][logstash.runner          ] config.test_and_exit: false
[2020-05-26T17:17:56,027][DEBUG][logstash.runner          ] config.reload.automatic: false
[2020-05-26T17:17:56,029][DEBUG][logstash.runner          ] config.reload.interval: 3000000000
[2020-05-26T17:17:56,030][DEBUG][logstash.runner          ] config.support_escapes: false
[2020-05-26T17:17:56,032][DEBUG][logstash.runner          ] config.field_reference.parser: "STRICT"
[2020-05-26T17:17:56,033][DEBUG][logstash.runner          ] metric.collect: true
[2020-05-26T17:17:56,034][DEBUG][logstash.runner          ] pipeline.id: "main"
[2020-05-26T17:17:56,036][DEBUG][logstash.runner          ] pipeline.system: false
[2020-05-26T17:17:56,038][DEBUG][logstash.runner          ] pipeline.workers: 8
[2020-05-26T17:17:56,040][DEBUG][logstash.runner          ] pipeline.batch.size: 125
[2020-05-26T17:17:56,041][DEBUG][logstash.runner          ] pipeline.batch.delay: 50
[2020-05-26T17:17:56,043][DEBUG][logstash.runner          ] pipeline.unsafe_shutdown: false
[2020-05-26T17:17:56,045][DEBUG][logstash.runner          ] pipeline.java_execution: true
[2020-05-26T17:17:56,047][DEBUG][logstash.runner          ] pipeline.reloadable: true
[2020-05-26T17:17:56,048][DEBUG][logstash.runner          ] pipeline.plugin_classloaders: false
[2020-05-26T17:17:56,050][DEBUG][logstash.runner          ] pipeline.separate_logs: false
[2020-05-26T17:17:56,051][DEBUG][logstash.runner          ] pipeline.ordered: "auto"
[2020-05-26T17:17:56,053][DEBUG][logstash.runner          ] path.plugins: []
[2020-05-26T17:17:56,055][DEBUG][logstash.runner          ] config.debug: false
[2020-05-26T17:17:56,056][DEBUG][logstash.runner          ] *log.level: "debug" (default: "info")
[2020-05-26T17:17:56,057][DEBUG][logstash.runner          ] version: false
[2020-05-26T17:17:56,059][DEBUG][logstash.runner          ] help: false
[2020-05-26T17:17:56,061][DEBUG][logstash.runner          ] log.format: "plain"
[2020-05-26T17:17:56,063][DEBUG][logstash.runner          ] http.host: "127.0.0.1"
[2020-05-26T17:17:56,064][DEBUG][logstash.runner          ] http.port: 9600..9700
[2020-05-26T17:17:56,065][DEBUG][logstash.runner          ] http.environment: "production"
[2020-05-26T17:17:56,067][DEBUG][logstash.runner          ] queue.type: "memory"
[2020-05-26T17:17:56,068][DEBUG][logstash.runner          ] queue.drain: false
[2020-05-26T17:17:56,070][DEBUG][logstash.runner          ] queue.page_capacity: 67108864
[2020-05-26T17:17:56,072][DEBUG][logstash.runner          ] queue.max_bytes: 1073741824
[2020-05-26T17:17:56,073][DEBUG][logstash.runner          ] queue.max_events: 0
[2020-05-26T17:17:56,075][DEBUG][logstash.runner          ] queue.checkpoint.acks: 1024
[2020-05-26T17:17:56,076][DEBUG][logstash.runner          ] queue.checkpoint.writes: 1024
[2020-05-26T17:17:56,077][DEBUG][logstash.runner          ] queue.checkpoint.interval: 1000
[2020-05-26T17:17:56,079][DEBUG][logstash.runner          ] queue.checkpoint.retry: false
[2020-05-26T17:17:56,080][DEBUG][logstash.runner          ] dead_letter_queue.enable: false
[2020-05-26T17:17:56,082][DEBUG][logstash.runner          ] dead_letter_queue.max_bytes: 1073741824
[2020-05-26T17:17:56,083][DEBUG][logstash.runner          ] slowlog.threshold.warn: -1
[2020-05-26T17:17:56,085][DEBUG][logstash.runner          ] slowlog.threshold.info: -1
[2020-05-26T17:17:56,087][DEBUG][logstash.runner          ] slowlog.threshold.debug: -1
[2020-05-26T17:17:56,088][DEBUG][logstash.runner          ] slowlog.threshold.trace: -1
[2020-05-26T17:17:56,089][DEBUG][logstash.runner          ] keystore.classname: "org.logstash.secret.store.backend.JavaKeyStore"
[2020-05-26T17:17:56,090][DEBUG][logstash.runner          ] keystore.file: "D:/Softwares/logstash-7.7.0/config/logstash.keystore"
[2020-05-26T17:17:56,092][DEBUG][logstash.runner          ] path.queue: "D:/Softwares/logstash-7.7.0/data/queue"
[2020-05-26T17:17:56,093][DEBUG][logstash.runner          ] path.dead_letter_queue: "D:/Softwares/logstash-7.7.0/data/dead_letter_queue"
[2020-05-26T17:17:56,095][DEBUG][logstash.runner          ] path.settings: "D:/Softwares/logstash-7.7.0/config"
[2020-05-26T17:17:56,096][DEBUG][logstash.runner          ] path.logs: "D:/Softwares/logstash-7.7.0/logs"
[2020-05-26T17:17:56,098][DEBUG][logstash.runner          ] xpack.management.enabled: false
[2020-05-26T17:17:56,099][DEBUG][logstash.runner          ] xpack.management.logstash.poll_interval: 5000000000
[2020-05-26T17:17:56,101][DEBUG][logstash.runner          ] xpack.management.pipeline.id: ["main"]
[2020-05-26T17:17:56,102][DEBUG][logstash.runner          ] xpack.management.elasticsearch.username: "logstash_system"
[2020-05-26T17:17:56,103][DEBUG][logstash.runner          ] xpack.management.elasticsearch.hosts: ["https://localhost:9200"]
[2020-05-26T17:17:56,104][DEBUG][logstash.runner          ] xpack.management.elasticsearch.ssl.verification_mode: "certificate"
[2020-05-26T17:17:56,105][DEBUG][logstash.runner          ] xpack.management.elasticsearch.sniffing: false
[2020-05-26T17:17:56,106][DEBUG][logstash.runner          ] xpack.monitoring.enabled: false
[2020-05-26T17:17:56,107][DEBUG][logstash.runner          ] xpack.monitoring.elasticsearch.hosts: ["http://localhost:9200"]
[2020-05-26T17:17:56,108][DEBUG][logstash.runner          ] xpack.monitoring.collection.interval: 10000000000
[2020-05-26T17:17:56,109][DEBUG][logstash.runner          ] xpack.monitoring.collection.timeout_interval: 600000000000
[2020-05-26T17:17:56,110][DEBUG][logstash.runner          ] xpack.monitoring.elasticsearch.username: "logstash_system"
[2020-05-26T17:17:56,112][DEBUG][logstash.runner          ] xpack.monitoring.elasticsearch.ssl.verification_mode: "certificate"
[2020-05-26T17:17:56,113][DEBUG][logstash.runner          ] xpack.monitoring.elasticsearch.sniffing: false
[2020-05-26T17:17:56,114][DEBUG][logstash.runner          ] xpack.monitoring.collection.pipeline.details.enabled: true
[2020-05-26T17:17:56,116][DEBUG][logstash.runner          ] xpack.monitoring.collection.config.enabled: true
[2020-05-26T17:17:56,117][DEBUG][logstash.runner          ] monitoring.enabled: false
[2020-05-26T17:17:56,118][DEBUG][logstash.runner          ] monitoring.elasticsearch.hosts: ["http://localhost:9200"]
[2020-05-26T17:17:56,119][DEBUG][logstash.runner          ] monitoring.collection.interval: 10000000000
[2020-05-26T17:17:56,121][DEBUG][logstash.runner          ] monitoring.collection.timeout_interval: 600000000000
[2020-05-26T17:17:56,122][DEBUG][logstash.runner          ] monitoring.elasticsearch.username: "logstash_system"
[2020-05-26T17:17:56,124][DEBUG][logstash.runner          ] monitoring.elasticsearch.ssl.verification_mode: "certificate"
[2020-05-26T17:17:56,125][DEBUG][logstash.runner          ] monitoring.elasticsearch.sniffing: false
[2020-05-26T17:17:56,126][DEBUG][logstash.runner          ] monitoring.collection.pipeline.details.enabled: true
[2020-05-26T17:17:56,128][DEBUG][logstash.runner          ] monitoring.collection.config.enabled: true
[2020-05-26T17:17:56,129][DEBUG][logstash.runner          ] node.uuid: ""
[2020-05-26T17:17:56,131][DEBUG][logstash.runner          ] --------------- Logstash Settings -------------------
[2020-05-26T17:17:56,187][WARN ][logstash.config.source.multilocal] Ignoring the 'pipelines.yml' file because modules or command line options are specified
[2020-05-26T17:17:56,196][INFO ][logstash.runner          ] Starting Logstash {"logstash.version"=>"7.7.0"}
[2020-05-26T17:17:56,233][DEBUG][logstash.agent           ] Setting up metric collection
[2020-05-26T17:17:56,284][DEBUG][logstash.instrument.periodicpoller.os] Starting {:polling_interval=>5, :polling_timeout=>120}
[2020-05-26T17:17:56,294][DEBUG][logstash.instrument.periodicpoller.cgroup] One or more required cgroup files or directories not found: /proc/self/cgroup, /sys/fs/cgroup/cpuacct, /sys/fs/cgroup/cpu
[2020-05-26T17:17:56,415][DEBUG][logstash.instrument.periodicpoller.jvm] Starting {:polling_interval=>5, :polling_timeout=>120}
[2020-05-26T17:17:56,525][DEBUG][logstash.instrument.periodicpoller.jvm] collector name {:name=>"ParNew"}
[2020-05-26T17:17:56,532][DEBUG][logstash.instrument.periodicpoller.jvm] collector name {:name=>"ConcurrentMarkSweep"}
[2020-05-26T17:17:56,556][DEBUG][logstash.instrument.periodicpoller.persistentqueue] Starting {:polling_interval=>5, :polling_timeout=>120}
[2020-05-26T17:17:56,569][DEBUG][logstash.instrument.periodicpoller.deadletterqueue] Starting {:polling_interval=>5, :polling_timeout=>120}
[2020-05-26T17:17:56,621][DEBUG][logstash.agent           ] Starting agent
[2020-05-26T17:17:56,684][DEBUG][logstash.config.source.local.configpathloader] Skipping the following files while reading config since they don't match the specified glob pattern {:files=>["D:/Softwares/logstash-7.7.0/config/jvm.options", "D:/Softwares/logstash-7.7.0/config/log4j2.properties", "D:/Softwares/logstash-7.7.0/config/logstash.yml", "D:/Softwares/logstash-7.7.0/config/pipelines.yml", "D:/Softwares/logstash-7.7.0/config/startup.options"]}
[2020-05-26T17:17:56,690][DEBUG][logstash.config.source.local.configpathloader] Reading config file {:config_file=>"D:/Softwares/logstash-7.7.0/config/logstash-sample.conf"}
[2020-05-26T17:17:56,737][DEBUG][logstash.agent           ] Converging pipelines state {:actions_count=>1}
[2020-05-26T17:17:56,757][DEBUG][logstash.agent           ] Executing action {:action=>LogStash::PipelineAction::Create/pipeline_id:main}
[2020-05-26T17:17:57,417][DEBUG][org.logstash.secret.store.SecretStoreFactory] Attempting to exists or secret store with implementation: org.logstash.secret.store.backend.JavaKeyStore
[2020-05-26T17:17:57,761][DEBUG][org.reflections.Reflections] going to scan these urls:
jar:file:/D:/Softwares/logstash-7.7.0/logstash-core/lib/jars/logstash-core.jar!/
[2020-05-26T17:17:57,808][INFO ][org.reflections.Reflections] Reflections took 43 ms to scan 1 urls, producing 21 keys and 41 values 
[2020-05-26T17:17:57,818][DEBUG][org.reflections.Reflections] expanded subtype co.elastic.logstash.api.Plugin -> co.elastic.logstash.api.Input
[2020-05-26T17:17:57,820][DEBUG][org.reflections.Reflections] expanded subtype co.elastic.logstash.api.Plugin -> co.elastic.logstash.api.Codec
[2020-05-26T17:17:57,822][DEBUG][org.reflections.Reflections] expanded subtype org.jruby.RubyBasicObject -> org.jruby.RubyObject
[2020-05-26T17:17:57,827][DEBUG][org.reflections.Reflections] expanded subtype java.lang.Cloneable -> org.jruby.RubyBasicObject
[2020-05-26T17:17:57,833][DEBUG][org.reflections.Reflections] expanded subtype org.jruby.runtime.builtin.IRubyObject -> org.jruby.RubyBasicObject
[2020-05-26T17:17:57,834][DEBUG][org.reflections.Reflections] expanded subtype java.io.Serializable -> org.jruby.RubyBasicObject
[2020-05-26T17:17:57,836][DEBUG][org.reflections.Reflections] expanded subtype java.lang.Comparable -> org.jruby.RubyBasicObject
[2020-05-26T17:17:57,854][DEBUG][org.reflections.Reflections] expanded subtype org.jruby.runtime.marshal.CoreObjectType -> org.jruby.RubyBasicObject
[2020-05-26T17:17:57,867][DEBUG][org.reflections.Reflections] expanded subtype org.jruby.runtime.builtin.InstanceVariables -> org.jruby.RubyBasicObject
[2020-05-26T17:17:57,871][DEBUG][org.reflections.Reflections] expanded subtype org.jruby.runtime.builtin.InternalVariables -> org.jruby.RubyBasicObject
[2020-05-26T17:17:57,884][DEBUG][org.reflections.Reflections] expanded subtype co.elastic.logstash.api.Plugin -> co.elastic.logstash.api.Output
[2020-05-26T17:17:57,901][DEBUG][org.reflections.Reflections] expanded subtype co.elastic.logstash.api.Metric -> co.elastic.logstash.api.NamespacedMetric
[2020-05-26T17:17:57,905][DEBUG][org.reflections.Reflections] expanded subtype java.security.SecureClassLoader -> java.net.URLClassLoader
[2020-05-26T17:17:57,911][DEBUG][org.reflections.Reflections] expanded subtype java.lang.ClassLoader -> java.security.SecureClassLoader
[2020-05-26T17:17:57,913][DEBUG][org.reflections.Reflections] expanded subtype java.io.Closeable -> java.net.URLClassLoader
[2020-05-26T17:17:57,918][DEBUG][org.reflections.Reflections] expanded subtype java.lang.AutoCloseable -> java.io.Closeable
[2020-05-26T17:17:57,924][DEBUG][org.reflections.Reflections] expanded subtype java.lang.Comparable -> java.lang.Enum
[2020-05-26T17:17:57,931][DEBUG][org.reflections.Reflections] expanded subtype java.io.Serializable -> java.lang.Enum
[2020-05-26T17:17:57,933][DEBUG][org.reflections.Reflections] expanded subtype co.elastic.logstash.api.Plugin -> co.elastic.logstash.api.Filter
[2020-05-26T17:17:58,429][DEBUG][logstash.plugins.registry] On demand adding plugin to the registry {:name=>"stdin", :type=>"input", :class=>LogStash::Inputs::Stdin}
[2020-05-26T17:17:58,577][DEBUG][logstash.plugins.registry] On demand adding plugin to the registry {:name=>"line", :type=>"codec", :class=>LogStash::Codecs::Line}
[2020-05-26T17:17:58,599][DEBUG][logstash.codecs.line     ] config LogStash::Codecs::Line/@id = "line_82ec511a-a23b-41ce-94c7-5297b602b505"
[2020-05-26T17:17:58,601][DEBUG][logstash.codecs.line     ] config LogStash::Codecs::Line/@enable_metric = true
[2020-05-26T17:17:58,605][DEBUG][logstash.codecs.line     ] config LogStash::Codecs::Line/@charset = "UTF-8"
[2020-05-26T17:17:58,608][DEBUG][logstash.codecs.line     ] config LogStash::Codecs::Line/@delimiter = "\n"
[2020-05-26T17:17:58,626][DEBUG][logstash.inputs.stdin    ] config LogStash::Inputs::Stdin/@id = "1b16c8cd7b3b20dfc7c7e978b1ad233146b4bc0c146eab70dc15976197b28091"
[2020-05-26T17:17:58,628][DEBUG][logstash.inputs.stdin    ] config LogStash::Inputs::Stdin/@enable_metric = true
[2020-05-26T17:17:58,644][DEBUG][logstash.inputs.stdin    ] config LogStash::Inputs::Stdin/@codec = <LogStash::Codecs::Line id=>"line_82ec511a-a23b-41ce-94c7-5297b602b505", enable_metric=>true, charset=>"UTF-8", delimiter=>"\n">
[2020-05-26T17:17:58,646][DEBUG][logstash.inputs.stdin    ] config LogStash::Inputs::Stdin/@add_field = {}
[2020-05-26T17:17:58,720][DEBUG][logstash.plugins.registry] On demand adding plugin to the registry {:name=>"grok", :type=>"filter", :class=>LogStash::Filters::Grok}
[2020-05-26T17:17:58,734][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@match = {"message"=>"%{COMBINEDAPACHELOG}"}
[2020-05-26T17:17:58,736][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@id = "509a6b1d2b3b0f807de9cd789e204701a13144926a3ade41642843011405b83e"
[2020-05-26T17:17:58,737][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@enable_metric = true
[2020-05-26T17:17:58,738][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@add_tag = []
[2020-05-26T17:17:58,739][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@remove_tag = []
[2020-05-26T17:17:58,740][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@add_field = {}
[2020-05-26T17:17:58,742][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@remove_field = []
[2020-05-26T17:17:58,750][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@periodic_flush = false
[2020-05-26T17:17:58,753][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@patterns_dir = []
[2020-05-26T17:17:58,755][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@pattern_definitions = {}
[2020-05-26T17:17:58,756][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@patterns_files_glob = "*"
[2020-05-26T17:17:58,757][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@break_on_match = true
[2020-05-26T17:17:58,760][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@named_captures_only = true
[2020-05-26T17:17:58,770][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@keep_empty_captures = false
[2020-05-26T17:17:58,771][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@tag_on_failure = ["_grokparsefailure"]
[2020-05-26T17:17:58,772][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@timeout_millis = 30000
[2020-05-26T17:17:58,773][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@timeout_scope = "pattern"
[2020-05-26T17:17:58,776][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@tag_on_timeout = "_groktimeout"
[2020-05-26T17:17:58,781][DEBUG][logstash.filters.grok    ] config LogStash::Filters::Grok/@overwrite = []
[2020-05-26T17:17:58,841][DEBUG][logstash.plugins.registry] On demand adding plugin to the registry {:name=>"date", :type=>"filter", :class=>LogStash::Filters::Date}
[2020-05-26T17:17:58,854][DEBUG][logstash.filters.date    ] config LogStash::Filters::Date/@match = ["timestamp", "dd/MMM/yyyy:HH:mm:ss Z"]
[2020-05-26T17:17:58,855][DEBUG][logstash.filters.date    ] config LogStash::Filters::Date/@id = "d0dd5bfe67d50aff720a70ec53cd7beb4c5c900ab31aacbc2e36f649937054de"
[2020-05-26T17:17:58,856][DEBUG][logstash.filters.date    ] config LogStash::Filters::Date/@enable_metric = true
[2020-05-26T17:17:58,857][DEBUG][logstash.filters.date    ] config LogStash::Filters::Date/@add_tag = []
[2020-05-26T17:17:58,860][DEBUG][logstash.filters.date    ] config LogStash::Filters::Date/@remove_tag = []
[2020-05-26T17:17:58,862][DEBUG][logstash.filters.date    ] config LogStash::Filters::Date/@add_field = {}
[2020-05-26T17:17:58,863][DEBUG][logstash.filters.date    ] config LogStash::Filters::Date/@remove_field = []
[2020-05-26T17:17:58,871][DEBUG][logstash.filters.date    ] config LogStash::Filters::Date/@periodic_flush = false
[2020-05-26T17:17:58,877][DEBUG][logstash.filters.date    ] config LogStash::Filters::Date/@target = "@timestamp"
[2020-05-26T17:17:58,878][DEBUG][logstash.filters.date    ] config LogStash::Filters::Date/@tag_on_failure = ["_dateparsefailure"]
[2020-05-26T17:17:58,981][DEBUG][org.logstash.filters.DateFilter] Date filter with format=dd/MMM/yyyy:HH:mm:ss Z, locale=null, timezone=null built as org.logstash.filters.parser.JodaParser
[2020-05-26T17:17:58,996][DEBUG][logstash.plugins.registry] On demand adding plugin to the registry {:name=>"elasticsearch", :type=>"output", :class=>LogStash::Outputs::ElasticSearch}
[2020-05-26T17:17:59,021][DEBUG][logstash.plugins.registry] On demand adding plugin to the registry {:name=>"plain", :type=>"codec", :class=>LogStash::Codecs::Plain}
[2020-05-26T17:17:59,034][DEBUG][logstash.codecs.plain    ] config LogStash::Codecs::Plain/@id = "plain_a4dadd9a-314b-426b-b1e4-90698bae234b"
[2020-05-26T17:17:59,035][DEBUG][logstash.codecs.plain    ] config LogStash::Codecs::Plain/@enable_metric = true
[2020-05-26T17:17:59,036][DEBUG][logstash.codecs.plain    ] config LogStash::Codecs::Plain/@charset = "UTF-8"
[2020-05-26T17:17:59,057][DEBUG][logstash.outputs.elasticsearch] config LogStash::Outputs::ElasticSearch/@hosts = [//localhost:9200]
[2020-05-26T17:17:59,058][DEBUG][logstash.outputs.elasticsearch] config LogStash::Outputs::ElasticSearch/@id = "6764790b55f694b02a490b3b4e902e269d8a1031342eaddd8b35d4093435a15b"
[2020-05-26T17:17:59,066][DEBUG][logstash.outputs.elasticsearch] config LogStash::Outputs::ElasticSearch/@enable_metric = true
[2020-05-26T17:17:59,068][DEBUG][logstash.outputs.elasticsearch] config LogStash::Outputs::ElasticSearch/@codec = <LogStash::Codecs::Plain id=>"plain_a4dadd9a-314b-426b-b1e4-90698bae234b", enable_metric=>true, charset=>"UTF-8">
[2020-05-26T17:17:59,072][DEBUG][logstash.outputs.elasticsearch] config LogStash::Outputs::ElasticSearch/@workers = 1
[2020-05-26T17:17:59,076][DEBUG][logstash.outputs.elasticsearch] config LogStash::Outputs::ElasticSearch/@index = "logstash-%{+yyyy.MM.dd}"
[2020-05-26T17:17:59,078][DEBUG][logstash.outputs.elasticsearch] config LogStash::Outputs::ElasticSearch/@manage_template = true
[2020-05-26T17:17:59,079][DEBUG][logstash.outputs.elasticsearch] config LogStash::Outputs::ElasticSearch/@template_name = "logstash"
[2020-05-26T17:17:59,080][DEBUG][logstash.outputs.elasticsearch] config LogStash::Outputs::ElasticSearch/@template_overwrite = false
[2020-05-26T17:17:59,081][DEBUG][logstash.outputs.elasticsearch] config LogStash::Outputs::ElasticSearch/@parent = nil
[2020-05-26T17:17:59,082][DEBUG][logstash.outputs.elasticsearch] config LogStash::Outputs::ElasticSearch/@join_field = nil
[2020-05-26T17:17:59,083][DEBUG][logstash.outputs.elasticsearch] config LogStash::Outputs::ElasticSearch/@upsert = ""
[2020-05-26T17:17:59,084][DEBUG][logstash.outputs.elasticsearch] config LogStash::Outputs::ElasticSearch/@doc_as_upsert = false
[2020-05-26T17:17:59,092][DEBUG][logstash.outputs.elasticsearch] config LogStash::Outputs::ElasticSearch/@script = ""
[2020-05-26T17:17:59,094][DEBUG][logstash.outputs.elasticsearch] config LogStash::Outputs::ElasticSearch/@script_type = "inline"
[2020-05-26T17:17:59,095][DEBUG][logstash.outputs.elasticsearch] config LogStash::Outputs::ElasticSearch/@script_lang = "painless"
[2020-05-26T17:17:59,096][DEBUG][logstash.outputs.elasticsearch] config LogStash::Outputs::ElasticSearch/@script_var_name = "event"
[2020-05-26T17:17:59,097][DEBUG][logstash.outputs.elasticsearch] config LogStash::Outputs::ElasticSearch/@scripted_upsert = false
[2020-05-26T17:17:59,099][DEBUG][logstash.outputs.elasticsearch] config LogStash::Outputs::ElasticSearch/@retry_initial_interval = 2
[2020-05-26T17:17:59,100][DEBUG][logstash.outputs.elasticsearch] config LogStash::Outputs::ElasticSearch/@retry_max_interval = 64
[2020-05-26T17:17:59,108][DEBUG][logstash.outputs.elasticsearch] config LogStash::Outputs::ElasticSearch/@retry_on_conflict = 1
[2020-05-26T17:17:59,109][DEBUG][logstash.outputs.elasticsearch] config LogStash::Outputs::ElasticSearch/@pipeline = nil
[2020-05-26T17:17:59,111][DEBUG][logstash.outputs.elasticsearch] config LogStash::Outputs::ElasticSearch/@ilm_enabled = "auto"
[2020-05-26T17:17:59,112][DEBUG][logstash.outputs.elasticsearch] config LogStash::Outputs::ElasticSearch/@ilm_rollover_alias = "logstash"
[2020-05-26T17:17:59,113][DEBUG][logstash.outputs.elasticsearch] config LogStash::Outputs::ElasticSearch/@ilm_pattern = "{now/d}-000001"
[2020-05-26T17:17:59,114][DEBUG][logstash.outputs.elasticsearch] config LogStash::Outputs::ElasticSearch/@ilm_policy = "logstash-policy"
[2020-05-26T17:17:59,115][DEBUG][logstash.outputs.elasticsearch] config LogStash::Outputs::ElasticSearch/@action = "index"
[2020-05-26T17:17:59,125][DEBUG][logstash.outputs.elasticsearch] config LogStash::Outputs::ElasticSearch/@ssl_certificate_verification = true
[2020-05-26T17:17:59,126][DEBUG][logstash.outputs.elasticsearch] config LogStash::Outputs::ElasticSearch/@sniffing = false
[2020-05-26T17:17:59,127][DEBUG][logstash.outputs.elasticsearch] config LogStash::Outputs::ElasticSearch/@sniffing_delay = 5
[2020-05-26T17:17:59,128][DEBUG][logstash.outputs.elasticsearch] config LogStash::Outputs::ElasticSearch/@timeout = 60
[2020-05-26T17:17:59,129][DEBUG][logstash.outputs.elasticsearch] config LogStash::Outputs::ElasticSearch/@failure_type_logging_whitelist = []
[2020-05-26T17:17:59,130][DEBUG][logstash.outputs.elasticsearch] config LogStash::Outputs::ElasticSearch/@pool_max = 1000
[2020-05-26T17:17:59,132][DEBUG][logstash.outputs.elasticsearch] config LogStash::Outputs::ElasticSearch/@pool_max_per_route = 100
[2020-05-26T17:17:59,141][DEBUG][logstash.outputs.elasticsearch] config LogStash::Outputs::ElasticSearch/@resurrect_delay = 5
[2020-05-26T17:17:59,142][DEBUG][logstash.outputs.elasticsearch] config LogStash::Outputs::ElasticSearch/@validate_after_inactivity = 10000
[2020-05-26T17:17:59,143][DEBUG][logstash.outputs.elasticsearch] config LogStash::Outputs::ElasticSearch/@http_compression = false
[2020-05-26T17:17:59,145][DEBUG][logstash.outputs.elasticsearch] config LogStash::Outputs::ElasticSearch/@custom_headers = {}
[2020-05-26T17:17:59,183][DEBUG][logstash.plugins.registry] On demand adding plugin to the registry {:name=>"stdout", :type=>"output", :class=>LogStash::Outputs::Stdout}
[2020-05-26T17:17:59,214][DEBUG][logstash.plugins.registry] On demand adding plugin to the registry {:name=>"rubydebug", :type=>"codec", :class=>LogStash::Codecs::RubyDebug}
[2020-05-26T17:17:59,223][DEBUG][logstash.codecs.rubydebug] config LogStash::Codecs::RubyDebug/@id = "rubydebug_58b71dd6-a9e0-447d-aede-f0928b46b446"
[2020-05-26T17:17:59,226][DEBUG][logstash.codecs.rubydebug] config LogStash::Codecs::RubyDebug/@enable_metric = true
[2020-05-26T17:17:59,227][DEBUG][logstash.codecs.rubydebug] config LogStash::Codecs::RubyDebug/@metadata = false
[2020-05-26T17:18:00,517][DEBUG][logstash.outputs.stdout  ] config LogStash::Outputs::Stdout/@codec = <LogStash::Codecs::RubyDebug id=>"rubydebug_58b71dd6-a9e0-447d-aede-f0928b46b446", enable_metric=>true, metadata=>false>
[2020-05-26T17:18:00,518][DEBUG][logstash.outputs.stdout  ] config LogStash::Outputs::Stdout/@id = "d642165cea4bde0374aca78118f334e5869abc3c9865feeb2c9f60392af107db"
[2020-05-26T17:18:00,522][DEBUG][logstash.outputs.stdout  ] config LogStash::Outputs::Stdout/@enable_metric = true
[2020-05-26T17:18:00,526][DEBUG][logstash.outputs.stdout  ] config LogStash::Outputs::Stdout/@workers = 1
[2020-05-26T17:18:00,568][DEBUG][logstash.javapipeline    ] Starting pipeline {:pipeline_id=>"main"}
[2020-05-26T17:18:00,622][DEBUG][logstash.outputs.elasticsearch][main] Normalizing http path {:path=>nil, :normalized=>nil}
[2020-05-26T17:18:00,964][INFO ][logstash.outputs.elasticsearch][main] Elasticsearch pool URLs updated {:changes=>{:removed=>[], :added=>[http://localhost:9200/]}}
[2020-05-26T17:18:00,977][DEBUG][logstash.outputs.elasticsearch][main] Running health check to see if an Elasticsearch connection is working {:healthcheck_url=>http://localhost:9200/, :path=>"/"}
[2020-05-26T17:18:01,411][DEBUG][logstash.instrument.periodicpoller.cgroup] One or more required cgroup files or directories not found: /proc/self/cgroup, /sys/fs/cgroup/cpuacct, /sys/fs/cgroup/cpu
[2020-05-26T17:18:01,568][DEBUG][logstash.instrument.periodicpoller.jvm] collector name {:name=>"ParNew"}
[2020-05-26T17:18:01,571][DEBUG][logstash.instrument.periodicpoller.jvm] collector name {:name=>"ConcurrentMarkSweep"}
[2020-05-26T17:18:05,163][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:18:05,187][DEBUG][logstash.outputs.elasticsearch][main] Waiting for connectivity to Elasticsearch cluster. Retrying in 2s
[2020-05-26T17:18:05,188][INFO ][logstash.outputs.elasticsearch][main] New Elasticsearch output {:class=>"LogStash::Outputs::ElasticSearch", :hosts=>["//localhost:9200"]}
[2020-05-26T17:18:05,208][DEBUG][logstash.filters.grok    ][main] Grok patterns path {:paths=>["D:/Softwares/logstash-7.7.0/vendor/bundle/jruby/2.5.0/gems/logstash-patterns-core-4.1.2/patterns", "D:/Softwares/logstash-7.7.0/patterns/*"]}
[2020-05-26T17:18:05,214][DEBUG][logstash.filters.grok    ][main] Grok patterns path {:paths=>[]}
[2020-05-26T17:18:05,219][DEBUG][logstash.filters.grok    ][main] Match data {:match=>{"message"=>"%{COMBINEDAPACHELOG}"}}
[2020-05-26T17:18:05,225][DEBUG][logstash.filters.grok    ][main] regexp: /message {:pattern=>"%{COMBINEDAPACHELOG}"}
[2020-05-26T17:18:05,271][DEBUG][logstash.filters.grok    ][main] Adding pattern {"S3_REQUEST_LINE"=>"(?:%{WORD:verb} %{NOTSPACE:request}(?: HTTP/%{NUMBER:httpversion})?|%{DATA:rawrequest})"}
[2020-05-26T17:18:05,273][DEBUG][logstash.filters.grok    ][main] Adding pattern {"S3_ACCESS_LOG"=>"%{WORD:owner} %{NOTSPACE:bucket} \\[%{HTTPDATE:timestamp}\\] %{IP:clientip} %{NOTSPACE:requester} %{NOTSPACE:request_id} %{NOTSPACE:operation} %{NOTSPACE:key} (?:\"%{S3_REQUEST_LINE}\"|-) (?:%{INT:response:int}|-) (?:-|%{NOTSPACE:error_code}) (?:%{INT:bytes:int}|-) (?:%{INT:object_size:int}|-) (?:%{INT:request_time_ms:int}|-) (?:%{INT:turnaround_time_ms:int}|-) (?:%{QS:referrer}|-) (?:\"?%{QS:agent}\"?|-) (?:-|%{NOTSPACE:version_id})"}
[2020-05-26T17:18:05,280][DEBUG][logstash.filters.grok    ][main] Adding pattern {"ELB_URIPATHPARAM"=>"%{URIPATH:path}(?:%{URIPARAM:params})?"}
[2020-05-26T17:18:05,281][DEBUG][logstash.filters.grok    ][main] Adding pattern {"ELB_URI"=>"%{URIPROTO:proto}://(?:%{USER}(?::[^@]*)?@)?(?:%{URIHOST:urihost})?(?:%{ELB_URIPATHPARAM})?"}
[2020-05-26T17:18:05,283][DEBUG][logstash.filters.grok    ][main] Adding pattern {"ELB_REQUEST_LINE"=>"(?:%{WORD:verb} %{ELB_URI:request}(?: HTTP/%{NUMBER:httpversion})?|%{DATA:rawrequest})"}
[2020-05-26T17:18:05,284][DEBUG][logstash.filters.grok    ][main] Adding pattern {"ELB_ACCESS_LOG"=>"%{TIMESTAMP_ISO8601:timestamp} %{NOTSPACE:elb} %{IP:clientip}:%{INT:clientport:int} (?:(%{IP:backendip}:?:%{INT:backendport:int})|-) %{NUMBER:request_processing_time:float} %{NUMBER:backend_processing_time:float} %{NUMBER:response_processing_time:float} %{INT:response:int} %{INT:backend_response:int} %{INT:received_bytes:int} %{INT:bytes:int} \"%{ELB_REQUEST_LINE}\""}
[2020-05-26T17:18:05,289][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CLOUDFRONT_ACCESS_LOG"=>"(?<timestamp>%{YEAR}-%{MONTHNUM}-%{MONTHDAY}\\t%{TIME})\\t%{WORD:x_edge_location}\\t(?:%{NUMBER:sc_bytes:int}|-)\\t%{IPORHOST:clientip}\\t%{WORD:cs_method}\\t%{HOSTNAME:cs_host}\\t%{NOTSPACE:cs_uri_stem}\\t%{NUMBER:sc_status:int}\\t%{GREEDYDATA:referrer}\\t%{GREEDYDATA:agent}\\t%{GREEDYDATA:cs_uri_query}\\t%{GREEDYDATA:cookies}\\t%{WORD:x_edge_result_type}\\t%{NOTSPACE:x_edge_request_id}\\t%{HOSTNAME:x_host_header}\\t%{URIPROTO:cs_protocol}\\t%{INT:cs_bytes:int}\\t%{GREEDYDATA:time_taken:float}\\t%{GREEDYDATA:x_forwarded_for}\\t%{GREEDYDATA:ssl_protocol}\\t%{GREEDYDATA:ssl_cipher}\\t%{GREEDYDATA:x_edge_response_result_type}"}
[2020-05-26T17:18:05,293][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_TIMESTAMP"=>"%{MONTHDAY}-%{MONTH} %{HOUR}:%{MINUTE}"}
[2020-05-26T17:18:05,294][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_HOST"=>"[a-zA-Z0-9-]+"}
[2020-05-26T17:18:05,296][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_VOLUME"=>"%{USER}"}
[2020-05-26T17:18:05,297][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_DEVICE"=>"%{USER}"}
[2020-05-26T17:18:05,308][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_DEVICEPATH"=>"%{UNIXPATH}"}
[2020-05-26T17:18:05,310][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_CAPACITY"=>"%{INT}{1,3}(,%{INT}{3})*"}
[2020-05-26T17:18:05,311][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_VERSION"=>"%{USER}"}
[2020-05-26T17:18:05,312][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_JOB"=>"%{USER}"}
[2020-05-26T17:18:05,314][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_MAX_CAPACITY"=>"User defined maximum volume capacity %{BACULA_CAPACITY} exceeded on device \\\"%{BACULA_DEVICE:device}\\\" \\(%{BACULA_DEVICEPATH}\\)"}
[2020-05-26T17:18:05,315][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_END_VOLUME"=>"End of medium on Volume \\\"%{BACULA_VOLUME:volume}\\\" Bytes=%{BACULA_CAPACITY} Blocks=%{BACULA_CAPACITY} at %{MONTHDAY}-%{MONTH}-%{YEAR} %{HOUR}:%{MINUTE}."}
[2020-05-26T17:18:05,321][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_NEW_VOLUME"=>"Created new Volume \\\"%{BACULA_VOLUME:volume}\\\" in catalog."}
[2020-05-26T17:18:05,327][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_NEW_LABEL"=>"Labeled new Volume \\\"%{BACULA_VOLUME:volume}\\\" on device \\\"%{BACULA_DEVICE:device}\\\" \\(%{BACULA_DEVICEPATH}\\)."}
[2020-05-26T17:18:05,329][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_WROTE_LABEL"=>"Wrote label to prelabeled Volume \\\"%{BACULA_VOLUME:volume}\\\" on device \\\"%{BACULA_DEVICE}\\\" \\(%{BACULA_DEVICEPATH}\\)"}
[2020-05-26T17:18:05,335][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_NEW_MOUNT"=>"New volume \\\"%{BACULA_VOLUME:volume}\\\" mounted on device \\\"%{BACULA_DEVICE:device}\\\" \\(%{BACULA_DEVICEPATH}\\) at %{MONTHDAY}-%{MONTH}-%{YEAR} %{HOUR}:%{MINUTE}."}
[2020-05-26T17:18:05,338][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_NOOPEN"=>"\\s+Cannot open %{DATA}: ERR=%{GREEDYDATA:berror}"}
[2020-05-26T17:18:05,339][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_NOOPENDIR"=>"\\s+Could not open directory %{DATA}: ERR=%{GREEDYDATA:berror}"}
[2020-05-26T17:18:05,341][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_NOSTAT"=>"\\s+Could not stat %{DATA}: ERR=%{GREEDYDATA:berror}"}
[2020-05-26T17:18:05,343][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_NOJOBS"=>"There are no more Jobs associated with Volume \\\"%{BACULA_VOLUME:volume}\\\". Marking it purged."}
[2020-05-26T17:18:05,349][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_ALL_RECORDS_PRUNED"=>"All records pruned from Volume \\\"%{BACULA_VOLUME:volume}\\\"; marking it \\\"Purged\\\""}
[2020-05-26T17:18:05,353][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_BEGIN_PRUNE_JOBS"=>"Begin pruning Jobs older than %{INT} month %{INT} days ."}
[2020-05-26T17:18:05,354][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_BEGIN_PRUNE_FILES"=>"Begin pruning Files."}
[2020-05-26T17:18:05,355][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_PRUNED_JOBS"=>"Pruned %{INT} Jobs* for client %{BACULA_HOST:client} from catalog."}
[2020-05-26T17:18:05,356][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_PRUNED_FILES"=>"Pruned Files from %{INT} Jobs* for client %{BACULA_HOST:client} from catalog."}
[2020-05-26T17:18:05,363][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_ENDPRUNE"=>"End auto prune."}
[2020-05-26T17:18:05,367][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_STARTJOB"=>"Start Backup JobId %{INT}, Job=%{BACULA_JOB:job}"}
[2020-05-26T17:18:05,368][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_STARTRESTORE"=>"Start Restore Job %{BACULA_JOB:job}"}
[2020-05-26T17:18:05,370][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_USEDEVICE"=>"Using Device \\\"%{BACULA_DEVICE:device}\\\""}
[2020-05-26T17:18:05,371][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_DIFF_FS"=>"\\s+%{UNIXPATH} is a different filesystem. Will not descend from %{UNIXPATH} into it."}
[2020-05-26T17:18:05,376][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_JOBEND"=>"Job write elapsed time = %{DATA:elapsed}, Transfer rate = %{NUMBER} (K|M|G)? Bytes/second"}
[2020-05-26T17:18:05,380][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_NOPRUNE_JOBS"=>"No Jobs found to prune."}
[2020-05-26T17:18:05,381][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_NOPRUNE_FILES"=>"No Files found to prune."}
[2020-05-26T17:18:05,382][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_VOLUME_PREVWRITTEN"=>"Volume \\\"%{BACULA_VOLUME:volume}\\\" previously written, moving to end of data."}
[2020-05-26T17:18:05,383][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_READYAPPEND"=>"Ready to append to end of Volume \\\"%{BACULA_VOLUME:volume}\\\" size=%{INT}"}
[2020-05-26T17:18:05,384][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_CANCELLING"=>"Cancelling duplicate JobId=%{INT}."}
[2020-05-26T17:18:05,391][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_MARKCANCEL"=>"JobId %{INT}, Job %{BACULA_JOB:job} marked to be canceled."}
[2020-05-26T17:18:05,392][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_CLIENT_RBJ"=>"shell command: run ClientRunBeforeJob \\\"%{GREEDYDATA:runjob}\\\""}
[2020-05-26T17:18:05,393][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_VSS"=>"(Generate )?VSS (Writer)?"}
[2020-05-26T17:18:05,395][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_MAXSTART"=>"Fatal error: Job canceled because max start delay time exceeded."}
[2020-05-26T17:18:05,396][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_DUPLICATE"=>"Fatal error: JobId %{INT:duplicate} already running. Duplicate job not allowed."}
[2020-05-26T17:18:05,397][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_NOJOBSTAT"=>"Fatal error: No Job status returned from FD."}
[2020-05-26T17:18:05,398][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_FATAL_CONN"=>"Fatal error: bsock.c:133 Unable to connect to (Client: %{BACULA_HOST:client}|Storage daemon) on %{HOSTNAME}:%{POSINT}. ERR=(?<berror>%{GREEDYDATA})"}
[2020-05-26T17:18:05,399][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_NO_CONNECT"=>"Warning: bsock.c:127 Could not connect to (Client: %{BACULA_HOST:client}|Storage daemon) on %{HOSTNAME}:%{POSINT}. ERR=(?<berror>%{GREEDYDATA})"}
[2020-05-26T17:18:05,408][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_NO_AUTH"=>"Fatal error: Unable to authenticate with File daemon at %{HOSTNAME}. Possible causes:"}
[2020-05-26T17:18:05,409][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_NOSUIT"=>"No prior or suitable Full backup found in catalog. Doing FULL backup."}
[2020-05-26T17:18:05,410][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_NOPRIOR"=>"No prior Full backup Job record found."}
[2020-05-26T17:18:05,412][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOG_JOB"=>"(Error: )?Bacula %{BACULA_HOST} %{BACULA_VERSION} \\(%{BACULA_VERSION}\\):"}
[2020-05-26T17:18:05,413][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BACULA_LOGLINE"=>"%{BACULA_TIMESTAMP:bts} %{BACULA_HOST:hostname} JobId %{INT:jobid}: (%{BACULA_LOG_MAX_CAPACITY}|%{BACULA_LOG_END_VOLUME}|%{BACULA_LOG_NEW_VOLUME}|%{BACULA_LOG_NEW_LABEL}|%{BACULA_LOG_WROTE_LABEL}|%{BACULA_LOG_NEW_MOUNT}|%{BACULA_LOG_NOOPEN}|%{BACULA_LOG_NOOPENDIR}|%{BACULA_LOG_NOSTAT}|%{BACULA_LOG_NOJOBS}|%{BACULA_LOG_ALL_RECORDS_PRUNED}|%{BACULA_LOG_BEGIN_PRUNE_JOBS}|%{BACULA_LOG_BEGIN_PRUNE_FILES}|%{BACULA_LOG_PRUNED_JOBS}|%{BACULA_LOG_PRUNED_FILES}|%{BACULA_LOG_ENDPRUNE}|%{BACULA_LOG_STARTJOB}|%{BACULA_LOG_STARTRESTORE}|%{BACULA_LOG_USEDEVICE}|%{BACULA_LOG_DIFF_FS}|%{BACULA_LOG_JOBEND}|%{BACULA_LOG_NOPRUNE_JOBS}|%{BACULA_LOG_NOPRUNE_FILES}|%{BACULA_LOG_VOLUME_PREVWRITTEN}|%{BACULA_LOG_READYAPPEND}|%{BACULA_LOG_CANCELLING}|%{BACULA_LOG_MARKCANCEL}|%{BACULA_LOG_CLIENT_RBJ}|%{BACULA_LOG_VSS}|%{BACULA_LOG_MAXSTART}|%{BACULA_LOG_DUPLICATE}|%{BACULA_LOG_NOJOBSTAT}|%{BACULA_LOG_FATAL_CONN}|%{BACULA_LOG_NO_CONNECT}|%{BACULA_LOG_NO_AUTH}|%{BACULA_LOG_NOSUIT}|%{BACULA_LOG_JOB}|%{BACULA_LOG_NOPRIOR})"}
[2020-05-26T17:18:05,415][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BIND9_TIMESTAMP"=>"%{MONTHDAY}[-]%{MONTH}[-]%{YEAR} %{TIME}"}
[2020-05-26T17:18:05,417][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BIND9"=>"%{BIND9_TIMESTAMP:timestamp} queries: %{LOGLEVEL:loglevel}: client %{IP:clientip}#%{POSINT:clientport} \\(%{GREEDYDATA:query}\\): query: %{GREEDYDATA:query} IN %{GREEDYDATA:querytype} \\(%{IP:dns}\\)"}
[2020-05-26T17:18:05,426][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BRO_HTTP"=>"%{NUMBER:ts}\\t%{NOTSPACE:uid}\\t%{IP:orig_h}\\t%{INT:orig_p}\\t%{IP:resp_h}\\t%{INT:resp_p}\\t%{INT:trans_depth}\\t%{GREEDYDATA:method}\\t%{GREEDYDATA:domain}\\t%{GREEDYDATA:uri}\\t%{GREEDYDATA:referrer}\\t%{GREEDYDATA:user_agent}\\t%{NUMBER:request_body_len}\\t%{NUMBER:response_body_len}\\t%{GREEDYDATA:status_code}\\t%{GREEDYDATA:status_msg}\\t%{GREEDYDATA:info_code}\\t%{GREEDYDATA:info_msg}\\t%{GREEDYDATA:filename}\\t%{GREEDYDATA:bro_tags}\\t%{GREEDYDATA:username}\\t%{GREEDYDATA:password}\\t%{GREEDYDATA:proxied}\\t%{GREEDYDATA:orig_fuids}\\t%{GREEDYDATA:orig_mime_types}\\t%{GREEDYDATA:resp_fuids}\\t%{GREEDYDATA:resp_mime_types}"}
[2020-05-26T17:18:05,428][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BRO_DNS"=>"%{NUMBER:ts}\\t%{NOTSPACE:uid}\\t%{IP:orig_h}\\t%{INT:orig_p}\\t%{IP:resp_h}\\t%{INT:resp_p}\\t%{WORD:proto}\\t%{INT:trans_id}\\t%{GREEDYDATA:query}\\t%{GREEDYDATA:qclass}\\t%{GREEDYDATA:qclass_name}\\t%{GREEDYDATA:qtype}\\t%{GREEDYDATA:qtype_name}\\t%{GREEDYDATA:rcode}\\t%{GREEDYDATA:rcode_name}\\t%{GREEDYDATA:AA}\\t%{GREEDYDATA:TC}\\t%{GREEDYDATA:RD}\\t%{GREEDYDATA:RA}\\t%{GREEDYDATA:Z}\\t%{GREEDYDATA:answers}\\t%{GREEDYDATA:TTLs}\\t%{GREEDYDATA:rejected}"}
[2020-05-26T17:18:05,429][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BRO_CONN"=>"%{NUMBER:ts}\\t%{NOTSPACE:uid}\\t%{IP:orig_h}\\t%{INT:orig_p}\\t%{IP:resp_h}\\t%{INT:resp_p}\\t%{WORD:proto}\\t%{GREEDYDATA:service}\\t%{NUMBER:duration}\\t%{NUMBER:orig_bytes}\\t%{NUMBER:resp_bytes}\\t%{GREEDYDATA:conn_state}\\t%{GREEDYDATA:local_orig}\\t%{GREEDYDATA:missed_bytes}\\t%{GREEDYDATA:history}\\t%{GREEDYDATA:orig_pkts}\\t%{GREEDYDATA:orig_ip_bytes}\\t%{GREEDYDATA:resp_pkts}\\t%{GREEDYDATA:resp_ip_bytes}\\t%{GREEDYDATA:tunnel_parents}"}
[2020-05-26T17:18:05,431][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BRO_FILES"=>"%{NUMBER:ts}\\t%{NOTSPACE:fuid}\\t%{IP:tx_hosts}\\t%{IP:rx_hosts}\\t%{NOTSPACE:conn_uids}\\t%{GREEDYDATA:source}\\t%{GREEDYDATA:depth}\\t%{GREEDYDATA:analyzers}\\t%{GREEDYDATA:mime_type}\\t%{GREEDYDATA:filename}\\t%{GREEDYDATA:duration}\\t%{GREEDYDATA:local_orig}\\t%{GREEDYDATA:is_orig}\\t%{GREEDYDATA:seen_bytes}\\t%{GREEDYDATA:total_bytes}\\t%{GREEDYDATA:missing_bytes}\\t%{GREEDYDATA:overflow_bytes}\\t%{GREEDYDATA:timedout}\\t%{GREEDYDATA:parent_fuid}\\t%{GREEDYDATA:md5}\\t%{GREEDYDATA:sha1}\\t%{GREEDYDATA:sha256}\\t%{GREEDYDATA:extracted}"}
[2020-05-26T17:18:05,433][DEBUG][logstash.filters.grok    ][main] Adding pattern {"EXIM_MSGID"=>"[0-9A-Za-z]{6}-[0-9A-Za-z]{6}-[0-9A-Za-z]{2}"}
[2020-05-26T17:18:05,441][DEBUG][logstash.filters.grok    ][main] Adding pattern {"EXIM_FLAGS"=>"(<=|[-=>*]>|[*]{2}|==)"}
[2020-05-26T17:18:05,442][DEBUG][logstash.filters.grok    ][main] Adding pattern {"EXIM_DATE"=>"%{YEAR:exim_year}-%{MONTHNUM:exim_month}-%{MONTHDAY:exim_day} %{TIME:exim_time}"}
[2020-05-26T17:18:05,444][DEBUG][logstash.filters.grok    ][main] Adding pattern {"EXIM_PID"=>"\\[%{POSINT}\\]"}
[2020-05-26T17:18:05,445][DEBUG][logstash.filters.grok    ][main] Adding pattern {"EXIM_QT"=>"((\\d+y)?(\\d+w)?(\\d+d)?(\\d+h)?(\\d+m)?(\\d+s)?)"}
[2020-05-26T17:18:05,447][DEBUG][logstash.filters.grok    ][main] Adding pattern {"EXIM_EXCLUDE_TERMS"=>"(Message is frozen|(Start|End) queue run| Warning: | retry time not reached | no (IP address|host name) found for (IP address|host) | unexpected disconnection while reading SMTP command | no immediate delivery: |another process is handling this message)"}
[2020-05-26T17:18:05,448][DEBUG][logstash.filters.grok    ][main] Adding pattern {"EXIM_REMOTE_HOST"=>"(H=(%{NOTSPACE:remote_hostname} )?(\\(%{NOTSPACE:remote_heloname}\\) )?\\[%{IP:remote_host}\\])"}
[2020-05-26T17:18:05,449][DEBUG][logstash.filters.grok    ][main] Adding pattern {"EXIM_INTERFACE"=>"(I=\\[%{IP:exim_interface}\\](:%{NUMBER:exim_interface_port}))"}
[2020-05-26T17:18:05,458][DEBUG][logstash.filters.grok    ][main] Adding pattern {"EXIM_PROTOCOL"=>"(P=%{NOTSPACE:protocol})"}
[2020-05-26T17:18:05,459][DEBUG][logstash.filters.grok    ][main] Adding pattern {"EXIM_MSG_SIZE"=>"(S=%{NUMBER:exim_msg_size})"}
[2020-05-26T17:18:05,461][DEBUG][logstash.filters.grok    ][main] Adding pattern {"EXIM_HEADER_ID"=>"(id=%{NOTSPACE:exim_header_id})"}
[2020-05-26T17:18:05,463][DEBUG][logstash.filters.grok    ][main] Adding pattern {"EXIM_SUBJECT"=>"(T=%{QS:exim_subject})"}
[2020-05-26T17:18:05,464][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NETSCREENSESSIONLOG"=>"%{SYSLOGTIMESTAMP:date} %{IPORHOST:device} %{IPORHOST}: NetScreen device_id=%{WORD:device_id}%{DATA}: start_time=%{QUOTEDSTRING:start_time} duration=%{INT:duration} policy_id=%{INT:policy_id} service=%{DATA:service} proto=%{INT:proto} src zone=%{WORD:src_zone} dst zone=%{WORD:dst_zone} action=%{WORD:action} sent=%{INT:sent} rcvd=%{INT:rcvd} src=%{IPORHOST:src_ip} dst=%{IPORHOST:dst_ip} src_port=%{INT:src_port} dst_port=%{INT:dst_port} src-xlated ip=%{IPORHOST:src_xlated_ip} port=%{INT:src_xlated_port} dst-xlated ip=%{IPORHOST:dst_xlated_ip} port=%{INT:dst_xlated_port} session_id=%{INT:session_id} reason=%{GREEDYDATA:reason}"}
[2020-05-26T17:18:05,466][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCO_TAGGED_SYSLOG"=>"^<%{POSINT:syslog_pri}>%{CISCOTIMESTAMP:timestamp}( %{SYSLOGHOST:sysloghost})? ?: %%{CISCOTAG:ciscotag}:"}
[2020-05-26T17:18:05,474][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOTIMESTAMP"=>"%{MONTH} +%{MONTHDAY}(?: %{YEAR})? %{TIME}"}
[2020-05-26T17:18:05,476][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOTAG"=>"[A-Z0-9]+-%{INT}-(?:[A-Z0-9_]+)"}
[2020-05-26T17:18:05,477][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCO_ACTION"=>"Built|Teardown|Deny|Denied|denied|requested|permitted|denied by ACL|discarded|est-allowed|Dropping|created|deleted"}
[2020-05-26T17:18:05,479][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCO_REASON"=>"Duplicate TCP SYN|Failed to locate egress interface|Invalid transport field|No matching connection|DNS Response|DNS Query|(?:%{WORD}\\s*)*"}
[2020-05-26T17:18:05,480][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCO_DIRECTION"=>"Inbound|inbound|Outbound|outbound"}
[2020-05-26T17:18:05,481][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCO_INTERVAL"=>"first hit|%{INT}-second interval"}
[2020-05-26T17:18:05,482][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCO_XLATE_TYPE"=>"static|dynamic"}
[2020-05-26T17:18:05,491][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOFW104001"=>"\\((?:Primary|Secondary)\\) Switching to ACTIVE - %{GREEDYDATA:switch_reason}"}
[2020-05-26T17:18:05,493][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOFW104002"=>"\\((?:Primary|Secondary)\\) Switching to STANDBY - %{GREEDYDATA:switch_reason}"}
[2020-05-26T17:18:05,494][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOFW104003"=>"\\((?:Primary|Secondary)\\) Switching to FAILED\\."}
[2020-05-26T17:18:05,495][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOFW104004"=>"\\((?:Primary|Secondary)\\) Switching to OK\\."}
[2020-05-26T17:18:05,496][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOFW105003"=>"\\((?:Primary|Secondary)\\) Monitoring on [Ii]nterface %{GREEDYDATA:interface_name} waiting"}
[2020-05-26T17:18:05,498][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOFW105004"=>"\\((?:Primary|Secondary)\\) Monitoring on [Ii]nterface %{GREEDYDATA:interface_name} normal"}
[2020-05-26T17:18:05,499][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOFW105005"=>"\\((?:Primary|Secondary)\\) Lost Failover communications with mate on [Ii]nterface %{GREEDYDATA:interface_name}"}
[2020-05-26T17:18:05,508][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOFW105008"=>"\\((?:Primary|Secondary)\\) Testing [Ii]nterface %{GREEDYDATA:interface_name}"}
[2020-05-26T17:18:05,510][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOFW105009"=>"\\((?:Primary|Secondary)\\) Testing on [Ii]nterface %{GREEDYDATA:interface_name} (?:Passed|Failed)"}
[2020-05-26T17:18:05,511][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOFW106001"=>"%{CISCO_DIRECTION:direction} %{WORD:protocol} connection %{CISCO_ACTION:action} from %{IP:src_ip}/%{INT:src_port} to %{IP:dst_ip}/%{INT:dst_port} flags %{GREEDYDATA:tcp_flags} on interface %{GREEDYDATA:interface}"}
[2020-05-26T17:18:05,513][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOFW106006_106007_106010"=>"%{CISCO_ACTION:action} %{CISCO_DIRECTION:direction} %{WORD:protocol} (?:from|src) %{IP:src_ip}/%{INT:src_port}(\\(%{DATA:src_fwuser}\\))? (?:to|dst) %{IP:dst_ip}/%{INT:dst_port}(\\(%{DATA:dst_fwuser}\\))? (?:on interface %{DATA:interface}|due to %{CISCO_REASON:reason})"}
[2020-05-26T17:18:05,515][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOFW106014"=>"%{CISCO_ACTION:action} %{CISCO_DIRECTION:direction} %{WORD:protocol} src %{DATA:src_interface}:%{IP:src_ip}(\\(%{DATA:src_fwuser}\\))? dst %{DATA:dst_interface}:%{IP:dst_ip}(\\(%{DATA:dst_fwuser}\\))? \\(type %{INT:icmp_type}, code %{INT:icmp_code}\\)"}
[2020-05-26T17:18:05,516][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOFW106015"=>"%{CISCO_ACTION:action} %{WORD:protocol} \\(%{DATA:policy_id}\\) from %{IP:src_ip}/%{INT:src_port} to %{IP:dst_ip}/%{INT:dst_port} flags %{DATA:tcp_flags} on interface %{GREEDYDATA:interface}"}
[2020-05-26T17:18:05,525][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOFW106021"=>"%{CISCO_ACTION:action} %{WORD:protocol} reverse path check from %{IP:src_ip} to %{IP:dst_ip} on interface %{GREEDYDATA:interface}"}
[2020-05-26T17:18:05,526][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOFW106023"=>"%{CISCO_ACTION:action}( protocol)? %{WORD:protocol} src %{DATA:src_interface}:%{DATA:src_ip}(/%{INT:src_port})?(\\(%{DATA:src_fwuser}\\))? dst %{DATA:dst_interface}:%{DATA:dst_ip}(/%{INT:dst_port})?(\\(%{DATA:dst_fwuser}\\))?( \\(type %{INT:icmp_type}, code %{INT:icmp_code}\\))? by access-group \"?%{DATA:policy_id}\"? \\[%{DATA:hashcode1}, %{DATA:hashcode2}\\]"}
[2020-05-26T17:18:05,529][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOFW106100_2_3"=>"access-list %{NOTSPACE:policy_id} %{CISCO_ACTION:action} %{WORD:protocol} for user '%{DATA:src_fwuser}' %{DATA:src_interface}/%{IP:src_ip}\\(%{INT:src_port}\\) -> %{DATA:dst_interface}/%{IP:dst_ip}\\(%{INT:dst_port}\\) hit-cnt %{INT:hit_count} %{CISCO_INTERVAL:interval} \\[%{DATA:hashcode1}, %{DATA:hashcode2}\\]"}
[2020-05-26T17:18:05,531][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOFW106100"=>"access-list %{NOTSPACE:policy_id} %{CISCO_ACTION:action} %{WORD:protocol} %{DATA:src_interface}/%{IP:src_ip}\\(%{INT:src_port}\\)(\\(%{DATA:src_fwuser}\\))? -> %{DATA:dst_interface}/%{IP:dst_ip}\\(%{INT:dst_port}\\)(\\(%{DATA:src_fwuser}\\))? hit-cnt %{INT:hit_count} %{CISCO_INTERVAL:interval} \\[%{DATA:hashcode1}, %{DATA:hashcode2}\\]"}
[2020-05-26T17:18:05,532][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOFW304001"=>"%{IP:src_ip}(\\(%{DATA:src_fwuser}\\))? Accessed URL %{IP:dst_ip}:%{GREEDYDATA:dst_url}"}
[2020-05-26T17:18:05,534][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOFW110002"=>"%{CISCO_REASON:reason} for %{WORD:protocol} from %{DATA:src_interface}:%{IP:src_ip}/%{INT:src_port} to %{IP:dst_ip}/%{INT:dst_port}"}
[2020-05-26T17:18:05,542][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOFW302010"=>"%{INT:connection_count} in use, %{INT:connection_count_max} most used"}
[2020-05-26T17:18:05,543][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOFW302013_302014_302015_302016"=>"%{CISCO_ACTION:action}(?: %{CISCO_DIRECTION:direction})? %{WORD:protocol} connection %{INT:connection_id} for %{DATA:src_interface}:%{IP:src_ip}/%{INT:src_port}( \\(%{IP:src_mapped_ip}/%{INT:src_mapped_port}\\))?(\\(%{DATA:src_fwuser}\\))? to %{DATA:dst_interface}:%{IP:dst_ip}/%{INT:dst_port}( \\(%{IP:dst_mapped_ip}/%{INT:dst_mapped_port}\\))?(\\(%{DATA:dst_fwuser}\\))?( duration %{TIME:duration} bytes %{INT:bytes})?(?: %{CISCO_REASON:reason})?( \\(%{DATA:user}\\))?"}
[2020-05-26T17:18:05,545][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOFW302020_302021"=>"%{CISCO_ACTION:action}(?: %{CISCO_DIRECTION:direction})? %{WORD:protocol} connection for faddr %{IP:dst_ip}/%{INT:icmp_seq_num}(?:\\(%{DATA:fwuser}\\))? gaddr %{IP:src_xlated_ip}/%{INT:icmp_code_xlated} laddr %{IP:src_ip}/%{INT:icmp_code}( \\(%{DATA:user}\\))?"}
[2020-05-26T17:18:05,546][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOFW305011"=>"%{CISCO_ACTION:action} %{CISCO_XLATE_TYPE:xlate_type} %{WORD:protocol} translation from %{DATA:src_interface}:%{IP:src_ip}(/%{INT:src_port})?(\\(%{DATA:src_fwuser}\\))? to %{DATA:src_xlated_interface}:%{IP:src_xlated_ip}/%{DATA:src_xlated_port}"}
[2020-05-26T17:18:05,547][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOFW313001_313004_313008"=>"%{CISCO_ACTION:action} %{WORD:protocol} type=%{INT:icmp_type}, code=%{INT:icmp_code} from %{IP:src_ip} on interface %{DATA:interface}( to %{IP:dst_ip})?"}
[2020-05-26T17:18:05,548][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOFW313005"=>"%{CISCO_REASON:reason} for %{WORD:protocol} error message: %{WORD:err_protocol} src %{DATA:err_src_interface}:%{IP:err_src_ip}(\\(%{DATA:err_src_fwuser}\\))? dst %{DATA:err_dst_interface}:%{IP:err_dst_ip}(\\(%{DATA:err_dst_fwuser}\\))? \\(type %{INT:err_icmp_type}, code %{INT:err_icmp_code}\\) on %{DATA:interface} interface\\.  Original IP payload: %{WORD:protocol} src %{IP:orig_src_ip}/%{INT:orig_src_port}(\\(%{DATA:orig_src_fwuser}\\))? dst %{IP:orig_dst_ip}/%{INT:orig_dst_port}(\\(%{DATA:orig_dst_fwuser}\\))?"}
[2020-05-26T17:18:05,550][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOFW321001"=>"Resource '%{WORD:resource_name}' limit of %{POSINT:resource_limit} reached for system"}
[2020-05-26T17:18:05,559][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOFW402117"=>"%{WORD:protocol}: Received a non-IPSec packet \\(protocol= %{WORD:orig_protocol}\\) from %{IP:src_ip} to %{IP:dst_ip}"}
[2020-05-26T17:18:05,561][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOFW402119"=>"%{WORD:protocol}: Received an %{WORD:orig_protocol} packet \\(SPI= %{DATA:spi}, sequence number= %{DATA:seq_num}\\) from %{IP:src_ip} \\(user= %{DATA:user}\\) to %{IP:dst_ip} that failed anti-replay checking"}
[2020-05-26T17:18:05,562][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOFW419001"=>"%{CISCO_ACTION:action} %{WORD:protocol} packet from %{DATA:src_interface}:%{IP:src_ip}/%{INT:src_port} to %{DATA:dst_interface}:%{IP:dst_ip}/%{INT:dst_port}, reason: %{GREEDYDATA:reason}"}
[2020-05-26T17:18:05,563][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOFW419002"=>"%{CISCO_REASON:reason} from %{DATA:src_interface}:%{IP:src_ip}/%{INT:src_port} to %{DATA:dst_interface}:%{IP:dst_ip}/%{INT:dst_port} with different initial sequence number"}
[2020-05-26T17:18:05,564][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOFW500004"=>"%{CISCO_REASON:reason} for protocol=%{WORD:protocol}, from %{IP:src_ip}/%{INT:src_port} to %{IP:dst_ip}/%{INT:dst_port}"}
[2020-05-26T17:18:05,565][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOFW602303_602304"=>"%{WORD:protocol}: An %{CISCO_DIRECTION:direction} %{GREEDYDATA:tunnel_type} SA \\(SPI= %{DATA:spi}\\) between %{IP:src_ip} and %{IP:dst_ip} \\(user= %{DATA:user}\\) has been %{CISCO_ACTION:action}"}
[2020-05-26T17:18:05,575][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOFW710001_710002_710003_710005_710006"=>"%{WORD:protocol} (?:request|access) %{CISCO_ACTION:action} from %{IP:src_ip}/%{INT:src_port} to %{DATA:dst_interface}:%{IP:dst_ip}/%{INT:dst_port}"}
[2020-05-26T17:18:05,576][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOFW713172"=>"Group = %{GREEDYDATA:group}, IP = %{IP:src_ip}, Automatic NAT Detection Status:\\s+Remote end\\s*%{DATA:is_remote_natted}\\s*behind a NAT device\\s+This\\s+end\\s*%{DATA:is_local_natted}\\s*behind a NAT device"}
[2020-05-26T17:18:05,578][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOFW733100"=>"\\[\\s*%{DATA:drop_type}\\s*\\] drop %{DATA:drop_rate_id} exceeded. Current burst rate is %{INT:drop_rate_current_burst} per second, max configured rate is %{INT:drop_rate_max_burst}; Current average rate is %{INT:drop_rate_current_avg} per second, max configured rate is %{INT:drop_rate_max_avg}; Cumulative total count is %{INT:drop_total_count}"}
[2020-05-26T17:18:05,579][DEBUG][logstash.filters.grok    ][main] Adding pattern {"SHOREWALL"=>"(%{SYSLOGTIMESTAMP:timestamp}) (%{WORD:nf_host}) kernel:.*Shorewall:(%{WORD:nf_action1})?:(%{WORD:nf_action2})?.*IN=(%{USERNAME:nf_in_interface})?.*(OUT= *MAC=(%{COMMONMAC:nf_dst_mac}):(%{COMMONMAC:nf_src_mac})?|OUT=%{USERNAME:nf_out_interface}).*SRC=(%{IPV4:nf_src_ip}).*DST=(%{IPV4:nf_dst_ip}).*LEN=(%{WORD:nf_len}).?*TOS=(%{WORD:nf_tos}).?*PREC=(%{WORD:nf_prec}).?*TTL=(%{INT:nf_ttl}).?*ID=(%{INT:nf_id}).?*PROTO=(%{WORD:nf_protocol}).?*SPT=(%{INT:nf_src_port}?.*DPT=%{INT:nf_dst_port}?.*)"}
[2020-05-26T17:18:05,581][DEBUG][logstash.filters.grok    ][main] Adding pattern {"SFW2"=>"((%{SYSLOGTIMESTAMP})|(%{TIMESTAMP_ISO8601}))\\s*%{HOSTNAME}\\s*kernel\\S+\\s*%{NAGIOSTIME}\\s*SFW2\\-INext\\-%{NOTSPACE:nf_action}\\s*IN=%{USERNAME:nf_in_interface}.*OUT=((\\s*%{USERNAME:nf_out_interface})|(\\s*))MAC=((%{COMMONMAC:nf_dst_mac}:%{COMMONMAC:nf_src_mac})|(\\s*)).*SRC=%{IP:nf_src_ip}\\s*DST=%{IP:nf_dst_ip}.*PROTO=%{WORD:nf_protocol}((.*SPT=%{INT:nf_src_port}.*DPT=%{INT:nf_dst_port}.*)|())"}
[2020-05-26T17:18:05,583][DEBUG][logstash.filters.grok    ][main] Adding pattern {"USERNAME"=>"[a-zA-Z0-9._-]+"}
[2020-05-26T17:18:05,592][DEBUG][logstash.filters.grok    ][main] Adding pattern {"USER"=>"%{USERNAME}"}
[2020-05-26T17:18:05,593][DEBUG][logstash.filters.grok    ][main] Adding pattern {"EMAILLOCALPART"=>"[a-zA-Z][a-zA-Z0-9_.+-=:]+"}
[2020-05-26T17:18:05,594][DEBUG][logstash.filters.grok    ][main] Adding pattern {"EMAILADDRESS"=>"%{EMAILLOCALPART}@%{HOSTNAME}"}
[2020-05-26T17:18:05,595][DEBUG][logstash.filters.grok    ][main] Adding pattern {"INT"=>"(?:[+-]?(?:[0-9]+))"}
[2020-05-26T17:18:05,596][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BASE10NUM"=>"(?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\\.[0-9]+)?)|(?:\\.[0-9]+)))"}
[2020-05-26T17:18:05,597][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NUMBER"=>"(?:%{BASE10NUM})"}
[2020-05-26T17:18:05,598][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BASE16NUM"=>"(?<![0-9A-Fa-f])(?:[+-]?(?:0x)?(?:[0-9A-Fa-f]+))"}
[2020-05-26T17:18:05,599][DEBUG][logstash.filters.grok    ][main] Adding pattern {"BASE16FLOAT"=>"\\b(?<![0-9A-Fa-f.])(?:[+-]?(?:0x)?(?:(?:[0-9A-Fa-f]+(?:\\.[0-9A-Fa-f]*)?)|(?:\\.[0-9A-Fa-f]+)))\\b"}
[2020-05-26T17:18:05,608][DEBUG][logstash.filters.grok    ][main] Adding pattern {"POSINT"=>"\\b(?:[1-9][0-9]*)\\b"}
[2020-05-26T17:18:05,609][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NONNEGINT"=>"\\b(?:[0-9]+)\\b"}
[2020-05-26T17:18:05,610][DEBUG][logstash.filters.grok    ][main] Adding pattern {"WORD"=>"\\b\\w+\\b"}
[2020-05-26T17:18:05,611][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NOTSPACE"=>"\\S+"}
[2020-05-26T17:18:05,612][DEBUG][logstash.filters.grok    ][main] Adding pattern {"SPACE"=>"\\s*"}
[2020-05-26T17:18:05,613][DEBUG][logstash.filters.grok    ][main] Adding pattern {"DATA"=>".*?"}
[2020-05-26T17:18:05,614][DEBUG][logstash.filters.grok    ][main] Adding pattern {"GREEDYDATA"=>".*"}
[2020-05-26T17:18:05,615][DEBUG][logstash.filters.grok    ][main] Adding pattern {"QUOTEDSTRING"=>"(?>(?<!\\\\)(?>\"(?>\\\\.|[^\\\\\"]+)+\"|\"\"|(?>'(?>\\\\.|[^\\\\']+)+')|''|(?>`(?>\\\\.|[^\\\\`]+)+`)|``))"}
[2020-05-26T17:18:05,616][DEBUG][logstash.filters.grok    ][main] Adding pattern {"UUID"=>"[A-Fa-f0-9]{8}-(?:[A-Fa-f0-9]{4}-){3}[A-Fa-f0-9]{12}"}
[2020-05-26T17:18:05,625][DEBUG][logstash.filters.grok    ][main] Adding pattern {"URN"=>"urn:[0-9A-Za-z][0-9A-Za-z-]{0,31}:(?:%[0-9a-fA-F]{2}|[0-9A-Za-z()+,.:=@;$_!*'/?#-])+"}
[2020-05-26T17:18:05,626][DEBUG][logstash.filters.grok    ][main] Adding pattern {"MAC"=>"(?:%{CISCOMAC}|%{WINDOWSMAC}|%{COMMONMAC})"}
[2020-05-26T17:18:05,627][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CISCOMAC"=>"(?:(?:[A-Fa-f0-9]{4}\\.){2}[A-Fa-f0-9]{4})"}
[2020-05-26T17:18:05,628][DEBUG][logstash.filters.grok    ][main] Adding pattern {"WINDOWSMAC"=>"(?:(?:[A-Fa-f0-9]{2}-){5}[A-Fa-f0-9]{2})"}
[2020-05-26T17:18:05,629][DEBUG][logstash.filters.grok    ][main] Adding pattern {"COMMONMAC"=>"(?:(?:[A-Fa-f0-9]{2}:){5}[A-Fa-f0-9]{2})"}
[2020-05-26T17:18:05,630][DEBUG][logstash.filters.grok    ][main] Adding pattern {"IPV6"=>"((([0-9A-Fa-f]{1,4}:){7}([0-9A-Fa-f]{1,4}|:))|(([0-9A-Fa-f]{1,4}:){6}(:[0-9A-Fa-f]{1,4}|((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3})|:))|(([0-9A-Fa-f]{1,4}:){5}(((:[0-9A-Fa-f]{1,4}){1,2})|:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3})|:))|(([0-9A-Fa-f]{1,4}:){4}(((:[0-9A-Fa-f]{1,4}){1,3})|((:[0-9A-Fa-f]{1,4})?:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:))|(([0-9A-Fa-f]{1,4}:){3}(((:[0-9A-Fa-f]{1,4}){1,4})|((:[0-9A-Fa-f]{1,4}){0,2}:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:))|(([0-9A-Fa-f]{1,4}:){2}(((:[0-9A-Fa-f]{1,4}){1,5})|((:[0-9A-Fa-f]{1,4}){0,3}:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:))|(([0-9A-Fa-f]{1,4}:){1}(((:[0-9A-Fa-f]{1,4}){1,6})|((:[0-9A-Fa-f]{1,4}){0,4}:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:))|(:(((:[0-9A-Fa-f]{1,4}){1,7})|((:[0-9A-Fa-f]{1,4}){0,5}:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:)))(%.+)?"}
[2020-05-26T17:18:05,632][DEBUG][logstash.filters.grok    ][main] Adding pattern {"IPV4"=>"(?<![0-9])(?:(?:[0-1]?[0-9]{1,2}|2[0-4][0-9]|25[0-5])[.](?:[0-1]?[0-9]{1,2}|2[0-4][0-9]|25[0-5])[.](?:[0-1]?[0-9]{1,2}|2[0-4][0-9]|25[0-5])[.](?:[0-1]?[0-9]{1,2}|2[0-4][0-9]|25[0-5]))(?![0-9])"}
[2020-05-26T17:18:05,633][DEBUG][logstash.filters.grok    ][main] Adding pattern {"IP"=>"(?:%{IPV6}|%{IPV4})"}
[2020-05-26T17:18:05,644][DEBUG][logstash.filters.grok    ][main] Adding pattern {"HOSTNAME"=>"\\b(?:[0-9A-Za-z][0-9A-Za-z-]{0,62})(?:\\.(?:[0-9A-Za-z][0-9A-Za-z-]{0,62}))*(\\.?|\\b)"}
[2020-05-26T17:18:05,646][DEBUG][logstash.filters.grok    ][main] Adding pattern {"IPORHOST"=>"(?:%{IP}|%{HOSTNAME})"}
[2020-05-26T17:18:05,647][DEBUG][logstash.filters.grok    ][main] Adding pattern {"HOSTPORT"=>"%{IPORHOST}:%{POSINT}"}
[2020-05-26T17:18:05,648][DEBUG][logstash.filters.grok    ][main] Adding pattern {"PATH"=>"(?:%{UNIXPATH}|%{WINPATH})"}
[2020-05-26T17:18:05,650][DEBUG][logstash.filters.grok    ][main] Adding pattern {"UNIXPATH"=>"(/([\\w_%!$@:.,+~-]+|\\\\.)*)+"}
[2020-05-26T17:18:05,651][DEBUG][logstash.filters.grok    ][main] Adding pattern {"TTY"=>"(?:/dev/(pts|tty([pq])?)(\\w+)?/?(?:[0-9]+))"}
[2020-05-26T17:18:05,652][DEBUG][logstash.filters.grok    ][main] Adding pattern {"WINPATH"=>"(?>[A-Za-z]+:|\\\\)(?:\\\\[^\\\\?*]*)+"}
[2020-05-26T17:18:05,661][DEBUG][logstash.filters.grok    ][main] Adding pattern {"URIPROTO"=>"[A-Za-z]([A-Za-z0-9+\\-.]+)+"}
[2020-05-26T17:18:05,663][DEBUG][logstash.filters.grok    ][main] Adding pattern {"URIHOST"=>"%{IPORHOST}(?::%{POSINT:port})?"}
[2020-05-26T17:18:05,665][DEBUG][logstash.filters.grok    ][main] Adding pattern {"URIPATH"=>"(?:/[A-Za-z0-9$.+!*'(){},~:;=@#%&_\\-]*)+"}
[2020-05-26T17:18:05,666][DEBUG][logstash.filters.grok    ][main] Adding pattern {"URIPARAM"=>"\\?[A-Za-z0-9$.+!*'|(){},~@#%&/=:;_?\\-\\[\\]<>]*"}
[2020-05-26T17:18:05,667][DEBUG][logstash.filters.grok    ][main] Adding pattern {"URIPATHPARAM"=>"%{URIPATH}(?:%{URIPARAM})?"}
[2020-05-26T17:18:05,669][DEBUG][logstash.filters.grok    ][main] Adding pattern {"URI"=>"%{URIPROTO}://(?:%{USER}(?::[^@]*)?@)?(?:%{URIHOST})?(?:%{URIPATHPARAM})?"}
[2020-05-26T17:18:05,680][DEBUG][logstash.filters.grok    ][main] Adding pattern {"MONTH"=>"\\b(?:[Jj]an(?:uary|uar)?|[Ff]eb(?:ruary|ruar)?|[Mm](?:a|)?r(?:ch|z)?|[Aa]pr(?:il)?|[Mm]a(?:y|i)?|[Jj]un(?:e|i)?|[Jj]ul(?:y)?|[Aa]ug(?:ust)?|[Ss]ep(?:tember)?|[Oo](?:c|k)?t(?:ober)?|[Nn]ov(?:ember)?|[Dd]e(?:c|z)(?:ember)?)\\b"}
[2020-05-26T17:18:05,682][DEBUG][logstash.filters.grok    ][main] Adding pattern {"MONTHNUM"=>"(?:0?[1-9]|1[0-2])"}
[2020-05-26T17:18:05,683][DEBUG][logstash.filters.grok    ][main] Adding pattern {"MONTHNUM2"=>"(?:0[1-9]|1[0-2])"}
[2020-05-26T17:18:05,689][DEBUG][logstash.filters.grok    ][main] Adding pattern {"MONTHDAY"=>"(?:(?:0[1-9])|(?:[12][0-9])|(?:3[01])|[1-9])"}
[2020-05-26T17:18:05,694][DEBUG][logstash.filters.grok    ][main] Adding pattern {"DAY"=>"(?:Mon(?:day)?|Tue(?:sday)?|Wed(?:nesday)?|Thu(?:rsday)?|Fri(?:day)?|Sat(?:urday)?|Sun(?:day)?)"}
[2020-05-26T17:18:05,695][DEBUG][logstash.filters.grok    ][main] Adding pattern {"YEAR"=>"(?>\\d\\d){1,2}"}
[2020-05-26T17:18:05,696][DEBUG][logstash.filters.grok    ][main] Adding pattern {"HOUR"=>"(?:2[0123]|[01]?[0-9])"}
[2020-05-26T17:18:05,697][DEBUG][logstash.filters.grok    ][main] Adding pattern {"MINUTE"=>"(?:[0-5][0-9])"}
[2020-05-26T17:18:05,703][DEBUG][logstash.filters.grok    ][main] Adding pattern {"SECOND"=>"(?:(?:[0-5]?[0-9]|60)(?:[:.,][0-9]+)?)"}
[2020-05-26T17:18:05,708][DEBUG][logstash.filters.grok    ][main] Adding pattern {"TIME"=>"(?!<[0-9])%{HOUR}:%{MINUTE}(?::%{SECOND})(?![0-9])"}
[2020-05-26T17:18:05,708][DEBUG][logstash.filters.grok    ][main] Adding pattern {"DATE_US"=>"%{MONTHNUM}[/-]%{MONTHDAY}[/-]%{YEAR}"}
[2020-05-26T17:18:05,710][DEBUG][logstash.filters.grok    ][main] Adding pattern {"DATE_EU"=>"%{MONTHDAY}[./-]%{MONTHNUM}[./-]%{YEAR}"}
[2020-05-26T17:18:05,713][DEBUG][logstash.filters.grok    ][main] Adding pattern {"ISO8601_TIMEZONE"=>"(?:Z|[+-]%{HOUR}(?::?%{MINUTE}))"}
[2020-05-26T17:18:05,720][DEBUG][logstash.filters.grok    ][main] Adding pattern {"ISO8601_SECOND"=>"(?:%{SECOND}|60)"}
[2020-05-26T17:18:05,722][DEBUG][logstash.filters.grok    ][main] Adding pattern {"TIMESTAMP_ISO8601"=>"%{YEAR}-%{MONTHNUM}-%{MONTHDAY}[T ]%{HOUR}:?%{MINUTE}(?::?%{SECOND})?%{ISO8601_TIMEZONE}?"}
[2020-05-26T17:18:05,723][DEBUG][logstash.filters.grok    ][main] Adding pattern {"DATE"=>"%{DATE_US}|%{DATE_EU}"}
[2020-05-26T17:18:05,725][DEBUG][logstash.filters.grok    ][main] Adding pattern {"DATESTAMP"=>"%{DATE}[- ]%{TIME}"}
[2020-05-26T17:18:05,725][DEBUG][logstash.filters.grok    ][main] Adding pattern {"TZ"=>"(?:[APMCE][SD]T|UTC)"}
[2020-05-26T17:18:05,731][DEBUG][logstash.filters.grok    ][main] Adding pattern {"DATESTAMP_RFC822"=>"%{DAY} %{MONTH} %{MONTHDAY} %{YEAR} %{TIME} %{TZ}"}
[2020-05-26T17:18:05,735][DEBUG][logstash.filters.grok    ][main] Adding pattern {"DATESTAMP_RFC2822"=>"%{DAY}, %{MONTHDAY} %{MONTH} %{YEAR} %{TIME} %{ISO8601_TIMEZONE}"}
[2020-05-26T17:18:05,736][DEBUG][logstash.filters.grok    ][main] Adding pattern {"DATESTAMP_OTHER"=>"%{DAY} %{MONTH} %{MONTHDAY} %{TIME} %{TZ} %{YEAR}"}
[2020-05-26T17:18:05,737][DEBUG][logstash.filters.grok    ][main] Adding pattern {"DATESTAMP_EVENTLOG"=>"%{YEAR}%{MONTHNUM2}%{MONTHDAY}%{HOUR}%{MINUTE}%{SECOND}"}
[2020-05-26T17:18:05,739][DEBUG][logstash.filters.grok    ][main] Adding pattern {"SYSLOGTIMESTAMP"=>"%{MONTH} +%{MONTHDAY} %{TIME}"}
[2020-05-26T17:18:05,745][DEBUG][logstash.filters.grok    ][main] Adding pattern {"PROG"=>"[\\x21-\\x5a\\x5c\\x5e-\\x7e]+"}
[2020-05-26T17:18:05,752][DEBUG][logstash.filters.grok    ][main] Adding pattern {"SYSLOGPROG"=>"%{PROG:program}(?:\\[%{POSINT:pid}\\])?"}
[2020-05-26T17:18:05,762][DEBUG][logstash.filters.grok    ][main] Adding pattern {"SYSLOGHOST"=>"%{IPORHOST}"}
[2020-05-26T17:18:05,763][DEBUG][logstash.filters.grok    ][main] Adding pattern {"SYSLOGFACILITY"=>"<%{NONNEGINT:facility}.%{NONNEGINT:priority}>"}
[2020-05-26T17:18:05,765][DEBUG][logstash.filters.grok    ][main] Adding pattern {"HTTPDATE"=>"%{MONTHDAY}/%{MONTH}/%{YEAR}:%{TIME} %{INT}"}
[2020-05-26T17:18:05,767][DEBUG][logstash.filters.grok    ][main] Adding pattern {"QS"=>"%{QUOTEDSTRING}"}
[2020-05-26T17:18:05,769][DEBUG][logstash.filters.grok    ][main] Adding pattern {"SYSLOGBASE"=>"%{SYSLOGTIMESTAMP:timestamp} (?:%{SYSLOGFACILITY} )?%{SYSLOGHOST:logsource} %{SYSLOGPROG}:"}
[2020-05-26T17:18:05,781][DEBUG][logstash.filters.grok    ][main] Adding pattern {"LOGLEVEL"=>"([Aa]lert|ALERT|[Tt]race|TRACE|[Dd]ebug|DEBUG|[Nn]otice|NOTICE|[Ii]nfo|INFO|[Ww]arn?(?:ing)?|WARN?(?:ING)?|[Ee]rr?(?:or)?|ERR?(?:OR)?|[Cc]rit?(?:ical)?|CRIT?(?:ICAL)?|[Ff]atal|FATAL|[Ss]evere|SEVERE|EMERG(?:ENCY)?|[Ee]merg(?:ency)?)"}
[2020-05-26T17:18:05,783][DEBUG][logstash.filters.grok    ][main] Adding pattern {"HAPROXYTIME"=>"(?!<[0-9])%{HOUR:haproxy_hour}:%{MINUTE:haproxy_minute}(?::%{SECOND:haproxy_second})(?![0-9])"}
[2020-05-26T17:18:05,785][DEBUG][logstash.filters.grok    ][main] Adding pattern {"HAPROXYDATE"=>"%{MONTHDAY:haproxy_monthday}/%{MONTH:haproxy_month}/%{YEAR:haproxy_year}:%{HAPROXYTIME:haproxy_time}.%{INT:haproxy_milliseconds}"}
[2020-05-26T17:18:05,787][DEBUG][logstash.filters.grok    ][main] Adding pattern {"HAPROXYCAPTUREDREQUESTHEADERS"=>"%{DATA:captured_request_headers}"}
[2020-05-26T17:18:05,790][DEBUG][logstash.filters.grok    ][main] Adding pattern {"HAPROXYCAPTUREDRESPONSEHEADERS"=>"%{DATA:captured_response_headers}"}
[2020-05-26T17:18:05,803][DEBUG][logstash.filters.grok    ][main] Adding pattern {"HAPROXYHTTPBASE"=>"%{IP:client_ip}:%{INT:client_port} \\[%{HAPROXYDATE:accept_date}\\] %{NOTSPACE:frontend_name} %{NOTSPACE:backend_name}/%{NOTSPACE:server_name} %{INT:time_request}/%{INT:time_queue}/%{INT:time_backend_connect}/%{INT:time_backend_response}/%{NOTSPACE:time_duration} %{INT:http_status_code} %{NOTSPACE:bytes_read} %{DATA:captured_request_cookie} %{DATA:captured_response_cookie} %{NOTSPACE:termination_state} %{INT:actconn}/%{INT:feconn}/%{INT:beconn}/%{INT:srvconn}/%{NOTSPACE:retries} %{INT:srv_queue}/%{INT:backend_queue} (\\{%{HAPROXYCAPTUREDREQUESTHEADERS}\\})?( )?(\\{%{HAPROXYCAPTUREDRESPONSEHEADERS}\\})?( )?\"(<BADREQ>|(%{WORD:http_verb} (%{URIPROTO:http_proto}://)?(?:%{USER:http_user}(?::[^@]*)?@)?(?:%{URIHOST:http_host})?(?:%{URIPATHPARAM:http_request})?( HTTP/%{NUMBER:http_version})?))?\""}
[2020-05-26T17:18:05,806][DEBUG][logstash.filters.grok    ][main] Adding pattern {"HAPROXYHTTP"=>"(?:%{SYSLOGTIMESTAMP:syslog_timestamp}|%{TIMESTAMP_ISO8601:timestamp8601}) %{IPORHOST:syslog_server} %{SYSLOGPROG}: %{HAPROXYHTTPBASE}"}
[2020-05-26T17:18:05,809][DEBUG][logstash.filters.grok    ][main] Adding pattern {"HAPROXYTCP"=>"(?:%{SYSLOGTIMESTAMP:syslog_timestamp}|%{TIMESTAMP_ISO8601:timestamp8601}) %{IPORHOST:syslog_server} %{SYSLOGPROG}: %{IP:client_ip}:%{INT:client_port} \\[%{HAPROXYDATE:accept_date}\\] %{NOTSPACE:frontend_name} %{NOTSPACE:backend_name}/%{NOTSPACE:server_name} %{INT:time_queue}/%{INT:time_backend_connect}/%{NOTSPACE:time_duration} %{NOTSPACE:bytes_read} %{NOTSPACE:termination_state} %{INT:actconn}/%{INT:feconn}/%{INT:beconn}/%{INT:srvconn}/%{NOTSPACE:retries} %{INT:srv_queue}/%{INT:backend_queue}"}
[2020-05-26T17:18:05,821][DEBUG][logstash.filters.grok    ][main] Adding pattern {"HTTPDUSER"=>"%{EMAILADDRESS}|%{USER}"}
[2020-05-26T17:18:05,824][DEBUG][logstash.filters.grok    ][main] Adding pattern {"HTTPDERROR_DATE"=>"%{DAY} %{MONTH} %{MONTHDAY} %{TIME} %{YEAR}"}
[2020-05-26T17:18:05,826][DEBUG][logstash.filters.grok    ][main] Adding pattern {"HTTPD_COMMONLOG"=>"%{IPORHOST:clientip} %{HTTPDUSER:ident} %{HTTPDUSER:auth} \\[%{HTTPDATE:timestamp}\\] \"(?:%{WORD:verb} %{NOTSPACE:request}(?: HTTP/%{NUMBER:httpversion})?|%{DATA:rawrequest})\" %{NUMBER:response} (?:%{NUMBER:bytes}|-)"}
[2020-05-26T17:18:05,827][DEBUG][logstash.filters.grok    ][main] Adding pattern {"HTTPD_COMBINEDLOG"=>"%{HTTPD_COMMONLOG} %{QS:referrer} %{QS:agent}"}
[2020-05-26T17:18:05,839][DEBUG][logstash.filters.grok    ][main] Adding pattern {"HTTPD20_ERRORLOG"=>"\\[%{HTTPDERROR_DATE:timestamp}\\] \\[%{LOGLEVEL:loglevel}\\] (?:\\[client %{IPORHOST:clientip}\\] ){0,1}%{GREEDYDATA:message}"}
[2020-05-26T17:18:05,842][DEBUG][logstash.filters.grok    ][main] Adding pattern {"HTTPD24_ERRORLOG"=>"\\[%{HTTPDERROR_DATE:timestamp}\\] \\[%{WORD:module}:%{LOGLEVEL:loglevel}\\] \\[pid %{POSINT:pid}(:tid %{NUMBER:tid})?\\]( \\(%{POSINT:proxy_errorcode}\\)%{DATA:proxy_message}:)?( \\[client %{IPORHOST:clientip}:%{POSINT:clientport}\\])?( %{DATA:errorcode}:)? %{GREEDYDATA:message}"}
[2020-05-26T17:18:05,845][DEBUG][logstash.filters.grok    ][main] Adding pattern {"HTTPD_ERRORLOG"=>"%{HTTPD20_ERRORLOG}|%{HTTPD24_ERRORLOG}"}
[2020-05-26T17:18:05,847][DEBUG][logstash.filters.grok    ][main] Adding pattern {"COMMONAPACHELOG"=>"%{HTTPD_COMMONLOG}"}
[2020-05-26T17:18:05,858][DEBUG][logstash.filters.grok    ][main] Adding pattern {"COMBINEDAPACHELOG"=>"%{HTTPD_COMBINEDLOG}"}
[2020-05-26T17:18:05,860][DEBUG][logstash.filters.grok    ][main] Adding pattern {"JAVACLASS"=>"(?:[a-zA-Z$_][a-zA-Z$_0-9]*\\.)*[a-zA-Z$_][a-zA-Z$_0-9]*"}
[2020-05-26T17:18:05,861][DEBUG][logstash.filters.grok    ][main] Adding pattern {"JAVAFILE"=>"(?:[A-Za-z0-9_. -]+)"}
[2020-05-26T17:18:05,863][DEBUG][logstash.filters.grok    ][main] Adding pattern {"JAVAMETHOD"=>"(?:(<(?:cl)?init>)|[a-zA-Z$_][a-zA-Z$_0-9]*)"}
[2020-05-26T17:18:05,865][DEBUG][logstash.filters.grok    ][main] Adding pattern {"JAVASTACKTRACEPART"=>"%{SPACE}at %{JAVACLASS:class}\\.%{JAVAMETHOD:method}\\(%{JAVAFILE:file}(?::%{NUMBER:line})?\\)"}
[2020-05-26T17:18:05,872][DEBUG][logstash.filters.grok    ][main] Adding pattern {"JAVATHREAD"=>"(?:[A-Z]{2}-Processor[\\d]+)"}
[2020-05-26T17:18:05,878][DEBUG][logstash.filters.grok    ][main] Adding pattern {"JAVACLASS"=>"(?:[a-zA-Z0-9-]+\\.)+[A-Za-z0-9$]+"}
[2020-05-26T17:18:05,879][DEBUG][logstash.filters.grok    ][main] Adding pattern {"JAVAFILE"=>"(?:[A-Za-z0-9_.-]+)"}
[2020-05-26T17:18:05,880][DEBUG][logstash.filters.grok    ][main] Adding pattern {"JAVALOGMESSAGE"=>"(.*)"}
[2020-05-26T17:18:05,886][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CATALINA_DATESTAMP"=>"%{MONTH} %{MONTHDAY}, 20%{YEAR} %{HOUR}:?%{MINUTE}(?::?%{SECOND}) (?:AM|PM)"}
[2020-05-26T17:18:05,893][DEBUG][logstash.filters.grok    ][main] Adding pattern {"TOMCAT_DATESTAMP"=>"20%{YEAR}-%{MONTHNUM}-%{MONTHDAY} %{HOUR}:?%{MINUTE}(?::?%{SECOND}) %{ISO8601_TIMEZONE}"}
[2020-05-26T17:18:05,894][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CATALINALOG"=>"%{CATALINA_DATESTAMP:timestamp} %{JAVACLASS:class} %{JAVALOGMESSAGE:logmessage}"}
[2020-05-26T17:18:05,899][DEBUG][logstash.filters.grok    ][main] Adding pattern {"TOMCATLOG"=>"%{TOMCAT_DATESTAMP:timestamp} \\| %{LOGLEVEL:level} \\| %{JAVACLASS:class} - %{JAVALOGMESSAGE:logmessage}"}
[2020-05-26T17:18:05,915][DEBUG][logstash.filters.grok    ][main] Adding pattern {"RT_FLOW_EVENT"=>"(RT_FLOW_SESSION_CREATE|RT_FLOW_SESSION_CLOSE|RT_FLOW_SESSION_DENY)"}
[2020-05-26T17:18:05,917][DEBUG][logstash.filters.grok    ][main] Adding pattern {"RT_FLOW1"=>"%{RT_FLOW_EVENT:event}: %{GREEDYDATA:close-reason}: %{IP:src-ip}/%{INT:src-port}->%{IP:dst-ip}/%{INT:dst-port} %{DATA:service} %{IP:nat-src-ip}/%{INT:nat-src-port}->%{IP:nat-dst-ip}/%{INT:nat-dst-port} %{DATA:src-nat-rule-name} %{DATA:dst-nat-rule-name} %{INT:protocol-id} %{DATA:policy-name} %{DATA:from-zone} %{DATA:to-zone} %{INT:session-id} \\d+\\(%{DATA:sent}\\) \\d+\\(%{DATA:received}\\) %{INT:elapsed-time} .*"}
[2020-05-26T17:18:05,920][DEBUG][logstash.filters.grok    ][main] Adding pattern {"RT_FLOW2"=>"%{RT_FLOW_EVENT:event}: session created %{IP:src-ip}/%{INT:src-port}->%{IP:dst-ip}/%{INT:dst-port} %{DATA:service} %{IP:nat-src-ip}/%{INT:nat-src-port}->%{IP:nat-dst-ip}/%{INT:nat-dst-port} %{DATA:src-nat-rule-name} %{DATA:dst-nat-rule-name} %{INT:protocol-id} %{DATA:policy-name} %{DATA:from-zone} %{DATA:to-zone} %{INT:session-id} .*"}
[2020-05-26T17:18:05,921][DEBUG][logstash.filters.grok    ][main] Adding pattern {"RT_FLOW3"=>"%{RT_FLOW_EVENT:event}: session denied %{IP:src-ip}/%{INT:src-port}->%{IP:dst-ip}/%{INT:dst-port} %{DATA:service} %{INT:protocol-id}\\(\\d\\) %{DATA:policy-name} %{DATA:from-zone} %{DATA:to-zone} .*"}
[2020-05-26T17:18:05,923][DEBUG][logstash.filters.grok    ][main] Adding pattern {"SYSLOG5424PRINTASCII"=>"[!-~]+"}
[2020-05-26T17:18:05,924][DEBUG][logstash.filters.grok    ][main] Adding pattern {"SYSLOGBASE2"=>"(?:%{SYSLOGTIMESTAMP:timestamp}|%{TIMESTAMP_ISO8601:timestamp8601}) (?:%{SYSLOGFACILITY} )?%{SYSLOGHOST:logsource}+(?: %{SYSLOGPROG}:|)"}
[2020-05-26T17:18:05,931][DEBUG][logstash.filters.grok    ][main] Adding pattern {"SYSLOGPAMSESSION"=>"%{SYSLOGBASE} (?=%{GREEDYDATA:message})%{WORD:pam_module}\\(%{DATA:pam_caller}\\): session %{WORD:pam_session_state} for user %{USERNAME:username}(?: by %{GREEDYDATA:pam_by})?"}
[2020-05-26T17:18:05,934][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CRON_ACTION"=>"[A-Z ]+"}
[2020-05-26T17:18:05,935][DEBUG][logstash.filters.grok    ][main] Adding pattern {"CRONLOG"=>"%{SYSLOGBASE} \\(%{USER:user}\\) %{CRON_ACTION:action} \\(%{DATA:message}\\)"}
[2020-05-26T17:18:05,936][DEBUG][logstash.filters.grok    ][main] Adding pattern {"SYSLOGLINE"=>"%{SYSLOGBASE2} %{GREEDYDATA:message}"}
[2020-05-26T17:18:05,936][DEBUG][logstash.filters.grok    ][main] Adding pattern {"SYSLOG5424PRI"=>"<%{NONNEGINT:syslog5424_pri}>"}
[2020-05-26T17:18:05,937][DEBUG][logstash.filters.grok    ][main] Adding pattern {"SYSLOG5424SD"=>"\\[%{DATA}\\]+"}
[2020-05-26T17:18:05,938][DEBUG][logstash.filters.grok    ][main] Adding pattern {"SYSLOG5424BASE"=>"%{SYSLOG5424PRI}%{NONNEGINT:syslog5424_ver} +(?:%{TIMESTAMP_ISO8601:syslog5424_ts}|-) +(?:%{IPORHOST:syslog5424_host}|-) +(-|%{SYSLOG5424PRINTASCII:syslog5424_app}) +(-|%{SYSLOG5424PRINTASCII:syslog5424_proc}) +(-|%{SYSLOG5424PRINTASCII:syslog5424_msgid}) +(?:%{SYSLOG5424SD:syslog5424_sd}|-|)"}
[2020-05-26T17:18:05,939][DEBUG][logstash.filters.grok    ][main] Adding pattern {"SYSLOG5424LINE"=>"%{SYSLOG5424BASE} +%{GREEDYDATA:syslog5424_msg}"}
[2020-05-26T17:18:05,945][DEBUG][logstash.filters.grok    ][main] Adding pattern {"MAVEN_VERSION"=>"(?:(\\d+)\\.)?(?:(\\d+)\\.)?(\\*|\\d+)(?:[.-](RELEASE|SNAPSHOT))?"}
[2020-05-26T17:18:05,949][DEBUG][logstash.filters.grok    ][main] Adding pattern {"MCOLLECTIVEAUDIT"=>"%{TIMESTAMP_ISO8601:timestamp}:"}
[2020-05-26T17:18:05,950][DEBUG][logstash.filters.grok    ][main] Adding pattern {"MCOLLECTIVE"=>"., \\[%{TIMESTAMP_ISO8601:timestamp} #%{POSINT:pid}\\]%{SPACE}%{LOGLEVEL:event_level}"}
[2020-05-26T17:18:05,951][DEBUG][logstash.filters.grok    ][main] Adding pattern {"MCOLLECTIVEAUDIT"=>"%{TIMESTAMP_ISO8601:timestamp}:"}
[2020-05-26T17:18:05,953][DEBUG][logstash.filters.grok    ][main] Adding pattern {"MONGO_LOG"=>"%{SYSLOGTIMESTAMP:timestamp} \\[%{WORD:component}\\] %{GREEDYDATA:message}"}
[2020-05-26T17:18:05,958][DEBUG][logstash.filters.grok    ][main] Adding pattern {"MONGO_QUERY"=>"\\{ (?<={ ).*(?= } ntoreturn:) \\}"}
[2020-05-26T17:18:05,959][DEBUG][logstash.filters.grok    ][main] Adding pattern {"MONGO_SLOWQUERY"=>"%{WORD} %{MONGO_WORDDASH:database}\\.%{MONGO_WORDDASH:collection} %{WORD}: %{MONGO_QUERY:query} %{WORD}:%{NONNEGINT:ntoreturn} %{WORD}:%{NONNEGINT:ntoskip} %{WORD}:%{NONNEGINT:nscanned}.*nreturned:%{NONNEGINT:nreturned}..+ (?<duration>[0-9]+)ms"}
[2020-05-26T17:18:05,960][DEBUG][logstash.filters.grok    ][main] Adding pattern {"MONGO_WORDDASH"=>"\\b[\\w-]+\\b"}
[2020-05-26T17:18:05,961][DEBUG][logstash.filters.grok    ][main] Adding pattern {"MONGO3_SEVERITY"=>"\\w"}
[2020-05-26T17:18:05,961][DEBUG][logstash.filters.grok    ][main] Adding pattern {"MONGO3_COMPONENT"=>"%{WORD}|-"}
[2020-05-26T17:18:05,962][DEBUG][logstash.filters.grok    ][main] Adding pattern {"MONGO3_LOG"=>"%{TIMESTAMP_ISO8601:timestamp} %{MONGO3_SEVERITY:severity} %{MONGO3_COMPONENT:component}%{SPACE}(?:\\[%{DATA:context}\\])? %{GREEDYDATA:message}"}
[2020-05-26T17:18:05,963][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOSTIME"=>"\\[%{NUMBER:nagios_epoch}\\]"}
[2020-05-26T17:18:05,964][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_TYPE_CURRENT_SERVICE_STATE"=>"CURRENT SERVICE STATE"}
[2020-05-26T17:18:05,965][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_TYPE_CURRENT_HOST_STATE"=>"CURRENT HOST STATE"}
[2020-05-26T17:18:05,965][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_TYPE_SERVICE_NOTIFICATION"=>"SERVICE NOTIFICATION"}
[2020-05-26T17:18:05,966][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_TYPE_HOST_NOTIFICATION"=>"HOST NOTIFICATION"}
[2020-05-26T17:18:05,974][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_TYPE_SERVICE_ALERT"=>"SERVICE ALERT"}
[2020-05-26T17:18:05,975][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_TYPE_HOST_ALERT"=>"HOST ALERT"}
[2020-05-26T17:18:05,976][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_TYPE_SERVICE_FLAPPING_ALERT"=>"SERVICE FLAPPING ALERT"}
[2020-05-26T17:18:05,977][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_TYPE_HOST_FLAPPING_ALERT"=>"HOST FLAPPING ALERT"}
[2020-05-26T17:18:05,979][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_TYPE_SERVICE_DOWNTIME_ALERT"=>"SERVICE DOWNTIME ALERT"}
[2020-05-26T17:18:05,980][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_TYPE_HOST_DOWNTIME_ALERT"=>"HOST DOWNTIME ALERT"}
[2020-05-26T17:18:05,981][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_TYPE_PASSIVE_SERVICE_CHECK"=>"PASSIVE SERVICE CHECK"}
[2020-05-26T17:18:05,981][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_TYPE_PASSIVE_HOST_CHECK"=>"PASSIVE HOST CHECK"}
[2020-05-26T17:18:05,982][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_TYPE_SERVICE_EVENT_HANDLER"=>"SERVICE EVENT HANDLER"}
[2020-05-26T17:18:05,992][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_TYPE_HOST_EVENT_HANDLER"=>"HOST EVENT HANDLER"}
[2020-05-26T17:18:05,994][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_TYPE_EXTERNAL_COMMAND"=>"EXTERNAL COMMAND"}
[2020-05-26T17:18:05,996][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_TYPE_TIMEPERIOD_TRANSITION"=>"TIMEPERIOD TRANSITION"}
[2020-05-26T17:18:05,997][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_EC_DISABLE_SVC_CHECK"=>"DISABLE_SVC_CHECK"}
[2020-05-26T17:18:05,997][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_EC_ENABLE_SVC_CHECK"=>"ENABLE_SVC_CHECK"}
[2020-05-26T17:18:05,999][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_EC_DISABLE_HOST_CHECK"=>"DISABLE_HOST_CHECK"}
[2020-05-26T17:18:06,008][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_EC_ENABLE_HOST_CHECK"=>"ENABLE_HOST_CHECK"}
[2020-05-26T17:18:06,008][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_EC_PROCESS_SERVICE_CHECK_RESULT"=>"PROCESS_SERVICE_CHECK_RESULT"}
[2020-05-26T17:18:06,011][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_EC_PROCESS_HOST_CHECK_RESULT"=>"PROCESS_HOST_CHECK_RESULT"}
[2020-05-26T17:18:06,012][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_EC_SCHEDULE_SERVICE_DOWNTIME"=>"SCHEDULE_SERVICE_DOWNTIME"}
[2020-05-26T17:18:06,013][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_EC_SCHEDULE_HOST_DOWNTIME"=>"SCHEDULE_HOST_DOWNTIME"}
[2020-05-26T17:18:06,013][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_EC_DISABLE_HOST_SVC_NOTIFICATIONS"=>"DISABLE_HOST_SVC_NOTIFICATIONS"}
[2020-05-26T17:18:06,014][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_EC_ENABLE_HOST_SVC_NOTIFICATIONS"=>"ENABLE_HOST_SVC_NOTIFICATIONS"}
[2020-05-26T17:18:06,015][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_EC_DISABLE_HOST_NOTIFICATIONS"=>"DISABLE_HOST_NOTIFICATIONS"}
[2020-05-26T17:18:06,024][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_EC_ENABLE_HOST_NOTIFICATIONS"=>"ENABLE_HOST_NOTIFICATIONS"}
[2020-05-26T17:18:06,025][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_EC_DISABLE_SVC_NOTIFICATIONS"=>"DISABLE_SVC_NOTIFICATIONS"}
[2020-05-26T17:18:06,028][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_EC_ENABLE_SVC_NOTIFICATIONS"=>"ENABLE_SVC_NOTIFICATIONS"}
[2020-05-26T17:18:06,029][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_WARNING"=>"Warning:%{SPACE}%{GREEDYDATA:nagios_message}"}
[2020-05-26T17:18:06,030][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_CURRENT_SERVICE_STATE"=>"%{NAGIOS_TYPE_CURRENT_SERVICE_STATE:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_service};%{DATA:nagios_state};%{DATA:nagios_statetype};%{DATA:nagios_statecode};%{GREEDYDATA:nagios_message}"}
[2020-05-26T17:18:06,031][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_CURRENT_HOST_STATE"=>"%{NAGIOS_TYPE_CURRENT_HOST_STATE:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_state};%{DATA:nagios_statetype};%{DATA:nagios_statecode};%{GREEDYDATA:nagios_message}"}
[2020-05-26T17:18:06,033][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_SERVICE_NOTIFICATION"=>"%{NAGIOS_TYPE_SERVICE_NOTIFICATION:nagios_type}: %{DATA:nagios_notifyname};%{DATA:nagios_hostname};%{DATA:nagios_service};%{DATA:nagios_state};%{DATA:nagios_contact};%{GREEDYDATA:nagios_message}"}
[2020-05-26T17:18:06,041][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_HOST_NOTIFICATION"=>"%{NAGIOS_TYPE_HOST_NOTIFICATION:nagios_type}: %{DATA:nagios_notifyname};%{DATA:nagios_hostname};%{DATA:nagios_state};%{DATA:nagios_contact};%{GREEDYDATA:nagios_message}"}
[2020-05-26T17:18:06,042][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_SERVICE_ALERT"=>"%{NAGIOS_TYPE_SERVICE_ALERT:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_service};%{DATA:nagios_state};%{DATA:nagios_statelevel};%{NUMBER:nagios_attempt};%{GREEDYDATA:nagios_message}"}
[2020-05-26T17:18:06,044][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_HOST_ALERT"=>"%{NAGIOS_TYPE_HOST_ALERT:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_state};%{DATA:nagios_statelevel};%{NUMBER:nagios_attempt};%{GREEDYDATA:nagios_message}"}
[2020-05-26T17:18:06,045][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_SERVICE_FLAPPING_ALERT"=>"%{NAGIOS_TYPE_SERVICE_FLAPPING_ALERT:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_service};%{DATA:nagios_state};%{GREEDYDATA:nagios_message}"}
[2020-05-26T17:18:06,046][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_HOST_FLAPPING_ALERT"=>"%{NAGIOS_TYPE_HOST_FLAPPING_ALERT:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_state};%{GREEDYDATA:nagios_message}"}
[2020-05-26T17:18:06,047][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_SERVICE_DOWNTIME_ALERT"=>"%{NAGIOS_TYPE_SERVICE_DOWNTIME_ALERT:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_service};%{DATA:nagios_state};%{GREEDYDATA:nagios_comment}"}
[2020-05-26T17:18:06,049][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_HOST_DOWNTIME_ALERT"=>"%{NAGIOS_TYPE_HOST_DOWNTIME_ALERT:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_state};%{GREEDYDATA:nagios_comment}"}
[2020-05-26T17:18:06,058][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_PASSIVE_SERVICE_CHECK"=>"%{NAGIOS_TYPE_PASSIVE_SERVICE_CHECK:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_service};%{DATA:nagios_state};%{GREEDYDATA:nagios_comment}"}
[2020-05-26T17:18:06,059][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_PASSIVE_HOST_CHECK"=>"%{NAGIOS_TYPE_PASSIVE_HOST_CHECK:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_state};%{GREEDYDATA:nagios_comment}"}
[2020-05-26T17:18:06,060][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_SERVICE_EVENT_HANDLER"=>"%{NAGIOS_TYPE_SERVICE_EVENT_HANDLER:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_service};%{DATA:nagios_state};%{DATA:nagios_statelevel};%{DATA:nagios_event_handler_name}"}
[2020-05-26T17:18:06,061][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_HOST_EVENT_HANDLER"=>"%{NAGIOS_TYPE_HOST_EVENT_HANDLER:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_state};%{DATA:nagios_statelevel};%{DATA:nagios_event_handler_name}"}
[2020-05-26T17:18:06,062][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_TIMEPERIOD_TRANSITION"=>"%{NAGIOS_TYPE_TIMEPERIOD_TRANSITION:nagios_type}: %{DATA:nagios_service};%{DATA:nagios_unknown1};%{DATA:nagios_unknown2}"}
[2020-05-26T17:18:06,063][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_EC_LINE_DISABLE_SVC_CHECK"=>"%{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_DISABLE_SVC_CHECK:nagios_command};%{DATA:nagios_hostname};%{DATA:nagios_service}"}
[2020-05-26T17:18:06,065][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_EC_LINE_DISABLE_HOST_CHECK"=>"%{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_DISABLE_HOST_CHECK:nagios_command};%{DATA:nagios_hostname}"}
[2020-05-26T17:18:06,066][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_EC_LINE_ENABLE_SVC_CHECK"=>"%{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_ENABLE_SVC_CHECK:nagios_command};%{DATA:nagios_hostname};%{DATA:nagios_service}"}
[2020-05-26T17:18:06,075][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_EC_LINE_ENABLE_HOST_CHECK"=>"%{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_ENABLE_HOST_CHECK:nagios_command};%{DATA:nagios_hostname}"}
[2020-05-26T17:18:06,076][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_EC_LINE_PROCESS_SERVICE_CHECK_RESULT"=>"%{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_PROCESS_SERVICE_CHECK_RESULT:nagios_command};%{DATA:nagios_hostname};%{DATA:nagios_service};%{DATA:nagios_state};%{GREEDYDATA:nagios_check_result}"}
[2020-05-26T17:18:06,077][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_EC_LINE_PROCESS_HOST_CHECK_RESULT"=>"%{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_PROCESS_HOST_CHECK_RESULT:nagios_command};%{DATA:nagios_hostname};%{DATA:nagios_state};%{GREEDYDATA:nagios_check_result}"}
[2020-05-26T17:18:06,078][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_EC_LINE_DISABLE_HOST_SVC_NOTIFICATIONS"=>"%{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_DISABLE_HOST_SVC_NOTIFICATIONS:nagios_command};%{GREEDYDATA:nagios_hostname}"}
[2020-05-26T17:18:06,079][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_EC_LINE_DISABLE_HOST_NOTIFICATIONS"=>"%{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_DISABLE_HOST_NOTIFICATIONS:nagios_command};%{GREEDYDATA:nagios_hostname}"}
[2020-05-26T17:18:06,080][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_EC_LINE_DISABLE_SVC_NOTIFICATIONS"=>"%{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_DISABLE_SVC_NOTIFICATIONS:nagios_command};%{DATA:nagios_hostname};%{GREEDYDATA:nagios_service}"}
[2020-05-26T17:18:06,081][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_EC_LINE_ENABLE_HOST_SVC_NOTIFICATIONS"=>"%{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_ENABLE_HOST_SVC_NOTIFICATIONS:nagios_command};%{GREEDYDATA:nagios_hostname}"}
[2020-05-26T17:18:06,082][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_EC_LINE_ENABLE_HOST_NOTIFICATIONS"=>"%{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_ENABLE_HOST_NOTIFICATIONS:nagios_command};%{GREEDYDATA:nagios_hostname}"}
[2020-05-26T17:18:06,091][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_EC_LINE_ENABLE_SVC_NOTIFICATIONS"=>"%{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_ENABLE_SVC_NOTIFICATIONS:nagios_command};%{DATA:nagios_hostname};%{GREEDYDATA:nagios_service}"}
[2020-05-26T17:18:06,092][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOS_EC_LINE_SCHEDULE_HOST_DOWNTIME"=>"%{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_SCHEDULE_HOST_DOWNTIME:nagios_command};%{DATA:nagios_hostname};%{NUMBER:nagios_start_time};%{NUMBER:nagios_end_time};%{NUMBER:nagios_fixed};%{NUMBER:nagios_trigger_id};%{NUMBER:nagios_duration};%{DATA:author};%{DATA:comment}"}
[2020-05-26T17:18:06,094][DEBUG][logstash.filters.grok    ][main] Adding pattern {"NAGIOSLOGLINE"=>"%{NAGIOSTIME} (?:%{NAGIOS_WARNING}|%{NAGIOS_CURRENT_SERVICE_STATE}|%{NAGIOS_CURRENT_HOST_STATE}|%{NAGIOS_SERVICE_NOTIFICATION}|%{NAGIOS_HOST_NOTIFICATION}|%{NAGIOS_SERVICE_ALERT}|%{NAGIOS_HOST_ALERT}|%{NAGIOS_SERVICE_FLAPPING_ALERT}|%{NAGIOS_HOST_FLAPPING_ALERT}|%{NAGIOS_SERVICE_DOWNTIME_ALERT}|%{NAGIOS_HOST_DOWNTIME_ALERT}|%{NAGIOS_PASSIVE_SERVICE_CHECK}|%{NAGIOS_PASSIVE_HOST_CHECK}|%{NAGIOS_SERVICE_EVENT_HANDLER}|%{NAGIOS_HOST_EVENT_HANDLER}|%{NAGIOS_TIMEPERIOD_TRANSITION}|%{NAGIOS_EC_LINE_DISABLE_SVC_CHECK}|%{NAGIOS_EC_LINE_ENABLE_SVC_CHECK}|%{NAGIOS_EC_LINE_DISABLE_HOST_CHECK}|%{NAGIOS_EC_LINE_ENABLE_HOST_CHECK}|%{NAGIOS_EC_LINE_PROCESS_HOST_CHECK_RESULT}|%{NAGIOS_EC_LINE_PROCESS_SERVICE_CHECK_RESULT}|%{NAGIOS_EC_LINE_SCHEDULE_HOST_DOWNTIME}|%{NAGIOS_EC_LINE_DISABLE_HOST_SVC_NOTIFICATIONS}|%{NAGIOS_EC_LINE_ENABLE_HOST_SVC_NOTIFICATIONS}|%{NAGIOS_EC_LINE_DISABLE_HOST_NOTIFICATIONS}|%{NAGIOS_EC_LINE_ENABLE_HOST_NOTIFICATIONS}|%{NAGIOS_EC_LINE_DISABLE_SVC_NOTIFICATIONS}|%{NAGIOS_EC_LINE_ENABLE_SVC_NOTIFICATIONS})"}
[2020-05-26T17:18:06,096][DEBUG][logstash.filters.grok    ][main] Adding pattern {"POSTGRESQL"=>"%{DATESTAMP:timestamp} %{TZ} %{DATA:user_id} %{GREEDYDATA:connection_id} %{POSINT:pid}"}
[2020-05-26T17:18:06,097][DEBUG][logstash.filters.grok    ][main] Adding pattern {"RUUID"=>"\\h{32}"}
[2020-05-26T17:18:06,098][DEBUG][logstash.filters.grok    ][main] Adding pattern {"RCONTROLLER"=>"(?<controller>[^#]+)#(?<action>\\w+)"}
[2020-05-26T17:18:06,099][DEBUG][logstash.filters.grok    ][main] Adding pattern {"RAILS3HEAD"=>"(?m)Started %{WORD:verb} \"%{URIPATHPARAM:request}\" for %{IPORHOST:clientip} at (?<timestamp>%{YEAR}-%{MONTHNUM}-%{MONTHDAY} %{HOUR}:%{MINUTE}:%{SECOND} %{ISO8601_TIMEZONE})"}
[2020-05-26T17:18:06,108][DEBUG][logstash.filters.grok    ][main] Adding pattern {"RPROCESSING"=>"\\W*Processing by %{RCONTROLLER} as (?<format>\\S+)(?:\\W*Parameters: {%{DATA:params}}\\W*)?"}
[2020-05-26T17:18:06,109][DEBUG][logstash.filters.grok    ][main] Adding pattern {"RAILS3FOOT"=>"Completed %{NUMBER:response}%{DATA} in %{NUMBER:totalms}ms %{RAILS3PROFILE}%{GREEDYDATA}"}
[2020-05-26T17:18:06,111][DEBUG][logstash.filters.grok    ][main] Adding pattern {"RAILS3PROFILE"=>"(?:\\(Views: %{NUMBER:viewms}ms \\| ActiveRecord: %{NUMBER:activerecordms}ms|\\(ActiveRecord: %{NUMBER:activerecordms}ms)?"}
[2020-05-26T17:18:06,112][DEBUG][logstash.filters.grok    ][main] Adding pattern {"RAILS3"=>"%{RAILS3HEAD}(?:%{RPROCESSING})?(?<context>(?:%{DATA}\\n)*)(?:%{RAILS3FOOT})?"}
[2020-05-26T17:18:06,114][DEBUG][logstash.filters.grok    ][main] Adding pattern {"REDISTIMESTAMP"=>"%{MONTHDAY} %{MONTH} %{TIME}"}
[2020-05-26T17:18:06,115][DEBUG][logstash.filters.grok    ][main] Adding pattern {"REDISLOG"=>"\\[%{POSINT:pid}\\] %{REDISTIMESTAMP:timestamp} \\* "}
[2020-05-26T17:18:06,116][DEBUG][logstash.filters.grok    ][main] Adding pattern {"REDISMONLOG"=>"%{NUMBER:timestamp} \\[%{INT:database} %{IP:client}:%{NUMBER:port}\\] \"%{WORD:command}\"\\s?%{GREEDYDATA:params}"}
[2020-05-26T17:18:06,125][DEBUG][logstash.filters.grok    ][main] Adding pattern {"RUBY_LOGLEVEL"=>"(?:DEBUG|FATAL|ERROR|WARN|INFO)"}
[2020-05-26T17:18:06,126][DEBUG][logstash.filters.grok    ][main] Adding pattern {"RUBY_LOGGER"=>"[DFEWI], \\[%{TIMESTAMP_ISO8601:timestamp} #%{POSINT:pid}\\] *%{RUBY_LOGLEVEL:loglevel} -- +%{DATA:progname}: %{GREEDYDATA:message}"}
[2020-05-26T17:18:06,128][DEBUG][logstash.filters.grok    ][main] Adding pattern {"SQUID3"=>"%{NUMBER:timestamp}\\s+%{NUMBER:duration}\\s%{IP:client_address}\\s%{WORD:cache_result}/%{POSINT:status_code}\\s%{NUMBER:bytes}\\s%{WORD:request_method}\\s%{NOTSPACE:url}\\s(%{NOTSPACE:user}|-)\\s%{WORD:hierarchy_code}/%{IPORHOST:server}\\s%{NOTSPACE:content_type}"}
[2020-05-26T17:18:06,145][DEBUG][logstash.filters.grok    ][main] replacement_pattern => (?:%{HTTPD_COMBINEDLOG})
[2020-05-26T17:18:06,147][DEBUG][logstash.filters.grok    ][main] replacement_pattern => (?:%{HTTPD_COMMONLOG} %{QS:referrer} %{QS:agent})
[2020-05-26T17:18:06,148][DEBUG][logstash.filters.grok    ][main] replacement_pattern => (?:%{IPORHOST:clientip} %{HTTPDUSER:ident} %{HTTPDUSER:auth} \[%{HTTPDATE:timestamp}\] "(?:%{WORD:verb} %{NOTSPACE:request}(?: HTTP/%{NUMBER:httpversion})?|%{DATA:rawrequest})" %{NUMBER:response} (?:%{NUMBER:bytes}|-))
[2020-05-26T17:18:06,150][DEBUG][logstash.filters.grok    ][main] replacement_pattern => (?<IPORHOST:clientip>(?:%{IP}|%{HOSTNAME}))
[2020-05-26T17:18:06,155][DEBUG][logstash.filters.grok    ][main] replacement_pattern => (?:(?:%{IPV6}|%{IPV4}))
[2020-05-26T17:18:06,158][DEBUG][logstash.filters.grok    ][main] replacement_pattern => (?:((([0-9A-Fa-f]{1,4}:){7}([0-9A-Fa-f]{1,4}|:))|(([0-9A-Fa-f]{1,4}:){6}(:[0-9A-Fa-f]{1,4}|((25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)(\.(25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)){3})|:))|(([0-9A-Fa-f]{1,4}:){5}(((:[0-9A-Fa-f]{1,4}){1,2})|:((25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)(\.(25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)){3})|:))|(([0-9A-Fa-f]{1,4}:){4}(((:[0-9A-Fa-f]{1,4}){1,3})|((:[0-9A-Fa-f]{1,4})?:((25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)(\.(25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)){3}))|:))|(([0-9A-Fa-f]{1,4}:){3}(((:[0-9A-Fa-f]{1,4}){1,4})|((:[0-9A-Fa-f]{1,4}){0,2}:((25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)(\.(25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)){3}))|:))|(([0-9A-Fa-f]{1,4}:){2}(((:[0-9A-Fa-f]{1,4}){1,5})|((:[0-9A-Fa-f]{1,4}){0,3}:((25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)(\.(25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)){3}))|:))|(([0-9A-Fa-f]{1,4}:){1}(((:[0-9A-Fa-f]{1,4}){1,6})|((:[0-9A-Fa-f]{1,4}){0,4}:((25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)(\.(25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)){3}))|:))|(:(((:[0-9A-Fa-f]{1,4}){1,7})|((:[0-9A-Fa-f]{1,4}){0,5}:((25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)(\.(25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)){3}))|:)))(%.+)?)
[2020-05-26T17:18:06,160][DEBUG][logstash.filters.grok    ][main] replacement_pattern => (?:(?<![0-9])(?:(?:[0-1]?[0-9]{1,2}|2[0-4][0-9]|25[0-5])[.](?:[0-1]?[0-9]{1,2}|2[0-4][0-9]|25[0-5])[.](?:[0-1]?[0-9]{1,2}|2[0-4][0-9]|25[0-5])[.](?:[0-1]?[0-9]{1,2}|2[0-4][0-9]|25[0-5]))(?![0-9]))
[2020-05-26T17:18:06,161][DEBUG][logstash.filters.grok    ][main] replacement_pattern => (?:\b(?:[0-9A-Za-z][0-9A-Za-z-]{0,62})(?:\.(?:[0-9A-Za-z][0-9A-Za-z-]{0,62}))*(\.?|\b))
[2020-05-26T17:18:06,162][DEBUG][logstash.filters.grok    ][main] replacement_pattern => (?<HTTPDUSER:ident>%{EMAILADDRESS}|%{USER})
[2020-05-26T17:18:06,163][DEBUG][logstash.filters.grok    ][main] replacement_pattern => (?:%{EMAILLOCALPART}@%{HOSTNAME})
[2020-05-26T17:18:06,172][DEBUG][logstash.filters.grok    ][main] replacement_pattern => (?:[a-zA-Z][a-zA-Z0-9_.+-=:]+)
[2020-05-26T17:18:06,175][DEBUG][logstash.filters.grok    ][main] replacement_pattern => (?:\b(?:[0-9A-Za-z][0-9A-Za-z-]{0,62})(?:\.(?:[0-9A-Za-z][0-9A-Za-z-]{0,62}))*(\.?|\b))
[2020-05-26T17:18:06,176][DEBUG][logstash.filters.grok    ][main] replacement_pattern => (?:%{USERNAME})
[2020-05-26T17:18:06,177][DEBUG][logstash.filters.grok    ][main] replacement_pattern => (?:[a-zA-Z0-9._-]+)
[2020-05-26T17:18:06,178][DEBUG][logstash.filters.grok    ][main] replacement_pattern => (?<HTTPDUSER:auth>%{EMAILADDRESS}|%{USER})
[2020-05-26T17:18:06,179][DEBUG][logstash.filters.grok    ][main] replacement_pattern => (?:%{EMAILLOCALPART}@%{HOSTNAME})
[2020-05-26T17:18:06,180][DEBUG][logstash.filters.grok    ][main] replacement_pattern => (?:[a-zA-Z][a-zA-Z0-9_.+-=:]+)
[2020-05-26T17:18:06,187][DEBUG][logstash.filters.grok    ][main] replacement_pattern => (?:\b(?:[0-9A-Za-z][0-9A-Za-z-]{0,62})(?:\.(?:[0-9A-Za-z][0-9A-Za-z-]{0,62}))*(\.?|\b))
[2020-05-26T17:18:06,191][DEBUG][logstash.filters.grok    ][main] replacement_pattern => (?:%{USERNAME})
[2020-05-26T17:18:06,192][DEBUG][logstash.filters.grok    ][main] replacement_pattern => (?:[a-zA-Z0-9._-]+)
[2020-05-26T17:18:06,192][DEBUG][logstash.filters.grok    ][main] replacement_pattern => (?<HTTPDATE:timestamp>%{MONTHDAY}/%{MONTH}/%{YEAR}:%{TIME} %{INT})
[2020-05-26T17:18:06,193][DEBUG][logstash.filters.grok    ][main] replacement_pattern => (?:(?:(?:0[1-9])|(?:[12][0-9])|(?:3[01])|[1-9]))
[2020-05-26T17:18:06,194][DEBUG][logstash.filters.grok    ][main] replacement_pattern => (?:\b(?:[Jj]an(?:uary|uar)?|[Ff]eb(?:ruary|ruar)?|[Mm](?:a|)?r(?:ch|z)?|[Aa]pr(?:il)?|[Mm]a(?:y|i)?|[Jj]un(?:e|i)?|[Jj]ul(?:y)?|[Aa]ug(?:ust)?|[Ss]ep(?:tember)?|[Oo](?:c|k)?t(?:ober)?|[Nn]ov(?:ember)?|[Dd]e(?:c|z)(?:ember)?)\b)
[2020-05-26T17:18:06,197][DEBUG][logstash.filters.grok    ][main] replacement_pattern => (?:(?>\d\d){1,2})
[2020-05-26T17:18:06,205][DEBUG][logstash.filters.grok    ][main] replacement_pattern => (?:(?!<[0-9])%{HOUR}:%{MINUTE}(?::%{SECOND})(?![0-9]))
[2020-05-26T17:18:06,208][DEBUG][logstash.filters.grok    ][main] replacement_pattern => (?:(?:2[0123]|[01]?[0-9]))
[2020-05-26T17:18:06,209][DEBUG][logstash.filters.grok    ][main] replacement_pattern => (?:(?:[0-5][0-9]))
[2020-05-26T17:18:06,210][DEBUG][logstash.filters.grok    ][main] replacement_pattern => (?:(?:(?:[0-5]?[0-9]|60)(?:[:.,][0-9]+)?))
[2020-05-26T17:18:06,211][DEBUG][logstash.filters.grok    ][main] replacement_pattern => (?:(?:[+-]?(?:[0-9]+)))
[2020-05-26T17:18:06,212][DEBUG][logstash.filters.grok    ][main] replacement_pattern => (?<WORD:verb>\b\w+\b)
[2020-05-26T17:18:06,219][DEBUG][logstash.filters.grok    ][main] replacement_pattern => (?<NOTSPACE:request>\S+)
[2020-05-26T17:18:06,222][DEBUG][logstash.filters.grok    ][main] replacement_pattern => (?<NUMBER:httpversion>(?:%{BASE10NUM}))
[2020-05-26T17:18:06,223][DEBUG][logstash.filters.grok    ][main] replacement_pattern => (?:(?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\.[0-9]+)?)|(?:\.[0-9]+))))
[2020-05-26T17:18:06,227][DEBUG][logstash.filters.grok    ][main] replacement_pattern => (?<DATA:rawrequest>.*?)
[2020-05-26T17:18:06,232][DEBUG][logstash.filters.grok    ][main] replacement_pattern => (?<NUMBER:response>(?:%{BASE10NUM}))
[2020-05-26T17:18:06,236][DEBUG][logstash.filters.grok    ][main] replacement_pattern => (?:(?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\.[0-9]+)?)|(?:\.[0-9]+))))
[2020-05-26T17:18:06,237][DEBUG][logstash.filters.grok    ][main] replacement_pattern => (?<NUMBER:bytes>(?:%{BASE10NUM}))
[2020-05-26T17:18:06,238][DEBUG][logstash.filters.grok    ][main] replacement_pattern => (?:(?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\.[0-9]+)?)|(?:\.[0-9]+))))
[2020-05-26T17:18:06,240][DEBUG][logstash.filters.grok    ][main] replacement_pattern => (?<QS:referrer>%{QUOTEDSTRING})
[2020-05-26T17:18:06,240][DEBUG][logstash.filters.grok    ][main] replacement_pattern => (?:(?>(?<!\\)(?>"(?>\\.|[^\\"]+)+"|""|(?>'(?>\\.|[^\\']+)+')|''|(?>`(?>\\.|[^\\`]+)+`)|``)))
[2020-05-26T17:18:06,250][DEBUG][logstash.filters.grok    ][main] replacement_pattern => (?<QS:agent>%{QUOTEDSTRING})
[2020-05-26T17:18:06,254][DEBUG][logstash.filters.grok    ][main] replacement_pattern => (?:(?>(?<!\\)(?>"(?>\\.|[^\\"]+)+"|""|(?>'(?>\\.|[^\\']+)+')|''|(?>`(?>\\.|[^\\`]+)+`)|``)))
[2020-05-26T17:18:06,288][DEBUG][logstash.filters.grok    ][main] Grok compiled OK {:pattern=>"%{COMBINEDAPACHELOG}", :expanded_pattern=>"(?:(?:(?:(?<IPORHOST:clientip>(?:(?:(?:(?:((([0-9A-Fa-f]{1,4}:){7}([0-9A-Fa-f]{1,4}|:))|(([0-9A-Fa-f]{1,4}:){6}(:[0-9A-Fa-f]{1,4}|((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3})|:))|(([0-9A-Fa-f]{1,4}:){5}(((:[0-9A-Fa-f]{1,4}){1,2})|:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3})|:))|(([0-9A-Fa-f]{1,4}:){4}(((:[0-9A-Fa-f]{1,4}){1,3})|((:[0-9A-Fa-f]{1,4})?:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:))|(([0-9A-Fa-f]{1,4}:){3}(((:[0-9A-Fa-f]{1,4}){1,4})|((:[0-9A-Fa-f]{1,4}){0,2}:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:))|(([0-9A-Fa-f]{1,4}:){2}(((:[0-9A-Fa-f]{1,4}){1,5})|((:[0-9A-Fa-f]{1,4}){0,3}:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:))|(([0-9A-Fa-f]{1,4}:){1}(((:[0-9A-Fa-f]{1,4}){1,6})|((:[0-9A-Fa-f]{1,4}){0,4}:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:))|(:(((:[0-9A-Fa-f]{1,4}){1,7})|((:[0-9A-Fa-f]{1,4}){0,5}:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:)))(%.+)?)|(?:(?<![0-9])(?:(?:[0-1]?[0-9]{1,2}|2[0-4][0-9]|25[0-5])[.](?:[0-1]?[0-9]{1,2}|2[0-4][0-9]|25[0-5])[.](?:[0-1]?[0-9]{1,2}|2[0-4][0-9]|25[0-5])[.](?:[0-1]?[0-9]{1,2}|2[0-4][0-9]|25[0-5]))(?![0-9]))))|(?:\\b(?:[0-9A-Za-z][0-9A-Za-z-]{0,62})(?:\\.(?:[0-9A-Za-z][0-9A-Za-z-]{0,62}))*(\\.?|\\b)))) (?<HTTPDUSER:ident>(?:(?:[a-zA-Z][a-zA-Z0-9_.+-=:]+)@(?:\\b(?:[0-9A-Za-z][0-9A-Za-z-]{0,62})(?:\\.(?:[0-9A-Za-z][0-9A-Za-z-]{0,62}))*(\\.?|\\b)))|(?:(?:[a-zA-Z0-9._-]+))) (?<HTTPDUSER:auth>(?:(?:[a-zA-Z][a-zA-Z0-9_.+-=:]+)@(?:\\b(?:[0-9A-Za-z][0-9A-Za-z-]{0,62})(?:\\.(?:[0-9A-Za-z][0-9A-Za-z-]{0,62}))*(\\.?|\\b)))|(?:(?:[a-zA-Z0-9._-]+))) \\[(?<HTTPDATE:timestamp>(?:(?:(?:0[1-9])|(?:[12][0-9])|(?:3[01])|[1-9]))/(?:\\b(?:[Jj]an(?:uary|uar)?|[Ff]eb(?:ruary|ruar)?|[Mm](?:a|)?r(?:ch|z)?|[Aa]pr(?:il)?|[Mm]a(?:y|i)?|[Jj]un(?:e|i)?|[Jj]ul(?:y)?|[Aa]ug(?:ust)?|[Ss]ep(?:tember)?|[Oo](?:c|k)?t(?:ober)?|[Nn]ov(?:ember)?|[Dd]e(?:c|z)(?:ember)?)\\b)/(?:(?>\\d\\d){1,2}):(?:(?!<[0-9])(?:(?:2[0123]|[01]?[0-9])):(?:(?:[0-5][0-9]))(?::(?:(?:(?:[0-5]?[0-9]|60)(?:[:.,][0-9]+)?)))(?![0-9])) (?:(?:[+-]?(?:[0-9]+))))\\] \"(?:(?<WORD:verb>\\b\\w+\\b) (?<NOTSPACE:request>\\S+)(?: HTTP/(?<NUMBER:httpversion>(?:(?:(?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\\.[0-9]+)?)|(?:\\.[0-9]+)))))))?|(?<DATA:rawrequest>.*?))\" (?<NUMBER:response>(?:(?:(?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\\.[0-9]+)?)|(?:\\.[0-9]+)))))) (?:(?<NUMBER:bytes>(?:(?:(?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\\.[0-9]+)?)|(?:\\.[0-9]+))))))|-)) (?<QS:referrer>(?:(?>(?<!\\\\)(?>\"(?>\\\\.|[^\\\\\"]+)+\"|\"\"|(?>'(?>\\\\.|[^\\\\']+)+')|''|(?>`(?>\\\\.|[^\\\\`]+)+`)|``)))) (?<QS:agent>(?:(?>(?<!\\\\)(?>\"(?>\\\\.|[^\\\\\"]+)+\"|\"\"|(?>'(?>\\\\.|[^\\\\']+)+')|''|(?>`(?>\\\\.|[^\\\\`]+)+`)|``))))))"}
[2020-05-26T17:18:06,398][WARN ][org.logstash.instrument.metrics.gauge.LazyDelegatingGauge][main] A gauge metric of an unknown type (org.jruby.RubyArray) has been created for key: cluster_uuids. This may result in invalid serialization.  It is recommended to log an issue to the responsible developer/development team.
[2020-05-26T17:18:06,403][INFO ][logstash.javapipeline    ][main] Starting pipeline {:pipeline_id=>"main", "pipeline.workers"=>8, "pipeline.batch.size"=>125, "pipeline.batch.delay"=>50, "pipeline.max_inflight"=>1000, "pipeline.sources"=>["D:/Softwares/logstash-7.7.0/config/logstash-sample.conf"], :thread=>"#<Thread:0x422164e1 run>"}
[2020-05-26T17:18:06,432][DEBUG][logstash.instrument.periodicpoller.cgroup] One or more required cgroup files or directories not found: /proc/self/cgroup, /sys/fs/cgroup/cpuacct, /sys/fs/cgroup/cpu
[2020-05-26T17:18:06,583][DEBUG][logstash.instrument.periodicpoller.jvm] collector name {:name=>"ParNew"}
[2020-05-26T17:18:06,585][DEBUG][logstash.instrument.periodicpoller.jvm] collector name {:name=>"ConcurrentMarkSweep"}
[2020-05-26T17:18:06,958][DEBUG][org.logstash.config.ir.CompiledPipeline][main] Compiled filter
 P[filter-grok{"match"=>{"message"=>"%{COMBINEDAPACHELOG}"}}|[file]D:/Softwares/logstash-7.7.0/config/logstash-sample.conf:4:3:```
grok {
    match => { "message" => "%{COMBINEDAPACHELOG}" }
  }
```] 
 into 
 org.logstash.config.ir.compiler.ComputeStepSyntaxElement@5b7fe14a
[2020-05-26T17:18:06,958][DEBUG][org.logstash.config.ir.CompiledPipeline][main] Compiled filter
 P[filter-grok{"match"=>{"message"=>"%{COMBINEDAPACHELOG}"}}|[file]D:/Softwares/logstash-7.7.0/config/logstash-sample.conf:4:3:```
grok {
    match => { "message" => "%{COMBINEDAPACHELOG}" }
  }
```] 
 into 
 org.logstash.config.ir.compiler.ComputeStepSyntaxElement@5b7fe14a
[2020-05-26T17:18:06,958][DEBUG][org.logstash.config.ir.CompiledPipeline][main] Compiled filter
 P[filter-grok{"match"=>{"message"=>"%{COMBINEDAPACHELOG}"}}|[file]D:/Softwares/logstash-7.7.0/config/logstash-sample.conf:4:3:```
grok {
    match => { "message" => "%{COMBINEDAPACHELOG}" }
  }
```] 
 into 
 org.logstash.config.ir.compiler.ComputeStepSyntaxElement@5b7fe14a
[2020-05-26T17:18:06,958][DEBUG][org.logstash.config.ir.CompiledPipeline][main] Compiled filter
 P[filter-grok{"match"=>{"message"=>"%{COMBINEDAPACHELOG}"}}|[file]D:/Softwares/logstash-7.7.0/config/logstash-sample.conf:4:3:```
grok {
    match => { "message" => "%{COMBINEDAPACHELOG}" }
  }
```] 
 into 
 org.logstash.config.ir.compiler.ComputeStepSyntaxElement@5b7fe14a
[2020-05-26T17:18:06,958][DEBUG][org.logstash.config.ir.CompiledPipeline][main] Compiled filter
 P[filter-grok{"match"=>{"message"=>"%{COMBINEDAPACHELOG}"}}|[file]D:/Softwares/logstash-7.7.0/config/logstash-sample.conf:4:3:```
grok {
    match => { "message" => "%{COMBINEDAPACHELOG}" }
  }
```] 
 into 
 org.logstash.config.ir.compiler.ComputeStepSyntaxElement@5b7fe14a
[2020-05-26T17:18:06,958][DEBUG][org.logstash.config.ir.CompiledPipeline][main] Compiled filter
 P[filter-grok{"match"=>{"message"=>"%{COMBINEDAPACHELOG}"}}|[file]D:/Softwares/logstash-7.7.0/config/logstash-sample.conf:4:3:```
grok {
    match => { "message" => "%{COMBINEDAPACHELOG}" }
  }
```] 
 into 
 org.logstash.config.ir.compiler.ComputeStepSyntaxElement@5b7fe14a
[2020-05-26T17:18:06,958][DEBUG][org.logstash.config.ir.CompiledPipeline][main] Compiled filter
 P[filter-grok{"match"=>{"message"=>"%{COMBINEDAPACHELOG}"}}|[file]D:/Softwares/logstash-7.7.0/config/logstash-sample.conf:4:3:```
grok {
    match => { "message" => "%{COMBINEDAPACHELOG}" }
  }
```] 
 into 
 org.logstash.config.ir.compiler.ComputeStepSyntaxElement@5b7fe14a
[2020-05-26T17:18:06,958][DEBUG][org.logstash.config.ir.CompiledPipeline][main] Compiled filter
 P[filter-grok{"match"=>{"message"=>"%{COMBINEDAPACHELOG}"}}|[file]D:/Softwares/logstash-7.7.0/config/logstash-sample.conf:4:3:```
grok {
    match => { "message" => "%{COMBINEDAPACHELOG}" }
  }
```] 
 into 
 org.logstash.config.ir.compiler.ComputeStepSyntaxElement@5b7fe14a
[2020-05-26T17:18:07,104][DEBUG][org.logstash.config.ir.CompiledPipeline][main] Compiled filter
 P[filter-date{"match"=>["timestamp", "dd/MMM/yyyy:HH:mm:ss Z"]}|[file]D:/Softwares/logstash-7.7.0/config/logstash-sample.conf:7:3:```
date {
    match => [ "timestamp" , "dd/MMM/yyyy:HH:mm:ss Z" ]
  }
```] 
 into 
 org.logstash.config.ir.compiler.ComputeStepSyntaxElement@fde68ed3
[2020-05-26T17:18:07,106][DEBUG][org.logstash.config.ir.CompiledPipeline][main] Compiled filter
 P[filter-date{"match"=>["timestamp", "dd/MMM/yyyy:HH:mm:ss Z"]}|[file]D:/Softwares/logstash-7.7.0/config/logstash-sample.conf:7:3:```
date {
    match => [ "timestamp" , "dd/MMM/yyyy:HH:mm:ss Z" ]
  }
```] 
 into 
 org.logstash.config.ir.compiler.ComputeStepSyntaxElement@fde68ed3
[2020-05-26T17:18:07,116][DEBUG][org.logstash.config.ir.CompiledPipeline][main] Compiled filter
 P[filter-date{"match"=>["timestamp", "dd/MMM/yyyy:HH:mm:ss Z"]}|[file]D:/Softwares/logstash-7.7.0/config/logstash-sample.conf:7:3:```
date {
    match => [ "timestamp" , "dd/MMM/yyyy:HH:mm:ss Z" ]
  }
```] 
 into 
 org.logstash.config.ir.compiler.ComputeStepSyntaxElement@fde68ed3
[2020-05-26T17:18:07,113][DEBUG][org.logstash.config.ir.CompiledPipeline][main] Compiled filter
 P[filter-date{"match"=>["timestamp", "dd/MMM/yyyy:HH:mm:ss Z"]}|[file]D:/Softwares/logstash-7.7.0/config/logstash-sample.conf:7:3:```
date {
    match => [ "timestamp" , "dd/MMM/yyyy:HH:mm:ss Z" ]
  }
```] 
 into 
 org.logstash.config.ir.compiler.ComputeStepSyntaxElement@fde68ed3
[2020-05-26T17:18:07,110][DEBUG][org.logstash.config.ir.CompiledPipeline][main] Compiled filter
 P[filter-date{"match"=>["timestamp", "dd/MMM/yyyy:HH:mm:ss Z"]}|[file]D:/Softwares/logstash-7.7.0/config/logstash-sample.conf:7:3:```
date {
    match => [ "timestamp" , "dd/MMM/yyyy:HH:mm:ss Z" ]
  }
```] 
 into 
 org.logstash.config.ir.compiler.ComputeStepSyntaxElement@fde68ed3
[2020-05-26T17:18:07,108][DEBUG][org.logstash.config.ir.CompiledPipeline][main] Compiled filter
 P[filter-date{"match"=>["timestamp", "dd/MMM/yyyy:HH:mm:ss Z"]}|[file]D:/Softwares/logstash-7.7.0/config/logstash-sample.conf:7:3:```
date {
    match => [ "timestamp" , "dd/MMM/yyyy:HH:mm:ss Z" ]
  }
```] 
 into 
 org.logstash.config.ir.compiler.ComputeStepSyntaxElement@fde68ed3
[2020-05-26T17:18:07,108][DEBUG][org.logstash.config.ir.CompiledPipeline][main] Compiled filter
 P[filter-date{"match"=>["timestamp", "dd/MMM/yyyy:HH:mm:ss Z"]}|[file]D:/Softwares/logstash-7.7.0/config/logstash-sample.conf:7:3:```
date {
    match => [ "timestamp" , "dd/MMM/yyyy:HH:mm:ss Z" ]
  }
```] 
 into 
 org.logstash.config.ir.compiler.ComputeStepSyntaxElement@fde68ed3
[2020-05-26T17:18:07,107][DEBUG][org.logstash.config.ir.CompiledPipeline][main] Compiled filter
 P[filter-date{"match"=>["timestamp", "dd/MMM/yyyy:HH:mm:ss Z"]}|[file]D:/Softwares/logstash-7.7.0/config/logstash-sample.conf:7:3:```
date {
    match => [ "timestamp" , "dd/MMM/yyyy:HH:mm:ss Z" ]
  }
```] 
 into 
 org.logstash.config.ir.compiler.ComputeStepSyntaxElement@fde68ed3
[2020-05-26T17:18:07,141][DEBUG][org.logstash.config.ir.CompiledPipeline][main] Compiled output
 P[output-elasticsearch{"hosts"=>["localhost:9200"]}|[file]D:/Softwares/logstash-7.7.0/config/logstash-sample.conf:13:3:```
elasticsearch { hosts => ["localhost:9200"] }
```] 
 into 
 org.logstash.config.ir.compiler.ComputeStepSyntaxElement@aee1feb0
[2020-05-26T17:18:07,141][DEBUG][org.logstash.config.ir.CompiledPipeline][main] Compiled output
 P[output-elasticsearch{"hosts"=>["localhost:9200"]}|[file]D:/Softwares/logstash-7.7.0/config/logstash-sample.conf:13:3:```
elasticsearch { hosts => ["localhost:9200"] }
```] 
 into 
 org.logstash.config.ir.compiler.ComputeStepSyntaxElement@aee1feb0
[2020-05-26T17:18:07,141][DEBUG][org.logstash.config.ir.CompiledPipeline][main] Compiled output
 P[output-elasticsearch{"hosts"=>["localhost:9200"]}|[file]D:/Softwares/logstash-7.7.0/config/logstash-sample.conf:13:3:```
elasticsearch { hosts => ["localhost:9200"] }
```] 
 into 
 org.logstash.config.ir.compiler.ComputeStepSyntaxElement@aee1feb0
[2020-05-26T17:18:07,138][DEBUG][org.logstash.config.ir.CompiledPipeline][main] Compiled output
 P[output-elasticsearch{"hosts"=>["localhost:9200"]}|[file]D:/Softwares/logstash-7.7.0/config/logstash-sample.conf:13:3:```
elasticsearch { hosts => ["localhost:9200"] }
```] 
 into 
 org.logstash.config.ir.compiler.ComputeStepSyntaxElement@aee1feb0
[2020-05-26T17:18:07,140][DEBUG][org.logstash.config.ir.CompiledPipeline][main] Compiled output
 P[output-elasticsearch{"hosts"=>["localhost:9200"]}|[file]D:/Softwares/logstash-7.7.0/config/logstash-sample.conf:13:3:```
elasticsearch { hosts => ["localhost:9200"] }
```] 
 into 
 org.logstash.config.ir.compiler.ComputeStepSyntaxElement@aee1feb0
[2020-05-26T17:18:07,135][DEBUG][org.logstash.config.ir.CompiledPipeline][main] Compiled output
 P[output-elasticsearch{"hosts"=>["localhost:9200"]}|[file]D:/Softwares/logstash-7.7.0/config/logstash-sample.conf:13:3:```
elasticsearch { hosts => ["localhost:9200"] }
```] 
 into 
 org.logstash.config.ir.compiler.ComputeStepSyntaxElement@aee1feb0
[2020-05-26T17:18:07,148][DEBUG][org.logstash.config.ir.CompiledPipeline][main] Compiled output
 P[output-elasticsearch{"hosts"=>["localhost:9200"]}|[file]D:/Softwares/logstash-7.7.0/config/logstash-sample.conf:13:3:```
elasticsearch { hosts => ["localhost:9200"] }
```] 
 into 
 org.logstash.config.ir.compiler.ComputeStepSyntaxElement@aee1feb0
[2020-05-26T17:18:07,153][DEBUG][org.logstash.config.ir.CompiledPipeline][main] Compiled output
 P[output-elasticsearch{"hosts"=>["localhost:9200"]}|[file]D:/Softwares/logstash-7.7.0/config/logstash-sample.conf:13:3:```
elasticsearch { hosts => ["localhost:9200"] }
```] 
 into 
 org.logstash.config.ir.compiler.ComputeStepSyntaxElement@aee1feb0
[2020-05-26T17:18:07,175][DEBUG][org.logstash.config.ir.CompiledPipeline][main] Compiled output
 P[output-stdout{"codec"=>"rubydebug"}|[file]D:/Softwares/logstash-7.7.0/config/logstash-sample.conf:14:3:```
stdout { codec => rubydebug }
```] 
 into 
 org.logstash.config.ir.compiler.ComputeStepSyntaxElement@72d124b5
[2020-05-26T17:18:07,175][DEBUG][org.logstash.config.ir.CompiledPipeline][main] Compiled output
 P[output-stdout{"codec"=>"rubydebug"}|[file]D:/Softwares/logstash-7.7.0/config/logstash-sample.conf:14:3:```
stdout { codec => rubydebug }
```] 
 into 
 org.logstash.config.ir.compiler.ComputeStepSyntaxElement@72d124b5
[2020-05-26T17:18:07,172][DEBUG][org.logstash.config.ir.CompiledPipeline][main] Compiled output
 P[output-stdout{"codec"=>"rubydebug"}|[file]D:/Softwares/logstash-7.7.0/config/logstash-sample.conf:14:3:```
stdout { codec => rubydebug }
```] 
 into 
 org.logstash.config.ir.compiler.ComputeStepSyntaxElement@72d124b5
[2020-05-26T17:18:07,174][DEBUG][org.logstash.config.ir.CompiledPipeline][main] Compiled output
 P[output-stdout{"codec"=>"rubydebug"}|[file]D:/Softwares/logstash-7.7.0/config/logstash-sample.conf:14:3:```
stdout { codec => rubydebug }
```] 
 into 
 org.logstash.config.ir.compiler.ComputeStepSyntaxElement@72d124b5
[2020-05-26T17:18:07,167][DEBUG][org.logstash.config.ir.CompiledPipeline][main] Compiled output
 P[output-stdout{"codec"=>"rubydebug"}|[file]D:/Softwares/logstash-7.7.0/config/logstash-sample.conf:14:3:```
stdout { codec => rubydebug }
```] 
 into 
 org.logstash.config.ir.compiler.ComputeStepSyntaxElement@72d124b5
[2020-05-26T17:18:07,166][DEBUG][org.logstash.config.ir.CompiledPipeline][main] Compiled output
 P[output-stdout{"codec"=>"rubydebug"}|[file]D:/Softwares/logstash-7.7.0/config/logstash-sample.conf:14:3:```
stdout { codec => rubydebug }
```] 
 into 
 org.logstash.config.ir.compiler.ComputeStepSyntaxElement@72d124b5
[2020-05-26T17:18:07,164][DEBUG][org.logstash.config.ir.CompiledPipeline][main] Compiled output
 P[output-stdout{"codec"=>"rubydebug"}|[file]D:/Softwares/logstash-7.7.0/config/logstash-sample.conf:14:3:```
stdout { codec => rubydebug }
```] 
 into 
 org.logstash.config.ir.compiler.ComputeStepSyntaxElement@72d124b5
[2020-05-26T17:18:07,185][DEBUG][org.logstash.config.ir.CompiledPipeline][main] Compiled output
 P[output-stdout{"codec"=>"rubydebug"}|[file]D:/Softwares/logstash-7.7.0/config/logstash-sample.conf:14:3:```
stdout { codec => rubydebug }
```] 
 into 
 org.logstash.config.ir.compiler.ComputeStepSyntaxElement@72d124b5
[2020-05-26T17:18:07,214][DEBUG][logstash.outputs.elasticsearch][main] Waiting for connectivity to Elasticsearch cluster. Retrying in 4s
[2020-05-26T17:18:07,281][INFO ][logstash.javapipeline    ][main] Pipeline started {"pipeline.id"=>"main"}
[2020-05-26T17:18:07,297][DEBUG][logstash.javapipeline    ] Pipeline started successfully {:pipeline_id=>"main", :thread=>"#<Thread:0x422164e1 run>"}
[2020-05-26T17:18:07,314][DEBUG][org.logstash.execution.PeriodicFlush][main] Pushing flush onto pipeline.
[2020-05-26T17:18:07,352][INFO ][logstash.agent           ] Pipelines running {:count=>1, :running_pipelines=>[:main], :non_running_pipelines=>[]}
[2020-05-26T17:18:07,375][DEBUG][logstash.agent           ] Starting puma
[2020-05-26T17:18:07,388][DEBUG][logstash.agent           ] Trying to start WebServer {:port=>9600}
[2020-05-26T17:18:07,423][DEBUG][logstash.api.service     ] [api-service] start
[2020-05-26T17:18:07,574][INFO ][logstash.agent           ] Successfully started Logstash API endpoint {:port=>9600}
[2020-05-26T17:18:10,189][DEBUG][logstash.outputs.elasticsearch][main] Running health check to see if an Elasticsearch connection is working {:healthcheck_url=>http://localhost:9200/, :path=>"/"}
[2020-05-26T17:18:11,227][DEBUG][logstash.outputs.elasticsearch][main] Waiting for connectivity to Elasticsearch cluster. Retrying in 8s
[2020-05-26T17:18:11,443][DEBUG][logstash.instrument.periodicpoller.cgroup] One or more required cgroup files or directories not found: /proc/self/cgroup, /sys/fs/cgroup/cpuacct, /sys/fs/cgroup/cpu
[2020-05-26T17:18:11,600][DEBUG][logstash.instrument.periodicpoller.jvm] collector name {:name=>"ParNew"}
[2020-05-26T17:18:11,602][DEBUG][logstash.instrument.periodicpoller.jvm] collector name {:name=>"ConcurrentMarkSweep"}
[2020-05-26T17:18:14,235][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:18:15,238][DEBUG][logstash.outputs.elasticsearch][main] Running health check to see if an Elasticsearch connection is working {:healthcheck_url=>http://localhost:9200/, :path=>"/"}
[2020-05-26T17:18:16,450][DEBUG][logstash.instrument.periodicpoller.cgroup] One or more required cgroup files or directories not found: /proc/self/cgroup, /sys/fs/cgroup/cpuacct, /sys/fs/cgroup/cpu
[2020-05-26T17:18:16,615][DEBUG][logstash.instrument.periodicpoller.jvm] collector name {:name=>"ParNew"}
[2020-05-26T17:18:16,623][DEBUG][logstash.instrument.periodicpoller.jvm] collector name {:name=>"ConcurrentMarkSweep"}
[2020-05-26T17:18:19,230][DEBUG][logstash.outputs.elasticsearch][main] Waiting for connectivity to Elasticsearch cluster. Retrying in 16s
[2020-05-26T17:18:19,278][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:18:20,280][DEBUG][logstash.outputs.elasticsearch][main] Running health check to see if an Elasticsearch connection is working {:healthcheck_url=>http://localhost:9200/, :path=>"/"}
[2020-05-26T17:18:21,453][DEBUG][logstash.instrument.periodicpoller.cgroup] One or more required cgroup files or directories not found: /proc/self/cgroup, /sys/fs/cgroup/cpuacct, /sys/fs/cgroup/cpu
[2020-05-26T17:18:21,629][DEBUG][logstash.instrument.periodicpoller.jvm] collector name {:name=>"ParNew"}
[2020-05-26T17:18:21,631][DEBUG][logstash.instrument.periodicpoller.jvm] collector name {:name=>"ConcurrentMarkSweep"}
[2020-05-26T17:18:24,313][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:18:25,315][DEBUG][logstash.outputs.elasticsearch][main] Running health check to see if an Elasticsearch connection is working {:healthcheck_url=>http://localhost:9200/, :path=>"/"}
[2020-05-26T17:18:26,456][DEBUG][logstash.instrument.periodicpoller.cgroup] One or more required cgroup files or directories not found: /proc/self/cgroup, /sys/fs/cgroup/cpuacct, /sys/fs/cgroup/cpu
[2020-05-26T17:18:26,641][DEBUG][logstash.instrument.periodicpoller.jvm] collector name {:name=>"ParNew"}
[2020-05-26T17:18:26,642][DEBUG][logstash.instrument.periodicpoller.jvm] collector name {:name=>"ConcurrentMarkSweep"}
[2020-05-26T17:18:29,347][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:18:30,350][DEBUG][logstash.outputs.elasticsearch][main] Running health check to see if an Elasticsearch connection is working {:healthcheck_url=>http://localhost:9200/, :path=>"/"}
[2020-05-26T17:18:31,460][DEBUG][logstash.instrument.periodicpoller.cgroup] One or more required cgroup files or directories not found: /proc/self/cgroup, /sys/fs/cgroup/cpuacct, /sys/fs/cgroup/cpu
[2020-05-26T17:18:31,654][DEBUG][logstash.instrument.periodicpoller.jvm] collector name {:name=>"ParNew"}
[2020-05-26T17:18:31,656][DEBUG][logstash.instrument.periodicpoller.jvm] collector name {:name=>"ConcurrentMarkSweep"}
[2020-05-26T17:18:34,364][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:18:35,232][DEBUG][logstash.outputs.elasticsearch][main] Waiting for connectivity to Elasticsearch cluster. Retrying in 32s
[2020-05-26T17:18:35,367][DEBUG][logstash.outputs.elasticsearch][main] Running health check to see if an Elasticsearch connection is working {:healthcheck_url=>http://localhost:9200/, :path=>"/"}
[2020-05-26T17:18:36,464][DEBUG][logstash.instrument.periodicpoller.cgroup] One or more required cgroup files or directories not found: /proc/self/cgroup, /sys/fs/cgroup/cpuacct, /sys/fs/cgroup/cpu
[2020-05-26T17:18:36,670][DEBUG][logstash.instrument.periodicpoller.jvm] collector name {:name=>"ParNew"}
[2020-05-26T17:18:36,673][DEBUG][logstash.instrument.periodicpoller.jvm] collector name {:name=>"ConcurrentMarkSweep"}
[2020-05-26T17:18:39,389][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:18:40,393][DEBUG][logstash.outputs.elasticsearch][main] Running health check to see if an Elasticsearch connection is working {:healthcheck_url=>http://localhost:9200/, :path=>"/"}
[2020-05-26T17:18:41,471][DEBUG][logstash.instrument.periodicpoller.cgroup] One or more required cgroup files or directories not found: /proc/self/cgroup, /sys/fs/cgroup/cpuacct, /sys/fs/cgroup/cpu
[2020-05-26T17:18:41,686][DEBUG][logstash.instrument.periodicpoller.jvm] collector name {:name=>"ParNew"}
[2020-05-26T17:18:41,687][DEBUG][logstash.instrument.periodicpoller.jvm] collector name {:name=>"ConcurrentMarkSweep"}
[2020-05-26T17:18:44,424][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:18:45,427][DEBUG][logstash.outputs.elasticsearch][main] Running health check to see if an Elasticsearch connection is working {:healthcheck_url=>http://localhost:9200/, :path=>"/"}
[2020-05-26T17:18:46,475][DEBUG][logstash.instrument.periodicpoller.cgroup] One or more required cgroup files or directories not found: /proc/self/cgroup, /sys/fs/cgroup/cpuacct, /sys/fs/cgroup/cpu
[2020-05-26T17:18:46,704][DEBUG][logstash.instrument.periodicpoller.jvm] collector name {:name=>"ParNew"}
[2020-05-26T17:18:46,705][DEBUG][logstash.instrument.periodicpoller.jvm] collector name {:name=>"ConcurrentMarkSweep"}
[2020-05-26T17:18:49,454][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:18:49,863][DEBUG][logstash.inputs.stdin    ][main][1b16c8cd7b3b20dfc7c7e978b1ad233146b4bc0c146eab70dc15976197b28091] Stopping {:plugin=>"LogStash::Inputs::Stdin"}
[2020-05-26T17:18:49,875][WARN ][logstash.runner          ] SIGINT received. Shutting down.
[2020-05-26T17:18:49,881][DEBUG][logstash.javapipeline    ][main] Input plugins stopped! Will shutdown filter/output workers. {:pipeline_id=>"main", :thread=>"#<Thread:0x422164e1 run>"}
[2020-05-26T17:18:49,898][DEBUG][logstash.javapipeline    ][main] Shutdown waiting for worker thread {:pipeline_id=>"main", :thread=>"#<Thread:0x543fd973 sleep>"}
[2020-05-26T17:18:49,905][DEBUG][logstash.instrument.periodicpoller.os] Stopping
[2020-05-26T17:18:49,916][DEBUG][logstash.instrument.periodicpoller.jvm] Stopping
[2020-05-26T17:18:49,920][DEBUG][logstash.instrument.periodicpoller.persistentqueue] Stopping
[2020-05-26T17:18:49,923][DEBUG][logstash.instrument.periodicpoller.deadletterqueue] Stopping
[2020-05-26T17:18:49,940][DEBUG][logstash.agent           ] Shutting down all pipelines {:pipelines_count=>1}
[2020-05-26T17:18:49,948][DEBUG][logstash.agent           ] Converging pipelines state {:actions_count=>1}
[2020-05-26T17:18:49,960][DEBUG][logstash.agent           ] Executing action {:action=>LogStash::PipelineAction::Stop/pipeline_id:main}
[2020-05-26T17:18:49,979][DEBUG][logstash.javapipeline    ] Closing inputs {:pipeline_id=>"main", :thread=>"#<Thread:0x422164e1 sleep>"}
[2020-05-26T17:18:49,980][DEBUG][logstash.inputs.stdin    ] Stopping {:plugin=>"LogStash::Inputs::Stdin"}
[2020-05-26T17:18:49,989][DEBUG][logstash.javapipeline    ] Closed inputs {:pipeline_id=>"main", :thread=>"#<Thread:0x422164e1 sleep>"}
[2020-05-26T17:18:50,004][DEBUG][logstash.javapipeline    ] Closing inputs {:pipeline_id=>"main", :thread=>"#<Thread:0x422164e1 sleep>"}
[2020-05-26T17:18:50,456][DEBUG][logstash.outputs.elasticsearch][main] Running health check to see if an Elasticsearch connection is working {:healthcheck_url=>http://localhost:9200/, :path=>"/"}
[2020-05-26T17:18:54,486][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:18:54,895][WARN ][logstash.runner          ] Received shutdown signal, but pipeline is still waiting for in-flight events
to be processed. Sending another ^C will force quit Logstash, but this may cause
data loss.
[2020-05-26T17:18:55,065][WARN ][org.logstash.execution.ShutdownWatcherExt] {"inflight_count"=>0, "stalling_threads_info"=>{["LogStash::Filters::Grok", {"match"=>{"message"=>"%{COMBINEDAPACHELOG}"}, "id"=>"509a6b1d2b3b0f807de9cd789e204701a13144926a3ade41642843011405b83e"}]=>[{"thread_id"=>35, "name"=>"[main]>worker0", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>36, "name"=>"[main]>worker1", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>37, "name"=>"[main]>worker2", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>38, "name"=>"[main]>worker3", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>39, "name"=>"[main]>worker4", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>40, "name"=>"[main]>worker5", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>41, "name"=>"[main]>worker6", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>42, "name"=>"[main]>worker7", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}]}}
[2020-05-26T17:18:55,077][ERROR][org.logstash.execution.ShutdownWatcherExt] The shutdown process appears to be stalled due to busy or blocked plugins. Check the logs for more information.
[2020-05-26T17:18:55,489][DEBUG][logstash.outputs.elasticsearch][main] Running health check to see if an Elasticsearch connection is working {:healthcheck_url=>http://localhost:9200/, :path=>"/"}
[2020-05-26T17:18:59,523][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:19:00,127][WARN ][org.logstash.execution.ShutdownWatcherExt] {"inflight_count"=>0, "stalling_threads_info"=>{["LogStash::Filters::Grok", {"match"=>{"message"=>"%{COMBINEDAPACHELOG}"}, "id"=>"509a6b1d2b3b0f807de9cd789e204701a13144926a3ade41642843011405b83e"}]=>[{"thread_id"=>35, "name"=>"[main]>worker0", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>36, "name"=>"[main]>worker1", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>37, "name"=>"[main]>worker2", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>38, "name"=>"[main]>worker3", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>39, "name"=>"[main]>worker4", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>40, "name"=>"[main]>worker5", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>41, "name"=>"[main]>worker6", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>42, "name"=>"[main]>worker7", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}]}}
[2020-05-26T17:19:00,525][DEBUG][logstash.outputs.elasticsearch][main] Running health check to see if an Elasticsearch connection is working {:healthcheck_url=>http://localhost:9200/, :path=>"/"}
[2020-05-26T17:19:04,554][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:19:05,196][WARN ][org.logstash.execution.ShutdownWatcherExt] {"inflight_count"=>0, "stalling_threads_info"=>{["LogStash::Filters::Grok", {"match"=>{"message"=>"%{COMBINEDAPACHELOG}"}, "id"=>"509a6b1d2b3b0f807de9cd789e204701a13144926a3ade41642843011405b83e"}]=>[{"thread_id"=>35, "name"=>"[main]>worker0", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>36, "name"=>"[main]>worker1", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>37, "name"=>"[main]>worker2", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>38, "name"=>"[main]>worker3", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>39, "name"=>"[main]>worker4", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>40, "name"=>"[main]>worker5", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>41, "name"=>"[main]>worker6", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>42, "name"=>"[main]>worker7", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}]}}
[2020-05-26T17:19:05,557][DEBUG][logstash.outputs.elasticsearch][main] Running health check to see if an Elasticsearch connection is working {:healthcheck_url=>http://localhost:9200/, :path=>"/"}
[2020-05-26T17:19:07,234][DEBUG][logstash.outputs.elasticsearch][main] Waiting for connectivity to Elasticsearch cluster. Retrying in 64s
[2020-05-26T17:19:09,586][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:19:10,265][WARN ][org.logstash.execution.ShutdownWatcherExt] {"inflight_count"=>0, "stalling_threads_info"=>{["LogStash::Filters::Grok", {"match"=>{"message"=>"%{COMBINEDAPACHELOG}"}, "id"=>"509a6b1d2b3b0f807de9cd789e204701a13144926a3ade41642843011405b83e"}]=>[{"thread_id"=>35, "name"=>"[main]>worker0", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>36, "name"=>"[main]>worker1", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>37, "name"=>"[main]>worker2", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>38, "name"=>"[main]>worker3", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>39, "name"=>"[main]>worker4", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>40, "name"=>"[main]>worker5", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>41, "name"=>"[main]>worker6", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>42, "name"=>"[main]>worker7", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}]}}
[2020-05-26T17:19:10,590][DEBUG][logstash.outputs.elasticsearch][main] Running health check to see if an Elasticsearch connection is working {:healthcheck_url=>http://localhost:9200/, :path=>"/"}
[2020-05-26T17:19:14,423][FATAL][logstash.runner          ] SIGINT received. Terminating immediately..
[2020-05-26T17:19:14,622][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:19:14,625][ERROR][org.logstash.Logstash    ] org.jruby.exceptions.ThreadKill
[2020-05-26T17:19:14,634][DEBUG][logstash.agent           ] Error in reactor loop escaped: closed stream (IOError)
[2020-05-26T17:20:33,137][WARN ][logstash.config.source.multilocal] Ignoring the 'pipelines.yml' file because modules or command line options are specified
[2020-05-26T17:20:33,248][INFO ][logstash.runner          ] Starting Logstash {"logstash.version"=>"7.7.0"}
[2020-05-26T17:20:34,919][INFO ][org.reflections.Reflections] Reflections took 34 ms to scan 1 urls, producing 21 keys and 41 values 
[2020-05-26T17:20:36,908][INFO ][logstash.outputs.elasticsearch][main] Elasticsearch pool URLs updated {:changes=>{:removed=>[], :added=>[http://localhost:9200/]}}
[2020-05-26T17:20:41,099][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:20:41,119][INFO ][logstash.outputs.elasticsearch][main] New Elasticsearch output {:class=>"LogStash::Outputs::ElasticSearch", :hosts=>["//localhost:9200"]}
[2020-05-26T17:20:41,370][WARN ][org.logstash.instrument.metrics.gauge.LazyDelegatingGauge][main] A gauge metric of an unknown type (org.jruby.RubyArray) has been created for key: cluster_uuids. This may result in invalid serialization.  It is recommended to log an issue to the responsible developer/development team.
[2020-05-26T17:20:41,376][INFO ][logstash.javapipeline    ][main] Starting pipeline {:pipeline_id=>"main", "pipeline.workers"=>8, "pipeline.batch.size"=>125, "pipeline.batch.delay"=>50, "pipeline.max_inflight"=>1000, "pipeline.sources"=>["D:/Softwares/logstash-7.7.0/config/logstash-sample.conf"], :thread=>"#<Thread:0x2328ec55 run>"}
[2020-05-26T17:20:42,278][INFO ][logstash.javapipeline    ][main] Pipeline started {"pipeline.id"=>"main"}
[2020-05-26T17:20:42,332][INFO ][logstash.agent           ] Pipelines running {:count=>1, :running_pipelines=>[:main], :non_running_pipelines=>[]}
[2020-05-26T17:20:42,547][INFO ][logstash.agent           ] Successfully started Logstash API endpoint {:port=>9600}
[2020-05-26T17:20:50,167][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:20:55,205][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:21:00,228][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:21:05,256][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:21:10,292][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:21:14,077][WARN ][logstash.runner          ] SIGINT received. Shutting down.
[2020-05-26T17:21:15,227][FATAL][logstash.runner          ] SIGINT received. Terminating immediately..
[2020-05-26T17:21:15,327][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:21:15,336][ERROR][org.logstash.Logstash    ] org.jruby.exceptions.ThreadKill
[2020-05-26T17:21:43,753][WARN ][logstash.config.source.multilocal] Ignoring the 'pipelines.yml' file because modules or command line options are specified
[2020-05-26T17:21:43,851][INFO ][logstash.runner          ] Starting Logstash {"logstash.version"=>"7.7.0"}
[2020-05-26T17:21:45,363][INFO ][org.reflections.Reflections] Reflections took 34 ms to scan 1 urls, producing 21 keys and 41 values 
[2020-05-26T17:21:47,169][INFO ][logstash.outputs.elasticsearch][main] Elasticsearch pool URLs updated {:changes=>{:removed=>[], :added=>[http://localhost:9200/]}}
[2020-05-26T17:21:51,336][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:21:51,356][INFO ][logstash.outputs.elasticsearch][main] New Elasticsearch output {:class=>"LogStash::Outputs::ElasticSearch", :hosts=>["//localhost:9200"]}
[2020-05-26T17:21:51,585][WARN ][org.logstash.instrument.metrics.gauge.LazyDelegatingGauge][main] A gauge metric of an unknown type (org.jruby.RubyArray) has been created for key: cluster_uuids. This may result in invalid serialization.  It is recommended to log an issue to the responsible developer/development team.
[2020-05-26T17:21:51,591][INFO ][logstash.javapipeline    ][main] Starting pipeline {:pipeline_id=>"main", "pipeline.workers"=>8, "pipeline.batch.size"=>125, "pipeline.batch.delay"=>50, "pipeline.max_inflight"=>1000, "pipeline.sources"=>["D:/Softwares/logstash-7.7.0/config/logstash-sample.conf"], :thread=>"#<Thread:0x773e630f run>"}
[2020-05-26T17:21:52,414][INFO ][logstash.javapipeline    ][main] Pipeline started {"pipeline.id"=>"main"}
[2020-05-26T17:21:52,467][INFO ][logstash.agent           ] Pipelines running {:count=>1, :running_pipelines=>[:main], :non_running_pipelines=>[]}
[2020-05-26T17:21:52,676][INFO ][logstash.agent           ] Successfully started Logstash API endpoint {:port=>9600}
[2020-05-26T17:22:00,391][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:22:05,437][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:22:10,469][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:22:15,500][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:22:21,891][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:22:26,922][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:22:31,955][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:22:36,989][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:22:42,023][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:22:47,053][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:22:53,610][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:22:58,637][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:23:03,669][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:23:08,702][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:23:13,735][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:23:18,756][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:23:23,789][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:23:28,825][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:23:33,859][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:23:38,889][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:23:43,921][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:23:48,954][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:23:53,987][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:23:59,020][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:24:04,052][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:24:09,089][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:24:14,122][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:24:19,153][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:24:24,184][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:24:29,215][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:24:34,248][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:24:39,277][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:24:44,310][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:24:49,341][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:24:54,376][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:24:59,395][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:25:04,431][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:25:09,464][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:25:14,498][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:25:19,529][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:25:24,561][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:25:29,581][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:25:34,590][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:25:39,622][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:25:44,652][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:25:49,682][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:25:54,717][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:25:59,751][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:26:04,785][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:26:09,802][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:26:14,833][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:26:19,864][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:26:24,896][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:26:29,926][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:26:34,958][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:26:39,991][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:26:45,022][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:26:50,059][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:26:55,091][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:27:00,124][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:27:05,153][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:27:10,182][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:27:15,221][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:27:20,256][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:27:25,287][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:27:30,308][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:27:35,330][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:27:40,365][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:27:45,393][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:27:50,414][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:27:55,445][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:28:00,476][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:28:05,495][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:28:10,522][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:28:15,545][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:28:20,576][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:28:25,609][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:28:30,639][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:28:35,669][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:28:40,698][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:28:45,732][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:28:50,757][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:28:55,789][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:29:00,810][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:29:05,837][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:29:10,865][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:29:15,896][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:29:20,917][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:29:25,947][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:29:30,968][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:29:35,999][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:29:41,031][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:29:46,059][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:29:51,082][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T17:29:56,113][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T19:53:48,248][ERROR][org.logstash.Logstash    ] java.lang.IllegalStateException: Logstash stopped processing because of an error: (SystemExit) exit
[2020-05-26T19:55:46,695][WARN ][logstash.config.source.multilocal] Ignoring the 'pipelines.yml' file because modules or command line options are specified
[2020-05-26T19:55:46,793][INFO ][logstash.runner          ] Starting Logstash {"logstash.version"=>"7.7.0"}
[2020-05-26T19:55:48,652][INFO ][org.reflections.Reflections] Reflections took 80 ms to scan 1 urls, producing 21 keys and 41 values 
[2020-05-26T19:55:51,144][INFO ][logstash.outputs.elasticsearch][main] Elasticsearch pool URLs updated {:changes=>{:removed=>[], :added=>[http://localhost:9200/]}}
[2020-05-26T19:55:55,338][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T19:55:55,359][INFO ][logstash.outputs.elasticsearch][main] New Elasticsearch output {:class=>"LogStash::Outputs::ElasticSearch", :hosts=>["//localhost:9200"]}
[2020-05-26T19:55:56,110][WARN ][org.logstash.instrument.metrics.gauge.LazyDelegatingGauge][main] A gauge metric of an unknown type (org.jruby.RubyArray) has been created for key: cluster_uuids. This may result in invalid serialization.  It is recommended to log an issue to the responsible developer/development team.
[2020-05-26T19:55:56,116][INFO ][logstash.javapipeline    ][main] Starting pipeline {:pipeline_id=>"main", "pipeline.workers"=>8, "pipeline.batch.size"=>125, "pipeline.batch.delay"=>50, "pipeline.max_inflight"=>1000, "pipeline.sources"=>["D:/Softwares/logstash-7.7.0/config/logstash-sample.conf"], :thread=>"#<Thread:0x354a5bde run>"}
[2020-05-26T19:55:57,812][INFO ][logstash.javapipeline    ][main] Pipeline started {"pipeline.id"=>"main"}
[2020-05-26T19:55:57,902][INFO ][logstash.agent           ] Pipelines running {:count=>1, :running_pipelines=>[:main], :non_running_pipelines=>[]}
[2020-05-26T19:55:58,198][INFO ][logstash.agent           ] Successfully started Logstash API endpoint {:port=>9600}
[2020-05-26T19:56:04,382][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T19:56:09,400][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T19:56:14,411][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T19:56:19,434][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T19:56:24,457][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T19:56:29,469][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T19:56:34,481][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T19:56:39,494][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T19:56:39,518][WARN ][logstash.runner          ] SIGINT received. Shutting down.
[2020-05-26T19:56:44,507][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T19:56:44,531][WARN ][logstash.runner          ] Received shutdown signal, but pipeline is still waiting for in-flight events
to be processed. Sending another ^C will force quit Logstash, but this may cause
data loss.
[2020-05-26T19:56:44,650][WARN ][org.logstash.execution.ShutdownWatcherExt] {"inflight_count"=>0, "stalling_threads_info"=>{"other"=>[{"thread_id"=>43, "name"=>"[main]<stdin", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-input-stdin-3.2.6/lib/logstash/inputs/stdin.rb:77:in `read'"}], ["LogStash::Filters::Grok", {"match"=>{"message"=>"%{COMBINEDAPACHELOG}"}, "id"=>"509a6b1d2b3b0f807de9cd789e204701a13144926a3ade41642843011405b83e"}]=>[{"thread_id"=>35, "name"=>"[main]>worker0", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>36, "name"=>"[main]>worker1", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>37, "name"=>"[main]>worker2", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>38, "name"=>"[main]>worker3", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>39, "name"=>"[main]>worker4", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>40, "name"=>"[main]>worker5", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>41, "name"=>"[main]>worker6", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>42, "name"=>"[main]>worker7", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}]}}
[2020-05-26T19:56:44,660][ERROR][org.logstash.execution.ShutdownWatcherExt] The shutdown process appears to be stalled due to busy or blocked plugins. Check the logs for more information.
[2020-05-26T19:56:49,535][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T19:56:49,707][WARN ][org.logstash.execution.ShutdownWatcherExt] {"inflight_count"=>0, "stalling_threads_info"=>{"other"=>[{"thread_id"=>43, "name"=>"[main]<stdin", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-input-stdin-3.2.6/lib/logstash/inputs/stdin.rb:77:in `read'"}], ["LogStash::Filters::Grok", {"match"=>{"message"=>"%{COMBINEDAPACHELOG}"}, "id"=>"509a6b1d2b3b0f807de9cd789e204701a13144926a3ade41642843011405b83e"}]=>[{"thread_id"=>35, "name"=>"[main]>worker0", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>36, "name"=>"[main]>worker1", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>37, "name"=>"[main]>worker2", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>38, "name"=>"[main]>worker3", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>39, "name"=>"[main]>worker4", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>40, "name"=>"[main]>worker5", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>41, "name"=>"[main]>worker6", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>42, "name"=>"[main]>worker7", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}]}}
[2020-05-26T19:56:54,560][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T19:56:54,756][WARN ][org.logstash.execution.ShutdownWatcherExt] {"inflight_count"=>0, "stalling_threads_info"=>{"other"=>[{"thread_id"=>43, "name"=>"[main]<stdin", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-input-stdin-3.2.6/lib/logstash/inputs/stdin.rb:77:in `read'"}], ["LogStash::Filters::Grok", {"match"=>{"message"=>"%{COMBINEDAPACHELOG}"}, "id"=>"509a6b1d2b3b0f807de9cd789e204701a13144926a3ade41642843011405b83e"}]=>[{"thread_id"=>35, "name"=>"[main]>worker0", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>36, "name"=>"[main]>worker1", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>37, "name"=>"[main]>worker2", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>38, "name"=>"[main]>worker3", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>39, "name"=>"[main]>worker4", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>40, "name"=>"[main]>worker5", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>41, "name"=>"[main]>worker6", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>42, "name"=>"[main]>worker7", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}]}}
[2020-05-26T19:56:59,573][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T19:56:59,798][WARN ][org.logstash.execution.ShutdownWatcherExt] {"inflight_count"=>0, "stalling_threads_info"=>{"other"=>[{"thread_id"=>43, "name"=>"[main]<stdin", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-input-stdin-3.2.6/lib/logstash/inputs/stdin.rb:77:in `read'"}], ["LogStash::Filters::Grok", {"match"=>{"message"=>"%{COMBINEDAPACHELOG}"}, "id"=>"509a6b1d2b3b0f807de9cd789e204701a13144926a3ade41642843011405b83e"}]=>[{"thread_id"=>35, "name"=>"[main]>worker0", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>36, "name"=>"[main]>worker1", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>37, "name"=>"[main]>worker2", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>38, "name"=>"[main]>worker3", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>39, "name"=>"[main]>worker4", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>40, "name"=>"[main]>worker5", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>41, "name"=>"[main]>worker6", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>42, "name"=>"[main]>worker7", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}]}}
[2020-05-26T19:57:04,609][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T19:57:04,839][WARN ][org.logstash.execution.ShutdownWatcherExt] {"inflight_count"=>0, "stalling_threads_info"=>{"other"=>[{"thread_id"=>43, "name"=>"[main]<stdin", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-input-stdin-3.2.6/lib/logstash/inputs/stdin.rb:77:in `read'"}], ["LogStash::Filters::Grok", {"match"=>{"message"=>"%{COMBINEDAPACHELOG}"}, "id"=>"509a6b1d2b3b0f807de9cd789e204701a13144926a3ade41642843011405b83e"}]=>[{"thread_id"=>35, "name"=>"[main]>worker0", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>36, "name"=>"[main]>worker1", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>37, "name"=>"[main]>worker2", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>38, "name"=>"[main]>worker3", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>39, "name"=>"[main]>worker4", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>40, "name"=>"[main]>worker5", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>41, "name"=>"[main]>worker6", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>42, "name"=>"[main]>worker7", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}]}}
[2020-05-26T19:57:09,621][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T19:57:09,882][WARN ][org.logstash.execution.ShutdownWatcherExt] {"inflight_count"=>0, "stalling_threads_info"=>{"other"=>[{"thread_id"=>43, "name"=>"[main]<stdin", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-input-stdin-3.2.6/lib/logstash/inputs/stdin.rb:77:in `read'"}], ["LogStash::Filters::Grok", {"match"=>{"message"=>"%{COMBINEDAPACHELOG}"}, "id"=>"509a6b1d2b3b0f807de9cd789e204701a13144926a3ade41642843011405b83e"}]=>[{"thread_id"=>35, "name"=>"[main]>worker0", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>36, "name"=>"[main]>worker1", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>37, "name"=>"[main]>worker2", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>38, "name"=>"[main]>worker3", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>39, "name"=>"[main]>worker4", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>40, "name"=>"[main]>worker5", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>41, "name"=>"[main]>worker6", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>42, "name"=>"[main]>worker7", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}]}}
[2020-05-26T19:57:14,633][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T19:57:14,922][WARN ][org.logstash.execution.ShutdownWatcherExt] {"inflight_count"=>0, "stalling_threads_info"=>{"other"=>[{"thread_id"=>43, "name"=>"[main]<stdin", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-input-stdin-3.2.6/lib/logstash/inputs/stdin.rb:77:in `read'"}], ["LogStash::Filters::Grok", {"match"=>{"message"=>"%{COMBINEDAPACHELOG}"}, "id"=>"509a6b1d2b3b0f807de9cd789e204701a13144926a3ade41642843011405b83e"}]=>[{"thread_id"=>35, "name"=>"[main]>worker0", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>36, "name"=>"[main]>worker1", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>37, "name"=>"[main]>worker2", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>38, "name"=>"[main]>worker3", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>39, "name"=>"[main]>worker4", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>40, "name"=>"[main]>worker5", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>41, "name"=>"[main]>worker6", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>42, "name"=>"[main]>worker7", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}]}}
[2020-05-26T19:57:19,642][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T19:57:19,960][WARN ][org.logstash.execution.ShutdownWatcherExt] {"inflight_count"=>0, "stalling_threads_info"=>{"other"=>[{"thread_id"=>43, "name"=>"[main]<stdin", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-input-stdin-3.2.6/lib/logstash/inputs/stdin.rb:77:in `read'"}], ["LogStash::Filters::Grok", {"match"=>{"message"=>"%{COMBINEDAPACHELOG}"}, "id"=>"509a6b1d2b3b0f807de9cd789e204701a13144926a3ade41642843011405b83e"}]=>[{"thread_id"=>35, "name"=>"[main]>worker0", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>36, "name"=>"[main]>worker1", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>37, "name"=>"[main]>worker2", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>38, "name"=>"[main]>worker3", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>39, "name"=>"[main]>worker4", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>40, "name"=>"[main]>worker5", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>41, "name"=>"[main]>worker6", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>42, "name"=>"[main]>worker7", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}]}}
[2020-05-26T19:57:24,653][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T19:57:24,996][WARN ][org.logstash.execution.ShutdownWatcherExt] {"inflight_count"=>0, "stalling_threads_info"=>{"other"=>[{"thread_id"=>43, "name"=>"[main]<stdin", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-input-stdin-3.2.6/lib/logstash/inputs/stdin.rb:77:in `read'"}], ["LogStash::Filters::Grok", {"match"=>{"message"=>"%{COMBINEDAPACHELOG}"}, "id"=>"509a6b1d2b3b0f807de9cd789e204701a13144926a3ade41642843011405b83e"}]=>[{"thread_id"=>35, "name"=>"[main]>worker0", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>36, "name"=>"[main]>worker1", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>37, "name"=>"[main]>worker2", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>38, "name"=>"[main]>worker3", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>39, "name"=>"[main]>worker4", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>40, "name"=>"[main]>worker5", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>41, "name"=>"[main]>worker6", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>42, "name"=>"[main]>worker7", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}]}}
[2020-05-26T19:57:29,664][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T19:57:30,037][WARN ][org.logstash.execution.ShutdownWatcherExt] {"inflight_count"=>0, "stalling_threads_info"=>{"other"=>[{"thread_id"=>43, "name"=>"[main]<stdin", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-input-stdin-3.2.6/lib/logstash/inputs/stdin.rb:77:in `read'"}], ["LogStash::Filters::Grok", {"match"=>{"message"=>"%{COMBINEDAPACHELOG}"}, "id"=>"509a6b1d2b3b0f807de9cd789e204701a13144926a3ade41642843011405b83e"}]=>[{"thread_id"=>35, "name"=>"[main]>worker0", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>36, "name"=>"[main]>worker1", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>37, "name"=>"[main]>worker2", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>38, "name"=>"[main]>worker3", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>39, "name"=>"[main]>worker4", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>40, "name"=>"[main]>worker5", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>41, "name"=>"[main]>worker6", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>42, "name"=>"[main]>worker7", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}]}}
[2020-05-26T19:57:34,696][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T19:57:35,082][WARN ][org.logstash.execution.ShutdownWatcherExt] {"inflight_count"=>0, "stalling_threads_info"=>{"other"=>[{"thread_id"=>43, "name"=>"[main]<stdin", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-input-stdin-3.2.6/lib/logstash/inputs/stdin.rb:77:in `read'"}], ["LogStash::Filters::Grok", {"match"=>{"message"=>"%{COMBINEDAPACHELOG}"}, "id"=>"509a6b1d2b3b0f807de9cd789e204701a13144926a3ade41642843011405b83e"}]=>[{"thread_id"=>35, "name"=>"[main]>worker0", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>36, "name"=>"[main]>worker1", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>37, "name"=>"[main]>worker2", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>38, "name"=>"[main]>worker3", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>39, "name"=>"[main]>worker4", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>40, "name"=>"[main]>worker5", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>41, "name"=>"[main]>worker6", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>42, "name"=>"[main]>worker7", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}]}}
[2020-05-26T19:57:39,729][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T19:57:40,123][WARN ][org.logstash.execution.ShutdownWatcherExt] {"inflight_count"=>0, "stalling_threads_info"=>{"other"=>[{"thread_id"=>43, "name"=>"[main]<stdin", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-input-stdin-3.2.6/lib/logstash/inputs/stdin.rb:77:in `read'"}], ["LogStash::Filters::Grok", {"match"=>{"message"=>"%{COMBINEDAPACHELOG}"}, "id"=>"509a6b1d2b3b0f807de9cd789e204701a13144926a3ade41642843011405b83e"}]=>[{"thread_id"=>35, "name"=>"[main]>worker0", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>36, "name"=>"[main]>worker1", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>37, "name"=>"[main]>worker2", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>38, "name"=>"[main]>worker3", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>39, "name"=>"[main]>worker4", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>40, "name"=>"[main]>worker5", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>41, "name"=>"[main]>worker6", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>42, "name"=>"[main]>worker7", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}]}}
[2020-05-26T19:57:44,764][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T19:57:45,163][WARN ][org.logstash.execution.ShutdownWatcherExt] {"inflight_count"=>0, "stalling_threads_info"=>{"other"=>[{"thread_id"=>43, "name"=>"[main]<stdin", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-input-stdin-3.2.6/lib/logstash/inputs/stdin.rb:77:in `read'"}], ["LogStash::Filters::Grok", {"match"=>{"message"=>"%{COMBINEDAPACHELOG}"}, "id"=>"509a6b1d2b3b0f807de9cd789e204701a13144926a3ade41642843011405b83e"}]=>[{"thread_id"=>35, "name"=>"[main]>worker0", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>36, "name"=>"[main]>worker1", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>37, "name"=>"[main]>worker2", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>38, "name"=>"[main]>worker3", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>39, "name"=>"[main]>worker4", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>40, "name"=>"[main]>worker5", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>41, "name"=>"[main]>worker6", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>42, "name"=>"[main]>worker7", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}]}}
[2020-05-26T19:57:49,794][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T19:57:50,207][WARN ][org.logstash.execution.ShutdownWatcherExt] {"inflight_count"=>0, "stalling_threads_info"=>{"other"=>[{"thread_id"=>43, "name"=>"[main]<stdin", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-input-stdin-3.2.6/lib/logstash/inputs/stdin.rb:77:in `read'"}], ["LogStash::Filters::Grok", {"match"=>{"message"=>"%{COMBINEDAPACHELOG}"}, "id"=>"509a6b1d2b3b0f807de9cd789e204701a13144926a3ade41642843011405b83e"}]=>[{"thread_id"=>35, "name"=>"[main]>worker0", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>36, "name"=>"[main]>worker1", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>37, "name"=>"[main]>worker2", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>38, "name"=>"[main]>worker3", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>39, "name"=>"[main]>worker4", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>40, "name"=>"[main]>worker5", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>41, "name"=>"[main]>worker6", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>42, "name"=>"[main]>worker7", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}]}}
[2020-05-26T19:57:54,836][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T19:57:55,251][WARN ][org.logstash.execution.ShutdownWatcherExt] {"inflight_count"=>0, "stalling_threads_info"=>{"other"=>[{"thread_id"=>43, "name"=>"[main]<stdin", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-input-stdin-3.2.6/lib/logstash/inputs/stdin.rb:77:in `read'"}], ["LogStash::Filters::Grok", {"match"=>{"message"=>"%{COMBINEDAPACHELOG}"}, "id"=>"509a6b1d2b3b0f807de9cd789e204701a13144926a3ade41642843011405b83e"}]=>[{"thread_id"=>35, "name"=>"[main]>worker0", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>36, "name"=>"[main]>worker1", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>37, "name"=>"[main]>worker2", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>38, "name"=>"[main]>worker3", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>39, "name"=>"[main]>worker4", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>40, "name"=>"[main]>worker5", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>41, "name"=>"[main]>worker6", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>42, "name"=>"[main]>worker7", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}]}}
[2020-05-26T19:57:59,879][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T19:58:00,293][WARN ][org.logstash.execution.ShutdownWatcherExt] {"inflight_count"=>0, "stalling_threads_info"=>{"other"=>[{"thread_id"=>43, "name"=>"[main]<stdin", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-input-stdin-3.2.6/lib/logstash/inputs/stdin.rb:77:in `read'"}], ["LogStash::Filters::Grok", {"match"=>{"message"=>"%{COMBINEDAPACHELOG}"}, "id"=>"509a6b1d2b3b0f807de9cd789e204701a13144926a3ade41642843011405b83e"}]=>[{"thread_id"=>35, "name"=>"[main]>worker0", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>36, "name"=>"[main]>worker1", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>37, "name"=>"[main]>worker2", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>38, "name"=>"[main]>worker3", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>39, "name"=>"[main]>worker4", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>40, "name"=>"[main]>worker5", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>41, "name"=>"[main]>worker6", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>42, "name"=>"[main]>worker7", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}]}}
[2020-05-26T19:58:04,949][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T19:58:05,331][WARN ][org.logstash.execution.ShutdownWatcherExt] {"inflight_count"=>0, "stalling_threads_info"=>{"other"=>[{"thread_id"=>43, "name"=>"[main]<stdin", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-input-stdin-3.2.6/lib/logstash/inputs/stdin.rb:77:in `read'"}], ["LogStash::Filters::Grok", {"match"=>{"message"=>"%{COMBINEDAPACHELOG}"}, "id"=>"509a6b1d2b3b0f807de9cd789e204701a13144926a3ade41642843011405b83e"}]=>[{"thread_id"=>35, "name"=>"[main]>worker0", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>36, "name"=>"[main]>worker1", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>37, "name"=>"[main]>worker2", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>38, "name"=>"[main]>worker3", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>39, "name"=>"[main]>worker4", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>40, "name"=>"[main]>worker5", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>41, "name"=>"[main]>worker6", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>42, "name"=>"[main]>worker7", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}]}}
[2020-05-26T19:58:09,969][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T19:58:10,370][WARN ][org.logstash.execution.ShutdownWatcherExt] {"inflight_count"=>0, "stalling_threads_info"=>{"other"=>[{"thread_id"=>43, "name"=>"[main]<stdin", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-input-stdin-3.2.6/lib/logstash/inputs/stdin.rb:77:in `read'"}], ["LogStash::Filters::Grok", {"match"=>{"message"=>"%{COMBINEDAPACHELOG}"}, "id"=>"509a6b1d2b3b0f807de9cd789e204701a13144926a3ade41642843011405b83e"}]=>[{"thread_id"=>35, "name"=>"[main]>worker0", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>36, "name"=>"[main]>worker1", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>37, "name"=>"[main]>worker2", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>38, "name"=>"[main]>worker3", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>39, "name"=>"[main]>worker4", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>40, "name"=>"[main]>worker5", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>41, "name"=>"[main]>worker6", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>42, "name"=>"[main]>worker7", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}]}}
[2020-05-26T19:58:15,003][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T19:58:15,411][WARN ][org.logstash.execution.ShutdownWatcherExt] {"inflight_count"=>0, "stalling_threads_info"=>{"other"=>[{"thread_id"=>43, "name"=>"[main]<stdin", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-input-stdin-3.2.6/lib/logstash/inputs/stdin.rb:77:in `read'"}], ["LogStash::Filters::Grok", {"match"=>{"message"=>"%{COMBINEDAPACHELOG}"}, "id"=>"509a6b1d2b3b0f807de9cd789e204701a13144926a3ade41642843011405b83e"}]=>[{"thread_id"=>35, "name"=>"[main]>worker0", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>36, "name"=>"[main]>worker1", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>37, "name"=>"[main]>worker2", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>38, "name"=>"[main]>worker3", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>39, "name"=>"[main]>worker4", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>40, "name"=>"[main]>worker5", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>41, "name"=>"[main]>worker6", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>42, "name"=>"[main]>worker7", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}]}}
[2020-05-26T19:58:20,037][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T19:58:20,465][WARN ][org.logstash.execution.ShutdownWatcherExt] {"inflight_count"=>0, "stalling_threads_info"=>{"other"=>[{"thread_id"=>43, "name"=>"[main]<stdin", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-input-stdin-3.2.6/lib/logstash/inputs/stdin.rb:77:in `read'"}], ["LogStash::Filters::Grok", {"match"=>{"message"=>"%{COMBINEDAPACHELOG}"}, "id"=>"509a6b1d2b3b0f807de9cd789e204701a13144926a3ade41642843011405b83e"}]=>[{"thread_id"=>35, "name"=>"[main]>worker0", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>36, "name"=>"[main]>worker1", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>37, "name"=>"[main]>worker2", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>38, "name"=>"[main]>worker3", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>39, "name"=>"[main]>worker4", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>40, "name"=>"[main]>worker5", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>41, "name"=>"[main]>worker6", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>42, "name"=>"[main]>worker7", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}]}}
[2020-05-26T19:58:25,071][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T19:58:25,514][WARN ][org.logstash.execution.ShutdownWatcherExt] {"inflight_count"=>0, "stalling_threads_info"=>{"other"=>[{"thread_id"=>43, "name"=>"[main]<stdin", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-input-stdin-3.2.6/lib/logstash/inputs/stdin.rb:77:in `read'"}], ["LogStash::Filters::Grok", {"match"=>{"message"=>"%{COMBINEDAPACHELOG}"}, "id"=>"509a6b1d2b3b0f807de9cd789e204701a13144926a3ade41642843011405b83e"}]=>[{"thread_id"=>35, "name"=>"[main]>worker0", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>36, "name"=>"[main]>worker1", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>37, "name"=>"[main]>worker2", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>38, "name"=>"[main]>worker3", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>39, "name"=>"[main]>worker4", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>40, "name"=>"[main]>worker5", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>41, "name"=>"[main]>worker6", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>42, "name"=>"[main]>worker7", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}]}}
[2020-05-26T19:58:30,106][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T19:58:30,605][WARN ][org.logstash.execution.ShutdownWatcherExt] {"inflight_count"=>0, "stalling_threads_info"=>{"other"=>[{"thread_id"=>43, "name"=>"[main]<stdin", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-input-stdin-3.2.6/lib/logstash/inputs/stdin.rb:77:in `read'"}], ["LogStash::Filters::Grok", {"match"=>{"message"=>"%{COMBINEDAPACHELOG}"}, "id"=>"509a6b1d2b3b0f807de9cd789e204701a13144926a3ade41642843011405b83e"}]=>[{"thread_id"=>35, "name"=>"[main]>worker0", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>36, "name"=>"[main]>worker1", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>37, "name"=>"[main]>worker2", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>38, "name"=>"[main]>worker3", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>39, "name"=>"[main]>worker4", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>40, "name"=>"[main]>worker5", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>41, "name"=>"[main]>worker6", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>42, "name"=>"[main]>worker7", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}]}}
[2020-05-26T19:58:35,141][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T19:58:35,708][WARN ][org.logstash.execution.ShutdownWatcherExt] {"inflight_count"=>0, "stalling_threads_info"=>{"other"=>[{"thread_id"=>43, "name"=>"[main]<stdin", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-input-stdin-3.2.6/lib/logstash/inputs/stdin.rb:77:in `read'"}], ["LogStash::Filters::Grok", {"match"=>{"message"=>"%{COMBINEDAPACHELOG}"}, "id"=>"509a6b1d2b3b0f807de9cd789e204701a13144926a3ade41642843011405b83e"}]=>[{"thread_id"=>35, "name"=>"[main]>worker0", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>36, "name"=>"[main]>worker1", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>37, "name"=>"[main]>worker2", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>38, "name"=>"[main]>worker3", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>39, "name"=>"[main]>worker4", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>40, "name"=>"[main]>worker5", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>41, "name"=>"[main]>worker6", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>42, "name"=>"[main]>worker7", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}]}}
[2020-05-26T19:58:40,172][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T19:58:40,790][WARN ][org.logstash.execution.ShutdownWatcherExt] {"inflight_count"=>0, "stalling_threads_info"=>{"other"=>[{"thread_id"=>43, "name"=>"[main]<stdin", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-input-stdin-3.2.6/lib/logstash/inputs/stdin.rb:77:in `read'"}], ["LogStash::Filters::Grok", {"match"=>{"message"=>"%{COMBINEDAPACHELOG}"}, "id"=>"509a6b1d2b3b0f807de9cd789e204701a13144926a3ade41642843011405b83e"}]=>[{"thread_id"=>35, "name"=>"[main]>worker0", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>36, "name"=>"[main]>worker1", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>37, "name"=>"[main]>worker2", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>38, "name"=>"[main]>worker3", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>39, "name"=>"[main]>worker4", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>40, "name"=>"[main]>worker5", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>41, "name"=>"[main]>worker6", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>42, "name"=>"[main]>worker7", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}]}}
[2020-05-26T19:58:45,203][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T19:58:45,874][WARN ][org.logstash.execution.ShutdownWatcherExt] {"inflight_count"=>0, "stalling_threads_info"=>{"other"=>[{"thread_id"=>43, "name"=>"[main]<stdin", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-input-stdin-3.2.6/lib/logstash/inputs/stdin.rb:77:in `read'"}], ["LogStash::Filters::Grok", {"match"=>{"message"=>"%{COMBINEDAPACHELOG}"}, "id"=>"509a6b1d2b3b0f807de9cd789e204701a13144926a3ade41642843011405b83e"}]=>[{"thread_id"=>35, "name"=>"[main]>worker0", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>36, "name"=>"[main]>worker1", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>37, "name"=>"[main]>worker2", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>38, "name"=>"[main]>worker3", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>39, "name"=>"[main]>worker4", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>40, "name"=>"[main]>worker5", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>41, "name"=>"[main]>worker6", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>42, "name"=>"[main]>worker7", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}]}}
[2020-05-26T19:58:50,226][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T19:58:50,929][WARN ][org.logstash.execution.ShutdownWatcherExt] {"inflight_count"=>0, "stalling_threads_info"=>{"other"=>[{"thread_id"=>43, "name"=>"[main]<stdin", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-input-stdin-3.2.6/lib/logstash/inputs/stdin.rb:77:in `read'"}], ["LogStash::Filters::Grok", {"match"=>{"message"=>"%{COMBINEDAPACHELOG}"}, "id"=>"509a6b1d2b3b0f807de9cd789e204701a13144926a3ade41642843011405b83e"}]=>[{"thread_id"=>35, "name"=>"[main]>worker0", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>36, "name"=>"[main]>worker1", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>37, "name"=>"[main]>worker2", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>38, "name"=>"[main]>worker3", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>39, "name"=>"[main]>worker4", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>40, "name"=>"[main]>worker5", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>41, "name"=>"[main]>worker6", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>42, "name"=>"[main]>worker7", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}]}}
[2020-05-26T19:58:55,247][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T19:58:56,009][WARN ][org.logstash.execution.ShutdownWatcherExt] {"inflight_count"=>0, "stalling_threads_info"=>{"other"=>[{"thread_id"=>43, "name"=>"[main]<stdin", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-input-stdin-3.2.6/lib/logstash/inputs/stdin.rb:77:in `read'"}], ["LogStash::Filters::Grok", {"match"=>{"message"=>"%{COMBINEDAPACHELOG}"}, "id"=>"509a6b1d2b3b0f807de9cd789e204701a13144926a3ade41642843011405b83e"}]=>[{"thread_id"=>35, "name"=>"[main]>worker0", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>36, "name"=>"[main]>worker1", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>37, "name"=>"[main]>worker2", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>38, "name"=>"[main]>worker3", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>39, "name"=>"[main]>worker4", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>40, "name"=>"[main]>worker5", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>41, "name"=>"[main]>worker6", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>42, "name"=>"[main]>worker7", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}]}}
[2020-05-26T19:59:00,283][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T19:59:01,092][WARN ][org.logstash.execution.ShutdownWatcherExt] {"inflight_count"=>0, "stalling_threads_info"=>{"other"=>[{"thread_id"=>43, "name"=>"[main]<stdin", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-input-stdin-3.2.6/lib/logstash/inputs/stdin.rb:77:in `read'"}], ["LogStash::Filters::Grok", {"match"=>{"message"=>"%{COMBINEDAPACHELOG}"}, "id"=>"509a6b1d2b3b0f807de9cd789e204701a13144926a3ade41642843011405b83e"}]=>[{"thread_id"=>35, "name"=>"[main]>worker0", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>36, "name"=>"[main]>worker1", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>37, "name"=>"[main]>worker2", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>38, "name"=>"[main]>worker3", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>39, "name"=>"[main]>worker4", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>40, "name"=>"[main]>worker5", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>41, "name"=>"[main]>worker6", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>42, "name"=>"[main]>worker7", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}]}}
[2020-05-26T19:59:05,321][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T19:59:06,199][WARN ][org.logstash.execution.ShutdownWatcherExt] {"inflight_count"=>0, "stalling_threads_info"=>{"other"=>[{"thread_id"=>43, "name"=>"[main]<stdin", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-input-stdin-3.2.6/lib/logstash/inputs/stdin.rb:77:in `read'"}], ["LogStash::Filters::Grok", {"match"=>{"message"=>"%{COMBINEDAPACHELOG}"}, "id"=>"509a6b1d2b3b0f807de9cd789e204701a13144926a3ade41642843011405b83e"}]=>[{"thread_id"=>35, "name"=>"[main]>worker0", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>36, "name"=>"[main]>worker1", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>37, "name"=>"[main]>worker2", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>38, "name"=>"[main]>worker3", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>39, "name"=>"[main]>worker4", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>40, "name"=>"[main]>worker5", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>41, "name"=>"[main]>worker6", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>42, "name"=>"[main]>worker7", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}]}}
[2020-05-26T19:59:10,353][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T19:59:11,293][WARN ][org.logstash.execution.ShutdownWatcherExt] {"inflight_count"=>0, "stalling_threads_info"=>{"other"=>[{"thread_id"=>43, "name"=>"[main]<stdin", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-input-stdin-3.2.6/lib/logstash/inputs/stdin.rb:77:in `read'"}], ["LogStash::Filters::Grok", {"match"=>{"message"=>"%{COMBINEDAPACHELOG}"}, "id"=>"509a6b1d2b3b0f807de9cd789e204701a13144926a3ade41642843011405b83e"}]=>[{"thread_id"=>35, "name"=>"[main]>worker0", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>36, "name"=>"[main]>worker1", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>37, "name"=>"[main]>worker2", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>38, "name"=>"[main]>worker3", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>39, "name"=>"[main]>worker4", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>40, "name"=>"[main]>worker5", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>41, "name"=>"[main]>worker6", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>42, "name"=>"[main]>worker7", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}]}}
[2020-05-26T19:59:15,385][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T19:59:16,384][WARN ][org.logstash.execution.ShutdownWatcherExt] {"inflight_count"=>0, "stalling_threads_info"=>{"other"=>[{"thread_id"=>43, "name"=>"[main]<stdin", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-input-stdin-3.2.6/lib/logstash/inputs/stdin.rb:77:in `read'"}], ["LogStash::Filters::Grok", {"match"=>{"message"=>"%{COMBINEDAPACHELOG}"}, "id"=>"509a6b1d2b3b0f807de9cd789e204701a13144926a3ade41642843011405b83e"}]=>[{"thread_id"=>35, "name"=>"[main]>worker0", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>36, "name"=>"[main]>worker1", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>37, "name"=>"[main]>worker2", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>38, "name"=>"[main]>worker3", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>39, "name"=>"[main]>worker4", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>40, "name"=>"[main]>worker5", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>41, "name"=>"[main]>worker6", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>42, "name"=>"[main]>worker7", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}]}}
[2020-05-26T19:59:20,423][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T19:59:21,449][WARN ][org.logstash.execution.ShutdownWatcherExt] {"inflight_count"=>0, "stalling_threads_info"=>{"other"=>[{"thread_id"=>43, "name"=>"[main]<stdin", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-input-stdin-3.2.6/lib/logstash/inputs/stdin.rb:77:in `read'"}], ["LogStash::Filters::Grok", {"match"=>{"message"=>"%{COMBINEDAPACHELOG}"}, "id"=>"509a6b1d2b3b0f807de9cd789e204701a13144926a3ade41642843011405b83e"}]=>[{"thread_id"=>35, "name"=>"[main]>worker0", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>36, "name"=>"[main]>worker1", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>37, "name"=>"[main]>worker2", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>38, "name"=>"[main]>worker3", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>39, "name"=>"[main]>worker4", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>40, "name"=>"[main]>worker5", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>41, "name"=>"[main]>worker6", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>42, "name"=>"[main]>worker7", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}]}}
[2020-05-26T19:59:25,447][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T19:59:26,501][WARN ][org.logstash.execution.ShutdownWatcherExt] {"inflight_count"=>0, "stalling_threads_info"=>{"other"=>[{"thread_id"=>43, "name"=>"[main]<stdin", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-input-stdin-3.2.6/lib/logstash/inputs/stdin.rb:77:in `read'"}], ["LogStash::Filters::Grok", {"match"=>{"message"=>"%{COMBINEDAPACHELOG}"}, "id"=>"509a6b1d2b3b0f807de9cd789e204701a13144926a3ade41642843011405b83e"}]=>[{"thread_id"=>35, "name"=>"[main]>worker0", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>36, "name"=>"[main]>worker1", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>37, "name"=>"[main]>worker2", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>38, "name"=>"[main]>worker3", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>39, "name"=>"[main]>worker4", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>40, "name"=>"[main]>worker5", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>41, "name"=>"[main]>worker6", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>42, "name"=>"[main]>worker7", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}]}}
[2020-05-26T19:59:30,478][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T19:59:31,579][WARN ][org.logstash.execution.ShutdownWatcherExt] {"inflight_count"=>0, "stalling_threads_info"=>{"other"=>[{"thread_id"=>43, "name"=>"[main]<stdin", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-input-stdin-3.2.6/lib/logstash/inputs/stdin.rb:77:in `read'"}], ["LogStash::Filters::Grok", {"match"=>{"message"=>"%{COMBINEDAPACHELOG}"}, "id"=>"509a6b1d2b3b0f807de9cd789e204701a13144926a3ade41642843011405b83e"}]=>[{"thread_id"=>35, "name"=>"[main]>worker0", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>36, "name"=>"[main]>worker1", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>37, "name"=>"[main]>worker2", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>38, "name"=>"[main]>worker3", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>39, "name"=>"[main]>worker4", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>40, "name"=>"[main]>worker5", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>41, "name"=>"[main]>worker6", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>42, "name"=>"[main]>worker7", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}]}}
[2020-05-26T19:59:35,503][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T19:59:36,671][WARN ][org.logstash.execution.ShutdownWatcherExt] {"inflight_count"=>0, "stalling_threads_info"=>{"other"=>[{"thread_id"=>43, "name"=>"[main]<stdin", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-input-stdin-3.2.6/lib/logstash/inputs/stdin.rb:77:in `read'"}], ["LogStash::Filters::Grok", {"match"=>{"message"=>"%{COMBINEDAPACHELOG}"}, "id"=>"509a6b1d2b3b0f807de9cd789e204701a13144926a3ade41642843011405b83e"}]=>[{"thread_id"=>35, "name"=>"[main]>worker0", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>36, "name"=>"[main]>worker1", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>37, "name"=>"[main]>worker2", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>38, "name"=>"[main]>worker3", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>39, "name"=>"[main]>worker4", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>40, "name"=>"[main]>worker5", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>41, "name"=>"[main]>worker6", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>42, "name"=>"[main]>worker7", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}]}}
[2020-05-26T19:59:40,538][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T19:59:41,769][WARN ][org.logstash.execution.ShutdownWatcherExt] {"inflight_count"=>0, "stalling_threads_info"=>{"other"=>[{"thread_id"=>43, "name"=>"[main]<stdin", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-input-stdin-3.2.6/lib/logstash/inputs/stdin.rb:77:in `read'"}], ["LogStash::Filters::Grok", {"match"=>{"message"=>"%{COMBINEDAPACHELOG}"}, "id"=>"509a6b1d2b3b0f807de9cd789e204701a13144926a3ade41642843011405b83e"}]=>[{"thread_id"=>35, "name"=>"[main]>worker0", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>36, "name"=>"[main]>worker1", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>37, "name"=>"[main]>worker2", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>38, "name"=>"[main]>worker3", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>39, "name"=>"[main]>worker4", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>40, "name"=>"[main]>worker5", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>41, "name"=>"[main]>worker6", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}, {"thread_id"=>42, "name"=>"[main]>worker7", "current_call"=>"[...]/vendor/bundle/jruby/2.5.0/gems/logstash-output-elasticsearch-10.5.1-java/lib/logstash/outputs/elasticsearch/common.rb:34:in `sleep'"}]}}
[2020-05-26T19:59:45,573][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://localhost:9200/][Manticore::SocketException] Connection refused: connect"}
[2020-05-26T20:00:27,969][WARN ][logstash.config.source.multilocal] Ignoring the 'pipelines.yml' file because modules or command line options are specified
[2020-05-26T20:00:28,065][INFO ][logstash.runner          ] Starting Logstash {"logstash.version"=>"7.7.0"}
[2020-05-26T20:00:29,615][INFO ][org.reflections.Reflections] Reflections took 35 ms to scan 1 urls, producing 21 keys and 41 values 
[2020-05-26T20:00:31,412][WARN ][org.logstash.instrument.metrics.gauge.LazyDelegatingGauge][main] A gauge metric of an unknown type (org.jruby.RubyArray) has been created for key: cluster_uuids. This may result in invalid serialization.  It is recommended to log an issue to the responsible developer/development team.
[2020-05-26T20:00:31,426][INFO ][logstash.javapipeline    ][main] Starting pipeline {:pipeline_id=>"main", "pipeline.workers"=>8, "pipeline.batch.size"=>125, "pipeline.batch.delay"=>50, "pipeline.max_inflight"=>1000, "pipeline.sources"=>["D:/Softwares/logstash-7.7.0/config/logstash-sample.conf"], :thread=>"#<Thread:0x181df0ca run>"}
[2020-05-26T20:00:32,257][INFO ][logstash.javapipeline    ][main] Pipeline started {"pipeline.id"=>"main"}
[2020-05-26T20:00:32,320][INFO ][logstash.agent           ] Pipelines running {:count=>1, :running_pipelines=>[:main], :non_running_pipelines=>[]}
[2020-05-26T20:00:32,565][INFO ][logstash.agent           ] Successfully started Logstash API endpoint {:port=>9600}
[2020-05-26T20:02:02,300][WARN ][logstash.runner          ] SIGINT received. Shutting down.
[2020-05-26T20:02:07,004][INFO ][logstash.javapipeline    ] Pipeline terminated {"pipeline.id"=>"main"}
[2020-05-26T20:02:07,048][INFO ][logstash.runner          ] Logstash shut down.
